"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í†µí•© ì½”ë“œ ì‹¤í–‰ê¸°
OnlineCodeExecutor + PerformanceBenchmark + CodeHistoryManager í†µí•©
"""

import os
import sys
import asyncio
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import asdict

# í•„ìš”í•œ ëª¨ë“ˆë“¤ import
try:
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)
    sys.path.insert(0, current_dir)
    sys.path.insert(0, parent_dir)
    
    from online_code_executor import OnlineCodeExecutor, ExecutionResult
    from performance_benchmark import PerformanceBenchmark, BenchmarkResult, OptimizationSuggestion, get_performance_benchmark
    from code_history_manager import CodeHistoryManager, history_manager
    
    MODULES_AVAILABLE = True
    print("âœ… ëª¨ë“  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ëª¨ë“ˆì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except ImportError as e:
    print(f"âš ï¸ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    MODULES_AVAILABLE = False

class EnhancedPerformanceExecutor(OnlineCodeExecutor):
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™” ì œì•ˆì´ í†µí•©ëœ í–¥ìƒëœ ì½”ë“œ ì‹¤í–‰ê¸°"""
    
    def __init__(self, ai_handler=None, 
                 performance_benchmark_instance: Optional[PerformanceBenchmark] = None,
                 history_manager_instance: Optional[CodeHistoryManager] = None):
        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        super().__init__(ai_handler)
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •
        self.performance_benchmark = performance_benchmark_instance or get_performance_benchmark()
        self.benchmark_enabled = performance_benchmark_instance is not None or MODULES_AVAILABLE
        
        # íˆìŠ¤í† ë¦¬ ë§¤ë‹ˆì € ì„¤ì •
        self.history_manager = history_manager_instance or history_manager
        self.history_enabled = history_manager_instance is not None or MODULES_AVAILABLE
        
        # ì„±ëŠ¥ ë¶„ì„ ì˜µì…˜
        self.enable_detailed_analysis = True
        self.enable_optimization_suggestions = True
        self.enable_comparative_analysis = True
        
        print(f"ğŸš€ EnhancedPerformanceExecutor ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬: {'í™œì„±í™”' if self.benchmark_enabled else 'ë¹„í™œì„±í™”'}")
        print(f"   ğŸ“ˆ íˆìŠ¤í† ë¦¬ ê´€ë¦¬: {'í™œì„±í™”' if self.history_enabled else 'ë¹„í™œì„±í™”'}")
    
    async def execute_code_with_performance_analysis(self, code: str, language: str, 
                                                   user_id: str = "default_user",
                                                   use_online_api: bool = False,
                                                   enable_ai_review: bool = True) -> Tuple[ExecutionResult, Optional[BenchmarkResult], List[OptimizationSuggestion]]:
        """ì„±ëŠ¥ ë¶„ì„ì´ í¬í•¨ëœ ì¢…í•© ì½”ë“œ ì‹¤í–‰"""
        
        print(f"ğŸ”¬ ì„±ëŠ¥ ë¶„ì„ í¬í•¨ ì½”ë“œ ì‹¤í–‰ ì‹œì‘: {language}")
        
        # 1. ê¸°ë³¸ ì½”ë“œ ì‹¤í–‰
        execution_result = await self.execute_code(code, language, use_online_api)
        
        benchmark_result = None
        optimization_suggestions = []
        
        # 2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ (ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ëœ ê²½ìš°ì—ë§Œ)
        if self.benchmark_enabled and execution_result.success:
            try:
                print("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ ì¤‘...")
                
                # CPU ì‚¬ìš©ëŸ‰ ì¶”ì • (ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜)
                cpu_usage = min(execution_result.execution_time * 10, 100.0)
                
                benchmark_result = self.performance_benchmark.analyze_performance(
                    code=code,
                    language=language,
                    execution_time=execution_result.execution_time,
                    memory_usage=execution_result.memory_usage,
                    cpu_usage=cpu_usage
                )
                
                print(f"   ğŸ¯ ì„±ëŠ¥ ì ìˆ˜: {benchmark_result.performance_score:.1f}/100")
                print(f"   ğŸ§® ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„: {benchmark_result.algorithm_complexity}")
                print(f"   ğŸ“ˆ ìµœì í™” ë ˆë²¨: {benchmark_result.optimization_level}")
                
            except Exception as e:
                print(f"âš ï¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # 3. ìµœì í™” ì œì•ˆ ìƒì„±
        if self.enable_optimization_suggestions and benchmark_result:
            try:
                print("ğŸ’¡ ìµœì í™” ì œì•ˆ ìƒì„± ì¤‘...")
                optimization_suggestions = self.performance_benchmark.generate_optimization_suggestions(
                    code, language, benchmark_result
                )
                print(f"   âœ¨ {len(optimization_suggestions)}ê°œì˜ ìµœì í™” ì œì•ˆ ìƒì„±ë¨")
                
            except Exception as e:
                print(f"âš ï¸ ìµœì í™” ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # 4. íˆìŠ¤í† ë¦¬ì— ê¸°ë¡ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.history_enabled:
            try:
                # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ íˆìŠ¤í† ë¦¬ ë°ì´í„°ì— ì¶”ê°€
                history_data = {
                    "success": execution_result.success,
                    "execution_time": execution_result.execution_time,
                    "memory_usage": execution_result.memory_usage,
                    "performance_score": execution_result.performance_score,
                    "output": execution_result.output,
                    "error": execution_result.error,
                    "optimization_suggestions": [asdict(s) for s in optimization_suggestions],
                }
                
                if benchmark_result:
                    history_data.update({
                        "benchmark_result": asdict(benchmark_result),
                        "algorithm_complexity": benchmark_result.algorithm_complexity,
                        "optimization_level": benchmark_result.optimization_level,
                        "comparative_ranking": benchmark_result.comparative_ranking
                    })
                
                await self.history_manager.add_execution_record(
                    user_id=user_id,
                    code=code,
                    language=language,
                    execution_data=history_data
                )
                
            except Exception as e:
                print(f"âš ï¸ íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì‹¤íŒ¨: {e}")
        
        return execution_result, benchmark_result, optimization_suggestions
    
    def get_performance_insights(self, code: str, language: str, 
                               benchmark_result: BenchmarkResult) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„"""
        
        insights = {
            "performance_summary": {
                "score": benchmark_result.performance_score,
                "level": benchmark_result.optimization_level,
                "complexity": benchmark_result.algorithm_complexity,
                "category": benchmark_result.benchmark_category,
                "ranking": benchmark_result.comparative_ranking
            }
        }
        
        # ê¸°ì¤€ì„ ê³¼ ë¹„êµ
        if self.enable_comparative_analysis:
            try:
                comparison = self.performance_benchmark.compare_with_baseline(benchmark_result)
                insights["baseline_comparison"] = comparison
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¤€ì„  ë¹„êµ ì‹¤íŒ¨: {e}")
        
        # ì–¸ì–´ë³„ í†µê³„
        try:
            language_stats = self.performance_benchmark.get_benchmark_statistics(
                language=language,
                category=benchmark_result.benchmark_category
            )
            insights["language_statistics"] = language_stats
        except Exception as e:
            print(f"âš ï¸ ì–¸ì–´ë³„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ì„±ëŠ¥ ê°œì„  ì¶”ì²œ
        insights["improvement_recommendations"] = self._generate_improvement_recommendations(
            benchmark_result
        )
        
        return insights
    
    def _generate_improvement_recommendations(self, benchmark_result: BenchmarkResult) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì„±ëŠ¥ ì ìˆ˜ ê¸°ë°˜ ì¶”ì²œ
        if benchmark_result.performance_score < 50:
            recommendations.append("ğŸ”§ ì „ì²´ì ì¸ ì•Œê³ ë¦¬ì¦˜ ì¬ì„¤ê³„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
            recommendations.append("ğŸ“š í•´ë‹¹ ë¬¸ì œì— íŠ¹í™”ëœ ìë£Œêµ¬ì¡°ë‚˜ ì•Œê³ ë¦¬ì¦˜ì„ ì—°êµ¬í•´ë³´ì„¸ìš”")
        elif benchmark_result.performance_score < 70:
            recommendations.append("âš¡ ì¼ë¶€ ë³‘ëª© ì§€ì ì„ ìµœì í™”í•˜ë©´ ì„±ëŠ¥ì´ í¬ê²Œ ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            recommendations.append("ğŸ” í”„ë¡œíŒŒì¼ë§ì„ í†µí•´ ê°€ì¥ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë¶€ë¶„ì„ ì°¾ì•„ë³´ì„¸ìš”")
        elif benchmark_result.performance_score < 90:
            recommendations.append("âœ¨ ì´ë¯¸ ì¢‹ì€ ì„±ëŠ¥ì…ë‹ˆë‹¤. ë¯¸ì„¸í•œ ìµœì í™”ë¡œ ë” ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        else:
            recommendations.append("ğŸŒŸ ìš°ìˆ˜í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤! í˜„ì¬ êµ¬í˜„ì´ ë§¤ìš° íš¨ìœ¨ì ì…ë‹ˆë‹¤")
        
        # ë³µì¡ë„ ê¸°ë°˜ ì¶”ì²œ
        if benchmark_result.algorithm_complexity == 'O(n^2)':
            recommendations.append("ğŸ“Š O(n log n) ë˜ëŠ” O(n) ë³µì¡ë„ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”")
        elif benchmark_result.algorithm_complexity == 'O(n^3)':
            recommendations.append("ğŸš¨ íë¹… ì‹œê°„ ë³µì¡ë„ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        elif benchmark_result.algorithm_complexity == 'O(2^n)':
            recommendations.append("âš ï¸ ì§€ìˆ˜ ì‹œê°„ ë³µì¡ë„ì…ë‹ˆë‹¤. ë™ì  í”„ë¡œê·¸ë˜ë°ì´ë‚˜ ë©”ëª¨ì´ì œì´ì…˜ì„ ê³ ë ¤í•˜ì„¸ìš”")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ì¶”ì²œ
        if benchmark_result.memory_usage > 1000:  # 1GB ì´ìƒ
            recommendations.append("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë‚˜ ì••ì¶•ì„ ê³ ë ¤í•˜ì„¸ìš”")
        elif benchmark_result.memory_usage > 500:  # 500MB ì´ìƒ
            recommendations.append("ğŸ—‚ï¸ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° êµ¬ì¡° ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”")
        
        return recommendations
    
    async def run_performance_benchmark_suite(self, language: str, 
                                            test_cases: List[Dict[str, str]]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        
        print(f"ğŸ§ª {language} ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰ ì‹œì‘")
        print(f"ğŸ“ ì´ {len(test_cases)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤")
        
        results = []
        total_execution_time = 0
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ”¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}/{len(test_cases)}: {test_case.get('name', f'Test_{i}')}")
            
            try:
                execution_result, benchmark_result, suggestions = await self.execute_code_with_performance_analysis(
                    code=test_case['code'],
                    language=language,
                    user_id=f"benchmark_user_{i}"
                )
                
                total_execution_time += execution_result.execution_time
                
                test_result = {
                    "test_name": test_case.get('name', f'Test_{i}'),
                    "execution_result": asdict(execution_result),
                    "benchmark_result": asdict(benchmark_result) if benchmark_result else None,
                    "optimization_suggestions": [asdict(s) for s in suggestions],
                    "insights": self.get_performance_insights(test_case['code'], language, benchmark_result) if benchmark_result else None
                }
                
                results.append(test_result)
                
                print(f"   âœ… ì™„ë£Œ - ì„±ëŠ¥ ì ìˆ˜: {benchmark_result.performance_score:.1f}/100" if benchmark_result else "   âœ… ì™„ë£Œ")
                
            except Exception as e:
                print(f"   âŒ ì‹¤íŒ¨: {e}")
                results.append({
                    "test_name": test_case.get('name', f'Test_{i}'),
                    "error": str(e),
                    "execution_result": None,
                    "benchmark_result": None
                })
        
        # ì „ì²´ ê²°ê³¼ ë¶„ì„
        successful_results = [r for r in results if r.get('benchmark_result')]
        
        suite_summary = {
            "total_tests": len(test_cases),
            "successful_tests": len(successful_results),
            "failed_tests": len(test_cases) - len(successful_results),
            "total_execution_time": total_execution_time,
            "average_performance_score": 0,
            "best_performance": None,
            "worst_performance": None,
            "results": results
        }
        
        if successful_results:
            scores = [r['benchmark_result']['performance_score'] for r in successful_results]
            suite_summary["average_performance_score"] = sum(scores) / len(scores)
            suite_summary["best_performance"] = max(successful_results, key=lambda x: x['benchmark_result']['performance_score'])
            suite_summary["worst_performance"] = min(successful_results, key=lambda x: x['benchmark_result']['performance_score'])
        
        print(f"\nğŸ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì™„ë£Œ")
        print(f"   ğŸ“Š ì„±ê³µë¥ : {len(successful_results)}/{len(test_cases)} ({len(successful_results)/len(test_cases)*100:.1f}%)")
        if suite_summary["average_performance_score"] > 0:
            print(f"   ğŸ¯ í‰ê·  ì„±ëŠ¥: {suite_summary['average_performance_score']:.1f}/100")
        
        return suite_summary
    
    def get_language_performance_comparison(self, languages: List[str]) -> Dict[str, Any]:
        """ì–¸ì–´ë³„ ì„±ëŠ¥ ë¹„êµ ë¶„ì„"""
        
        comparison_data = {}
        
        for language in languages:
            try:
                stats = self.performance_benchmark.get_benchmark_statistics(language=language)
                comparison_data[language] = {
                    "total_benchmarks": stats.get("total_benchmarks", 0),
                    "average_performance": stats.get("performance_statistics", {}).get("average", 0),
                    "average_execution_time": stats.get("execution_time_statistics", {}).get("average", 0),
                    "average_memory_usage": stats.get("memory_statistics", {}).get("average", 0),
                    "optimization_distribution": stats.get("distributions", {}).get("by_optimization_level", {})
                }
            except Exception as e:
                print(f"âš ï¸ {language} í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                comparison_data[language] = {"error": str(e)}
        
        # ì „ì²´ ë¹„êµ ì¸ì‚¬ì´íŠ¸
        valid_data = {lang: data for lang, data in comparison_data.items() if "error" not in data and data.get("total_benchmarks", 0) > 0}
        
        insights = []
        if len(valid_data) > 1:
            # ì„±ëŠ¥ ìˆœìœ„
            performance_ranking = sorted(valid_data.items(), key=lambda x: x[1]["average_performance"], reverse=True)
            performance_text = ' > '.join([f'{lang}({data["average_performance"]:.1f})' for lang, data in performance_ranking])
            insights.append(f"ğŸ† ì„±ëŠ¥ ìˆœìœ„: {performance_text}")
            
            # ì‹¤í–‰ ì†ë„ ìˆœìœ„
            speed_ranking = sorted(valid_data.items(), key=lambda x: x[1]["average_execution_time"])
            speed_text = ' > '.join([f'{lang}({data["average_execution_time"]:.3f}s)' for lang, data in speed_ranking])
            insights.append(f"âš¡ ì†ë„ ìˆœìœ„: {speed_text}")
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ìˆœìœ„
            memory_ranking = sorted(valid_data.items(), key=lambda x: x[1]["average_memory_usage"])
            memory_text = ' > '.join([f'{lang}({data["average_memory_usage"]:.1f}MB)' for lang, data in memory_ranking])
            insights.append(f"ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {memory_text}")
        
        return {
            "language_comparison": comparison_data,
            "insights": insights,
            "summary": {
                "total_languages_analyzed": len(valid_data),
                "best_performance_language": performance_ranking[0][0] if len(valid_data) > 0 else None,
                "fastest_language": speed_ranking[0][0] if len(valid_data) > 0 else None,
                "most_memory_efficient": memory_ranking[0][0] if len(valid_data) > 0 else None
            }
        }
    
    def generate_performance_report(self, user_id: str = "default_user", 
                                  time_period_days: int = 30) -> Dict[str, Any]:
        """ì‚¬ìš©ìë³„ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        
        report = {
            "user_id": user_id,
            "report_period": f"ìµœê·¼ {time_period_days}ì¼",
            "generated_at": time.time()
        }
        
        try:
            # ì‚¬ìš©ì ì‹¤í–‰ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
            if self.history_enabled:
                user_history = self.history_manager.get_user_execution_history(user_id)
                
                if user_history:
                    # ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
                    performance_scores = []
                    execution_times = []
                    languages_used = set()
                    
                    for record in user_history:
                        if record.success and hasattr(record, 'performance_score'):
                            performance_scores.append(record.performance_score)
                            execution_times.append(record.execution_time)
                            languages_used.add(record.language)
                    
                    if performance_scores:
                        report["performance_trends"] = {
                            "average_score": sum(performance_scores) / len(performance_scores),
                            "best_score": max(performance_scores),
                            "improvement_trend": "ê°œì„ ë¨" if len(performance_scores) > 1 and performance_scores[-1] > performance_scores[0] else "ìœ ì§€",
                            "total_executions": len(performance_scores),
                            "languages_explored": list(languages_used)
                        }
                
                # ì„±ì¥ ì¶”ì  ì •ë³´
                growth_metrics = self.history_manager.get_user_growth_metrics(user_id)
                if growth_metrics:
                    report["growth_analysis"] = growth_metrics
            
            # ë²¤ì¹˜ë§ˆí¬ í†µê³„ (ì „ì²´ ëŒ€ë¹„ ì‚¬ìš©ì ìœ„ì¹˜)
            if self.benchmark_enabled:
                overall_stats = self.performance_benchmark.get_benchmark_statistics()
                report["comparative_analysis"] = {
                    "system_average": overall_stats.get("performance_statistics", {}).get("average", 0),
                    "user_vs_system": "ì‹œìŠ¤í…œ íˆìŠ¤í† ë¦¬ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¹„êµ ë¶ˆê°€"
                }
            
            # ì¶”ì²œì‚¬í•­
            report["recommendations"] = self._generate_user_recommendations(report)
            
        except Exception as e:
            report["error"] = f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"
        
        return report
    
    def _generate_user_recommendations(self, report: Dict[str, Any]) -> List[str]:
        """ì‚¬ìš©ìë³„ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        performance_trends = report.get("performance_trends", {})
        
        if performance_trends:
            avg_score = performance_trends.get("average_score", 0)
            
            if avg_score < 50:
                recommendations.append("ğŸ¯ ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ê³¼ ë°ì´í„° êµ¬ì¡° í•™ìŠµì— ì§‘ì¤‘í•˜ì„¸ìš”")
                recommendations.append("ğŸ“š ì½”ë”© í…ŒìŠ¤íŠ¸ ê¸°ì´ˆ ë¬¸ì œë¶€í„° ì°¨ê·¼ì°¨ê·¼ ì—°ìŠµí•˜ì„¸ìš”")
            elif avg_score < 70:
                recommendations.append("âš¡ ì‹œê°„ ë³µì¡ë„ë¥¼ ì˜ì‹í•œ ì½”ë”© ì—°ìŠµì„ ëŠ˜ë ¤ë³´ì„¸ìš”")
                recommendations.append("ğŸ” ì½”ë“œ ë¦¬ë·°ë¥¼ í†µí•´ ìµœì í™” í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ë³´ì„¸ìš”")
            elif avg_score < 90:
                recommendations.append("ğŸŒŸ ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ê³¼ ìµœì í™” ê¸°ë²•ì„ í•™ìŠµí•´ë³´ì„¸ìš”")
                recommendations.append("ğŸ’¡ ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•´ë³´ì„¸ìš”")
            else:
                recommendations.append("ğŸ† í›Œë¥­í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤! ë‹¤ë¥¸ ê°œë°œìë“¤ì—ê²Œ ì§€ì‹ì„ ê³µìœ í•´ë³´ì„¸ìš”")
                recommendations.append("ğŸš€ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ ê¸°ì—¬ë‚˜ ì•Œê³ ë¦¬ì¦˜ ê²½ì§„ëŒ€íšŒ ì°¸ì—¬ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”")
            
            languages_count = len(performance_trends.get("languages_explored", []))
            if languages_count < 3:
                recommendations.append(f"ğŸŒˆ ë‹¤ì–‘í•œ ì–¸ì–´ ê²½í—˜ì„ ìœ„í•´ ìƒˆë¡œìš´ ì–¸ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš” (í˜„ì¬: {languages_count}ê°œ ì–¸ì–´)")
        
        return recommendations

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (í•„ìš”ì‹œ ì‚¬ìš©)
enhanced_performance_executor = None

def get_enhanced_performance_executor(ai_handler=None):
    """í•„ìš”ì‹œ EnhancedPerformanceExecutor ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜"""
    global enhanced_performance_executor
    if enhanced_performance_executor is None and MODULES_AVAILABLE:
        try:
            enhanced_performance_executor = EnhancedPerformanceExecutor(ai_handler)
        except Exception as e:
            print(f"âš ï¸ EnhancedPerformanceExecutor ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    return enhanced_performance_executor