"""
íŒœì†”ë¼ AI_Solarbot - ì§€ëŠ¥í˜• ì›¹ ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ
AI ê¸°ë°˜ ì›¹ ì½˜í…ì¸  ë¶„ì„, ìš”ì•½, ê°ì • ë¶„ì„, í’ˆì§ˆ í‰ê°€ ê¸°ëŠ¥ ì œê³µ
"""

import os
import asyncio
import aiohttp
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict, field
from cachetools import TTLCache
from loguru import logger
from urllib.parse import urlparse, urljoin
import hashlib
from bs4 import BeautifulSoup
import logging

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ê¸°ì¡´ ì‹œìŠ¤í…œ ëª¨ë“ˆ import
from src.ai_handler import AIHandler
from src.web_search_ide import WebSearchIDE
from src.google_drive_handler import GoogleDriveHandler

@dataclass
class ContentAnalysisResult:
    """ì½˜í…ì¸  ë¶„ì„ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    url: str
    title: str
    content_type: str  # news, blog, technical, academic, social, commercial, other
    summary: str
    key_points: List[str]
    sentiment_score: float  # -1.0 (ë§¤ìš° ë¶€ì •) ~ 1.0 (ë§¤ìš° ê¸ì •)
    sentiment_label: str  # positive, negative, neutral
    quality_score: float  # 0-100 ì ìˆ˜
    topics: List[str]
    language: str
    reading_time: int  # ì˜ˆìƒ ì½ê¸° ì‹œê°„ (ë¶„)
    complexity_level: str  # beginner, intermediate, advanced, expert
    analysis_timestamp: str
    ai_model_used: str
    word_count: int = 0
    image_count: int = 0
    link_count: int = 0
    error_message: str = ""
    
    # =================== 4ë‹¨ê³„ ì¶”ê°€: ê³ ê¸‰ ê°ì • ë¶„ì„ í•„ë“œ ===================
    detailed_emotions: Dict[str, float] = field(default_factory=dict)  # ì„¸ë¶„í™”ëœ ê°ì • ì ìˆ˜
    emotion_intensity: float = 0.0  # ê°ì • ê°•ë„ (0.0-1.0)
    emotion_confidence: float = 0.0  # ê°ì • ë¶„ì„ ì‹ ë¢°ë„ (0.0-1.0)
    dominant_emotion: str = ""  # ì£¼ìš” ê°ì •
    emotion_distribution: Dict[str, float] = field(default_factory=dict)  # ê°ì • ë¶„í¬
    contextual_sentiment: str = ""  # ë§¥ë½ì  ê°ì • (ì‚¬íšŒì , ê°œì¸ì  ë“±)
    
    # =================== 4ë‹¨ê³„ ì¶”ê°€: ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ í•„ë“œ ===================
    quality_dimensions: Dict[str, float] = field(default_factory=dict)  # ë‹¤ì°¨ì› í’ˆì§ˆ ì ìˆ˜
    quality_report: str = ""  # í’ˆì§ˆ í‰ê°€ ë¦¬í¬íŠ¸
    improvement_suggestions: List[str] = field(default_factory=list)  # ê°œì„  ì œì•ˆì‚¬í•­
    content_reliability: float = 0.0  # ì‹ ë¢°ë„ ì ìˆ˜
    content_usefulness: float = 0.0  # ìœ ìš©ì„± ì ìˆ˜
    content_accuracy: float = 0.0  # ì •í™•ì„± ì ìˆ˜

@dataclass
class BatchAnalysisReport:
    """ë°°ì¹˜ ë¶„ì„ ë¦¬í¬íŠ¸ ë°ì´í„° í´ë˜ìŠ¤"""
    total_analyzed: int
    success_count: int
    failed_count: int
    average_quality_score: float
    dominant_sentiment: str
    top_topics: List[Tuple[str, int]]  # (topic, count)
    content_type_distribution: Dict[str, int]
    analysis_summary: str
    processing_time: float
    report_timestamp: str

class IntelligentContentAnalyzer:
    """ì§€ëŠ¥í˜• ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™” - ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì—°ë™"""
        try:
            # ê¸°ì¡´ ëª¨ë“ˆ ì—°ë™
            self.ai_handler = AIHandler()
            self.web_search = WebSearchIDE()
            self.drive_handler = GoogleDriveHandler()
            
            # ìºì‹œ ì„¤ì • (2ì‹œê°„ TTL)
            self.analysis_cache = TTLCache(maxsize=500, ttl=7200)
            self.batch_cache = TTLCache(maxsize=100, ttl=3600)
            
            # ë¶„ì„ í…œí”Œë¦¿ ì„¤ì • (í™•ì¥ëœ ì½˜í…ì¸  íƒ€ì…)
            self.content_types = {
                'news': ['ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ë³´ë„', 'ì–¸ë¡ ', 'ì‹ ë¬¸', 'ì†ë³´', 'ì·¨ì¬'],
                'blog': ['ë¸”ë¡œê·¸', 'í¬ìŠ¤íŠ¸', 'ê°œì¸', 'ì¼ê¸°', 'í›„ê¸°', 'ë¦¬ë·°', 'ê²½í—˜'],
                'tech_doc': ['ê¸°ìˆ ', 'ê°œë°œ', 'í”„ë¡œê·¸ë˜ë°', 'API', 'ë¬¸ì„œ', 'ê°€ì´ë“œ', 'ë ˆí¼ëŸ°ìŠ¤'],
                'academic': ['ë…¼ë¬¸', 'ì—°êµ¬', 'í•™ìˆ ', 'ì €ë„', 'í•™íšŒ', 'ì‹¤í—˜', 'ë¶„ì„'],
                'tutorial': ['íŠœí† ë¦¬ì–¼', 'ë”°ë¼í•˜ê¸°', 'ë‹¨ê³„', 'ë°©ë²•', 'ë°°ìš°ê¸°', 'ìµíˆê¸°', 'ì„¤ëª…'],
                'commercial': ['ìƒí’ˆ', 'íŒë§¤', 'ë§ˆì¼€íŒ…', 'ê´‘ê³ ', 'ì‡¼í•‘', 'êµ¬ë§¤', 'í• ì¸'],
                'social': ['SNS', 'ì†Œì…œ', 'ì»¤ë®¤ë‹ˆí‹°', 'í¬ëŸ¼', 'ëŒ“ê¸€', 'í† ë¡ '],
                'general': ['ì¼ë°˜', 'ê¸°ë³¸', 'ì •ë³´', 'ë‚´ìš©', 'ê¸€', 'í…ìŠ¤íŠ¸']
            }
            
            # ê°ì • ë¶„ì„ í‚¤ì›Œë“œ
            self.sentiment_keywords = {
                'positive': ['ì¢‹ì€', 'í›Œë¥­í•œ', 'ì™„ë²½í•œ', 'ì„±ê³µ', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'ìš°ìˆ˜í•œ'],
                'negative': ['ë‚˜ìœ', 'ì‹¤ë§', 'ë¬¸ì œ', 'ì˜¤ë¥˜', 'ì‹¤íŒ¨', 'ë¶ˆë§Œ', 'ìµœì•…', 'ë¶€ì¡±í•œ'],
                'neutral': ['ë³´í†µ', 'ì¼ë°˜ì ì¸', 'í‰ë²”í•œ', 'í‘œì¤€', 'ê¸°ë³¸']
            }
            
            # =================== 4ë‹¨ê³„ ì¶”ê°€: í™•ì¥ëœ ê°ì • ë¶„ì„ í‚¤ì›Œë“œ ===================
            # ì„¸ë¶„í™”ëœ ê°ì • ì¹´í…Œê³ ë¦¬ (Plutchikì˜ ê°ì • ëª¨ë¸ ê¸°ë°˜)
            self.detailed_emotion_keywords = {
                'joy': {  # ê¸°ì¨
                    'korean': ['ê¸°ìœ', 'í–‰ë³µí•œ', 'ì¦ê±°ìš´', 'ì‹ ë‚˜ëŠ”', 'ìœ ì¾Œí•œ', 'í¬ë§ì§„ì§„í•œ', 'í™˜ìƒì ì¸', 
                              'ë†€ë¼ìš´', 'ë©‹ì§„', 'ì•„ë¦„ë‹¤ìš´', 'ì‚¬ë‘ìŠ¤ëŸ¬ìš´', 'ê°ë™ì ì¸', 'í¬ë§ì ì¸'],
                    'english': ['happy', 'joyful', 'excited', 'amazing', 'wonderful', 'fantastic', 
                               'delighted', 'cheerful', 'optimistic', 'thrilled'],
                    'weight': 1.0
                },
                'anger': {  # ë¶„ë…¸
                    'korean': ['í™”ë‚˜ëŠ”', 'ì§œì¦ë‚˜ëŠ”', 'ë¶„ë…¸í•œ', 'ì—´ë°›ëŠ”', 'ì–µìš¸í•œ', 'ë¶ˆê³µí‰í•œ', 'ì•…ì§ˆì ì¸',
                              'ë¯¸ì¹œ', 'ì–´ì´ì—†ëŠ”', 'í™©ë‹¹í•œ', 'ë‹µë‹µí•œ', 'ë¹¡ì¹˜ëŠ”'],
                    'english': ['angry', 'furious', 'mad', 'irritated', 'frustrated', 'outraged',
                               'annoyed', 'infuriated', 'enraged'],
                    'weight': 1.2
                },
                'sadness': {  # ìŠ¬í””
                    'korean': ['ìŠ¬í”ˆ', 'ìš°ìš¸í•œ', 'ëˆˆë¬¼ë‚˜ëŠ”', 'ì•ˆíƒ€ê¹Œìš´', 'ì“¸ì“¸í•œ', 'ì™¸ë¡œìš´', 'í—ˆë¬´í•œ',
                              'ì ˆë§ì ì¸', 'ë¹„ì°¸í•œ', 'ì²˜ì°¸í•œ', 'ë§ˆìŒì•„í”ˆ', 'ê°€ìŠ´ì•„í”ˆ'],
                    'english': ['sad', 'depressed', 'melancholy', 'gloomy', 'sorrowful', 'tragic',
                               'heartbroken', 'miserable', 'dejected'],
                    'weight': 1.1
                },
                'fear': {  # ë‘ë ¤ì›€
                    'korean': ['ë¬´ì„œìš´', 'ë‘ë ¤ìš´', 'ë¶ˆì•ˆí•œ', 'ê±±ì •ë˜ëŠ”', 'ìœ„í—˜í•œ', 'ê²ë‚˜ëŠ”', 'ë–¨ë¦¬ëŠ”',
                              'ê¸´ì¥ë˜ëŠ”', 'ì¡°ë§ˆì¡°ë§ˆí•œ', 'ì‹¬ê°í•œ', 'ìœ„ê¸°ì ì¸'],
                    'english': ['scary', 'frightening', 'anxious', 'worried', 'nervous', 'terrifying',
                               'alarming', 'threatening', 'dangerous'],
                    'weight': 1.1
                },
                'surprise': {  # ë†€ëŒ
                    'korean': ['ë†€ë¼ìš´', 'ê¹œì§', 'ì˜ˆìƒì™¸ì˜', 'ëœ»ë°–ì˜', 'ì‹ ê¸°í•œ', 'íŠ¹ì´í•œ', 'ì´ìƒí•œ',
                              'ì¶©ê²©ì ì¸', 'ë°˜ì „', 'ì˜ì™¸ì˜', 'ì˜ˆì¸¡ë¶ˆê°€í•œ'],
                    'english': ['surprising', 'shocking', 'unexpected', 'astonishing', 'amazing',
                               'incredible', 'unbelievable', 'stunning'],
                    'weight': 0.9
                },
                'disgust': {  # í˜ì˜¤
                    'korean': ['ì—­ê²¨ìš´', 'ì§•ê·¸ëŸ¬ìš´', 'ë”ëŸ¬ìš´', 'í˜ì˜¤ìŠ¤ëŸ¬ìš´', 'êµ¬ì—­ì§ˆë‚˜ëŠ”', 'ë”ì°í•œ',
                              'ì§€ê¸‹ì§€ê¸‹í•œ', 'ì‹«ì€', 'ë¶ˆì¾Œí•œ', 'ê±°ë¶€ê°ë“œëŠ”'],
                    'english': ['disgusting', 'revolting', 'repulsive', 'gross', 'awful', 'terrible',
                               'horrible', 'nasty', 'offensive'],
                    'weight': 1.1
                },
                'trust': {  # ì‹ ë¢°
                    'korean': ['ë¯¿ì„ë§Œí•œ', 'ì‹ ë¢°í• ë§Œí•œ', 'í™•ì‹¤í•œ', 'ì•ˆì „í•œ', 'ë“ ë“ í•œ', 'ì˜ì§€ê°€ë˜ëŠ”',
                              'ê²€ì¦ëœ', 'ë³´ì¥ëœ', 'í™•ì‹ í•˜ëŠ”', 'ì‹ ìš©ìˆëŠ”'],
                    'english': ['trustworthy', 'reliable', 'credible', 'dependable', 'secure',
                               'confident', 'certain', 'guaranteed'],
                    'weight': 1.0
                },
                'anticipation': {  # ê¸°ëŒ€
                    'korean': ['ê¸°ëŒ€ë˜ëŠ”', 'ê¸°ë‹¤ë ¤ì§€ëŠ”', 'ì„¤ë ˆëŠ”', 'ê¶ê¸ˆí•œ', 'í¥ë¯¸ë¡œìš´', 'ê´€ì‹¬ìˆëŠ”',
                              'ê³ ëŒ€í•˜ëŠ”', 'ë°”ë¼ëŠ”', 'í¬ë§í•˜ëŠ”', 'ì˜ˆìƒí•˜ëŠ”'],
                    'english': ['anticipated', 'expected', 'exciting', 'interesting', 'hopeful',
                               'eager', 'looking forward', 'awaiting'],
                    'weight': 0.8
                }
            }
            
            # ê°ì • ê°•ë„ ì§€ì‹œì–´
            self.emotion_intensity_modifiers = {
                'high': ['ë§¤ìš°', 'ì •ë§', 'ì§„ì§œ', 'ì™„ì „', 'ë„ˆë¬´', 'ì—„ì²­', 'ê·¹ë„ë¡œ', 'ìµœê³ ë¡œ', 'absolutely', 'extremely', 'incredibly', 'totally'],
                'medium': ['ê½¤', 'ìƒë‹¹íˆ', 'ì œë²•', 'ì–´ëŠì •ë„', 'quite', 'fairly', 'rather', 'somewhat'],
                'low': ['ì¡°ê¸ˆ', 'ì•½ê°„', 'ì‚´ì§', 'ë‹¤ì†Œ', 'slightly', 'a bit', 'somewhat', 'little']
            }
            
            # ë§¥ë½ì  ê°ì • íŒ¨í„´
            self.contextual_patterns = {
                'sarcasm': [r'ì •ë§\s*ì¢‹ë„¤ìš”', r'ì™„ì „\s*ëŒ€ë°•ì´ë„¤ìš”', r'ì—­ì‹œ\s*ìµœê³ ë„¤ìš”', r'ì°¸\s*ì˜í–ˆë„¤ìš”'],
                'irony': [r'ê·¸ëŸ¼\s*ê·¸ë ‡ì§€', r'ë‹¹ì—°íˆ\s*ê·¸ë ‡ê² ì£ ', r'ì˜ˆìƒëŒ€ë¡œë„¤ìš”'],
                'emphasis': [r'!{2,}', r'[ã…‹ã…]{3,}', r'[ã… ã…œ]{2,}', r'[.]{3,}']
            }
            
            # í•œêµ­ì–´ íŠ¹í™” ê°ì • í‘œí˜„
            self.korean_emotion_expressions = {
                'positive_slang': ['ëŒ€ë°•', 'ì©”ì–´', 'êµ¿', 'ì§±', 'í‚¹ì™•ì§±', 'ê°œì¢‹ì•„', 'ë ˆì „ë“œ'],
                'negative_slang': ['ë³„ë¡œ', 'ë…¸ë‹µ', 'í—¬', 'ë§í•¨', 'ì“°ë ˆê¸°', 'ê°œë³„ë¡œ', 'ìµœì•…'],
                'neutral_slang': ['ê·¸ëƒ¥', 'ë­', 'ë³„ê±°ì—†ìŒ', 'í‰ë²”', 'ë¬´ë‚œ']
            }
            
            # =================== 4ë‹¨ê³„ ì¶”ê°€: ë‹¤ì°¨ì› í’ˆì§ˆ í‰ê°€ ì„¤ì • ===================
            # 6ê°œ í’ˆì§ˆ ì°¨ì› ì •ì˜
            self.quality_dimensions = {
                'reliability': {  # ì‹ ë¢°ë„
                    'name': 'ì‹ ë¢°ë„',
                    'description': 'ì •ë³´ì˜ ì •í™•ì„±ê³¼ ì¶œì²˜ì˜ ì‹ ë¢°ì„±',
                    'weight': 1.2,
                    'indicators': {
                        'source_credibility': ['edu', 'gov', 'org', 'ac.kr', 'go.kr'],
                        'author_expertise': ['ë°•ì‚¬', 'êµìˆ˜', 'ì „ë¬¸ê°€', 'ì—°êµ¬ì›', 'PhD', 'Dr.'],
                        'citation_quality': ['ì°¸ê³ ë¬¸í—Œ', 'ì¶œì²˜', 'ì¸ìš©', 'reference', 'citation'],
                        'fact_checking': ['ê²€ì¦', 'í™•ì¸', 'ì‚¬ì‹¤', 'ë°ì´í„°', 'í†µê³„']
                    }
                },
                'usefulness': {  # ìœ ìš©ì„±
                    'name': 'ìœ ìš©ì„±',
                    'description': 'ë…ìì—ê²Œ ì‹¤ì§ˆì ì¸ ë„ì›€ì´ ë˜ëŠ” ì •ë„',
                    'weight': 1.1,
                    'indicators': {
                        'practical_value': ['ë°©ë²•', 'í•´ê²°', 'íŒ', 'ê°€ì´ë“œ', 'ë‹¨ê³„', 'how-to', 'tutorial'],
                        'actionable_content': ['ì‹¤í–‰', 'ì ìš©', 'í™œìš©', 'êµ¬í˜„', 'ì‹¤ì œ', 'action', 'implement'],
                        'problem_solving': ['ë¬¸ì œ', 'í•´ê²°ì±…', 'ì†”ë£¨ì…˜', 'ëŒ€ì•ˆ', 'solution', 'fix'],
                        'learning_value': ['ë°°ìš°ê¸°', 'ìµíˆê¸°', 'ì´í•´', 'í•™ìŠµ', 'learn', 'understand']
                    }
                },
                'accuracy': {  # ì •í™•ì„±
                    'name': 'ì •í™•ì„±',
                    'description': 'ë‚´ìš©ì˜ ì •í™•ì„±ê³¼ ì˜¤ë¥˜ ì—†ìŒ',
                    'weight': 1.3,
                    'indicators': {
                        'technical_precision': ['ì •í™•í•œ', 'ì •ë°€í•œ', 'ì—„ë°€í•œ', 'accurate', 'precise'],
                        'error_indicators': ['ì˜¤ë¥˜', 'í‹€ë¦°', 'ì˜ëª»ëœ', 'ë¶€ì •í™•í•œ', 'error', 'wrong', 'incorrect'],
                        'verification_marks': ['ê²€ì¦ë¨', 'í™•ì¸ë¨', 'í…ŒìŠ¤íŠ¸ë¨', 'verified', 'tested'],
                        'update_frequency': ['ìµœì‹ ', 'ì—…ë°ì´íŠ¸', 'ê°±ì‹ ', 'updated', 'latest', 'current']
                    }
                },
                'completeness': {  # ì™„ì„±ë„
                    'name': 'ì™„ì„±ë„',
                    'description': 'ë‚´ìš©ì˜ ì™„ì „ì„±ê³¼ í¬ê´„ì„±',
                    'weight': 1.0,
                    'indicators': {
                        'comprehensive_coverage': ['ì „ì²´', 'ì™„ì „í•œ', 'í¬ê´„ì ', 'ì¢…í•©ì ', 'comprehensive', 'complete'],
                        'detailed_explanation': ['ìì„¸í•œ', 'ìƒì„¸í•œ', 'êµ¬ì²´ì ', 'detailed', 'specific'],
                        'example_provision': ['ì˜ˆì œ', 'ì‚¬ë¡€', 'ì‹¤ë¡€', 'example', 'case study'],
                        'step_by_step': ['ë‹¨ê³„ë³„', 'ìˆœì„œ', 'ì ˆì°¨', 'step-by-step', 'procedure']
                    }
                },
                'readability': {  # ê°€ë…ì„±
                    'name': 'ê°€ë…ì„±',
                    'description': 'ì½ê¸° ì‰½ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì •ë„',
                    'weight': 0.9,
                    'indicators': {
                        'clear_structure': ['ëª©ì°¨', 'ì œëª©', 'ì†Œì œëª©', 'êµ¬ì¡°', 'heading', 'structure'],
                        'simple_language': ['ì‰¬ìš´', 'ê°„ë‹¨í•œ', 'ëª…í™•í•œ', 'simple', 'clear', 'easy'],
                        'visual_aids': ['ê·¸ë¦¼', 'ë„í‘œ', 'ì°¨íŠ¸', 'ì´ë¯¸ì§€', 'image', 'chart', 'diagram'],
                        'formatting': ['ëª©ë¡', 'ë²ˆí˜¸', 'ê°•ì¡°', 'list', 'bullet', 'bold', 'highlight']
                    }
                },
                'originality': {  # ë…ì°½ì„±
                    'name': 'ë…ì°½ì„±',
                    'description': 'ë…ì°½ì ì´ê³  ìƒˆë¡œìš´ ê´€ì ì˜ ì œê³µ',
                    'weight': 0.8,
                    'indicators': {
                        'unique_perspective': ['ìƒˆë¡œìš´', 'ë…íŠ¹í•œ', 'ë…ì°½ì ', 'í˜ì‹ ì ', 'unique', 'innovative', 'novel'],
                        'personal_insight': ['ê°œì¸ì ', 'ê²½í—˜', 'ì¸ì‚¬ì´íŠ¸', 'í†µì°°', 'insight', 'experience'],
                        'creative_approach': ['ì°½ì˜ì ', 'ì°½ì¡°ì ', 'ì°¸ì‹ í•œ', 'creative', 'original'],
                        'thought_provoking': ['ìƒê°í•´ë³¼', 'ê³ ë¯¼', 'ì„±ì°°', 'thought-provoking', 'reflection']
                    }
                }
            }
            
            # ì½˜í…ì¸  íƒ€ì…ë³„ í’ˆì§ˆ ê°€ì¤‘ì¹˜
            self.content_type_quality_weights = {
                'news': {
                    'reliability': 1.5, 'accuracy': 1.4, 'usefulness': 1.0, 
                    'completeness': 1.1, 'readability': 1.0, 'originality': 0.7
                },
                'blog': {
                    'reliability': 0.9, 'accuracy': 1.0, 'usefulness': 1.2, 
                    'completeness': 0.9, 'readability': 1.3, 'originality': 1.4
                },
                'tech_doc': {
                    'reliability': 1.3, 'accuracy': 1.5, 'usefulness': 1.4, 
                    'completeness': 1.3, 'readability': 1.2, 'originality': 0.8
                },
                'academic': {
                    'reliability': 1.6, 'accuracy': 1.5, 'usefulness': 1.1, 
                    'completeness': 1.4, 'readability': 0.9, 'originality': 1.2
                },
                'tutorial': {
                    'reliability': 1.1, 'accuracy': 1.3, 'usefulness': 1.5, 
                    'completeness': 1.4, 'readability': 1.4, 'originality': 0.9
                },
                'commercial': {
                    'reliability': 0.8, 'accuracy': 1.0, 'usefulness': 1.3, 
                    'completeness': 1.0, 'readability': 1.2, 'originality': 1.1
                },
                'general': {
                    'reliability': 1.0, 'accuracy': 1.0, 'usefulness': 1.0, 
                    'completeness': 1.0, 'readability': 1.0, 'originality': 1.0
                }
            }
            
            # ì–¸ì–´ í’ˆì§ˆ í‰ê°€ ì§€í‘œ
            self.language_quality_indicators = {
                'grammar_errors': [
                    r'ì´/ê°€\s+ì´/ê°€',  # ì¡°ì‚¬ ì¤‘ë³µ
                    r'ì„/ë¥¼\s+ì„/ë¥¼',  # ì¡°ì‚¬ ì¤‘ë³µ
                    r'í•œë‹¤ê³ \s+í•œë‹¤',  # ì–´ë¯¸ ì¤‘ë³µ
                    r'ìˆë‹¤\s+ìˆë‹¤',   # ë™ì‚¬ ì¤‘ë³µ
                ],
                'spelling_errors': [
                    r'ë¬ë‹¤',  # ëë‹¤
                    r'ì•Šë¨',  # ì•ˆë¨
                    r'ë˜ì—¬',  # ë˜ì–´
                    r'ì–´ë–»í•´',  # ì–´ë–»ê²Œ
                ],
                'good_expressions': [
                    r'ë”°ë¼ì„œ', r'ê·¸ëŸ¬ë¯€ë¡œ', r'ê²°ê³¼ì ìœ¼ë¡œ',  # ë…¼ë¦¬ì  ì—°ê²°
                    r'ì˜ˆë¥¼\s*ë“¤ì–´', r'êµ¬ì²´ì ìœ¼ë¡œ', r'ì‹¤ì œë¡œ',  # êµ¬ì²´í™”
                    r'ë°˜ë©´ì—', r'í•œí¸', r'ê·¸ëŸ¬ë‚˜',  # ëŒ€ì¡°
                    r'ì²«ì§¸', r'ë‘˜ì§¸', r'ë§ˆì§€ë§‰ìœ¼ë¡œ'  # ìˆœì„œ
                ]
            }
            
            # ë³µì¡ë„ íŒë‹¨ ê¸°ì¤€
            self.complexity_indicators = {
                'beginner': ['ê¸°ì´ˆ', 'ì…ë¬¸', 'ì´ˆë³´', 'ì‹œì‘', 'ì²˜ìŒ'],
                'intermediate': ['ì¤‘ê¸‰', 'ì¼ë°˜', 'ì‹¤ë¬´', 'í™œìš©'],
                'advanced': ['ê³ ê¸‰', 'ì‹¬í™”', 'ì „ë¬¸', 'ì‘ìš©'],
                'expert': ['ì „ë¬¸ê°€', 'ë§ˆìŠ¤í„°', 'ê³ ìˆ˜', 'ì „ë¬¸']
            }
            
            # ë¶„ì„ í†µê³„
            self.analysis_stats = {
                'total_analyzed': 0,
                'successful_analyses': 0,
                'failed_analyses': 0,
                'average_processing_time': 0.0,
                'most_common_content_type': '',
                'average_quality_score': 0.0
            }
            
            logger.info("IntelligentContentAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"IntelligentContentAnalyzer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _generate_cache_key(self, url: str, analysis_type: str = "full") -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{url}_{analysis_type}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _detect_content_type(self, title: str, content: str, url: str, html_soup=None) -> str:
        """ê³ ê¸‰ ì½˜í…ì¸  íƒ€ì… ê°ì§€ - web_search_ide.py íŒ¨í„´ í™•ì¥"""
        try:
            text = f"{title} {content}".lower()
            domain = urlparse(url).netloc.lower()
            
            # 1. ë„ë©”ì¸ ê¸°ë°˜ ì •í™•í•œ ë¶„ë¥˜ (web_search_ide íŒ¨í„´ í™•ì¥)
            domain_types = {
                # ë‰´ìŠ¤ ì‚¬ì´íŠ¸
                'news': ['news', 'naver.com', 'daum.net', 'chosun.com', 'joongang.co.kr', 
                        'donga.com', 'hani.co.kr', 'ytn.co.kr', 'sbs.co.kr', 'kbs.co.kr'],
                
                # ê¸°ìˆ  ë¬¸ì„œ ë° ê°œë°œ
                'tech_doc': ['github.com', 'stackoverflow.com', 'dev.to', 'docs.python.org',
                           'developer.mozilla.org', 'reactjs.org', 'nodejs.org', 'django.com'],
                
                # ë¸”ë¡œê·¸ í”Œë«í¼
                'blog': ['medium.com', 'tistory.com', 'wordpress', 'blogger.com', 'velog.io',
                        'brunch.co.kr', 'steemit.com'],
                
                # í•™ìˆ  ë° ì—°êµ¬
                'academic': ['arxiv.org', 'scholar.google', 'researchgate.net', 'ieee.org',
                           'acm.org', 'springer.com', 'sciencedirect.com'],
                
                # íŠœí† ë¦¬ì–¼ ë° êµìœ¡
                'tutorial': ['codecademy.com', 'freecodecamp.org', 'w3schools.com', 
                           'tutorialspoint.com', 'coursera.org', 'udemy.com'],
                
                # ìƒì—…ì  ì‚¬ì´íŠ¸
                'commercial': ['amazon.com', 'ebay.com', 'coupang.com', '11st.co.kr',
                             'gmarket.co.kr', 'interpark.com']
            }
            
            # ë„ë©”ì¸ ë§¤ì¹­ ê²€ì‚¬
            for content_type, domains in domain_types.items():
                if any(domain_name in domain for domain_name in domains):
                    return content_type
            
            # 2. URL ê²½ë¡œ ë¶„ì„
            url_path = urlparse(url).path.lower()
            if any(path_indicator in url_path for path_indicator in ['/blog/', '/post/', '/article/']):
                return 'blog'
            elif any(path_indicator in url_path for path_indicator in ['/docs/', '/documentation/', '/api/']):
                return 'tech_doc'
            elif any(path_indicator in url_path for path_indicator in ['/tutorial/', '/guide/', '/how-to/']):
                return 'tutorial'
            elif any(path_indicator in url_path for path_indicator in ['/news/', '/press/', '/article/']):
                return 'news'
            
            # 3. ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„ (HTML ë©”íƒ€ë°ì´í„° í™œìš©)
            if html_soup:
                # ë©”íƒ€ íƒœê·¸ ë¶„ì„
                meta_tags = html_soup.find_all('meta')
                for meta in meta_tags:
                    content_attr = meta.get('content', '').lower()
                    name_attr = meta.get('name', '').lower()
                    property_attr = meta.get('property', '').lower()
                    
                    if 'article' in property_attr or 'news' in content_attr:
                        return 'news'
                    elif 'blog' in content_attr or 'blog' in name_attr:
                        return 'blog'
                    elif 'documentation' in content_attr:
                        return 'tech_doc'
                
                # ìŠ¤í‚¤ë§ˆ ë§ˆí¬ì—… ë¶„ì„
                schema_elements = html_soup.find_all(attrs={"itemtype": True})
                for element in schema_elements:
                    itemtype = element.get('itemtype', '').lower()
                    if 'newsarticle' in itemtype:
                        return 'news'
                    elif 'blogarticle' in itemtype:
                        return 'blog'
                    elif 'technicalarticle' in itemtype:
                        return 'tech_doc'
            
            # 4. ê³ ê¸‰ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
            advanced_content_patterns = {
                'news': {
                    'keywords': ['ê¸°ì', 'ë‰´ìŠ¤', 'ë³´ë„', 'ì·¨ì¬', 'ì–¸ë¡ ', 'ì‹ ë¬¸', 'ë°©ì†¡', 'ì†ë³´'],
                    'patterns': [r'\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼', r'ê¸°ì\s*=', r'ë‰´ìŠ¤\s*\|'],
                    'weight': 2.0
                },
                'tech_doc': {
                    'keywords': ['API', 'í•¨ìˆ˜', 'ë©”ì„œë“œ', 'í´ë˜ìŠ¤', 'ë¼ì´ë¸ŒëŸ¬ë¦¬', 'í”„ë ˆì„ì›Œí¬', 
                               'ì½”ë“œ', 'ì˜ˆì œ', 'import', 'function', 'class', 'def'],
                    'patterns': [r'```\w*\n', r'<code>', r'def\s+\w+\(', r'function\s+\w+'],
                    'weight': 2.5
                },
                'blog': {
                    'keywords': ['ê°œì¸ì ìœ¼ë¡œ', 'ìƒê°í•´ë³´ë‹ˆ', 'í›„ê¸°', 'ë¦¬ë·°', 'ê²½í—˜', 'ëŠë‚Œ', 
                               'ì¶”ì²œ', 'ê°œì¸', 'ì¼ìƒ', 'ë¸”ë¡œê·¸'],
                    'patterns': [r'ì•ˆë…•í•˜ì„¸ìš”', r'ì˜¤ëŠ˜ì€', r'ê°œì¸ì ìœ¼ë¡œ'],
                    'weight': 1.5
                },
                'academic': {
                    'keywords': ['ë…¼ë¬¸', 'ì—°êµ¬', 'í•™ìˆ ', 'ì €ë„', 'í•™íšŒ', 'ì°¸ê³ ë¬¸í—Œ', 'ì¸ìš©',
                               'ì‹¤í—˜', 'ë¶„ì„', 'ê²°ê³¼', 'abstract', 'introduction', 'conclusion'],
                    'patterns': [r'\[\d+\]', r'et al\.', r'Abstract:', r'References:'],
                    'weight': 3.0
                },
                'tutorial': {
                    'keywords': ['ë‹¨ê³„', 'ë”°ë¼í•˜ê¸°', 'íŠœí† ë¦¬ì–¼', 'ê°€ì´ë“œ', 'ë°©ë²•', 'ì„¤ëª…',
                               'ì²˜ìŒ', 'ì‹œì‘', 'ë°°ìš°ê¸°', 'ìµíˆê¸°'],
                    'patterns': [r'1\.\s', r'ì²«\s*ë²ˆì§¸', r'ë‹¨ê³„\s*\d+', r'Step\s*\d+'],
                    'weight': 2.0
                },
                'commercial': {
                    'keywords': ['êµ¬ë§¤', 'íŒë§¤', 'ê°€ê²©', 'í• ì¸', 'ë°°ì†¡', 'ìƒí’ˆ', 'ì£¼ë¬¸',
                               'ê²°ì œ', 'ì‡¼í•‘', 'ë§ˆì¼€íŒ…'],
                    'patterns': [r'\d+ì›', r'\$\d+', r'í• ì¸\s*\d+%', r'ë¬´ë£Œ\s*ë°°ì†¡'],
                    'weight': 1.8
                }
            }
            
            # ê° íƒ€ì…ë³„ ì ìˆ˜ ê³„ì‚°
            type_scores = {}
            for content_type, pattern_data in advanced_content_patterns.items():
                score = 0
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                keyword_matches = sum(1 for keyword in pattern_data['keywords'] if keyword in text)
                score += keyword_matches * pattern_data['weight']
                
                # ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­
                pattern_matches = 0
                for pattern in pattern_data['patterns']:
                    if re.search(pattern, content, re.IGNORECASE):
                        pattern_matches += 1
                score += pattern_matches * pattern_data['weight'] * 1.5
                
                type_scores[content_type] = score
            
            # 5. ìµœì¢… ê²°ì •
            if type_scores:
                max_score = max(type_scores.values())
                if max_score > 2.0:  # ì„ê³„ê°’ ì„¤ì •
                    return max(type_scores, key=type_scores.get)
            
            # 6. ê¸°ë³¸ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (fallback)
            for content_type, keywords in self.content_types.items():
                score = sum(1 for keyword in keywords if keyword in text)
                if score >= 2:  # ìµœì†Œ 2ê°œ í‚¤ì›Œë“œ ë§¤ì¹­
                    return content_type
            
            return 'general'
            
        except Exception as e:
            logger.error(f"ì½˜í…ì¸  íƒ€ì… ê°ì§€ ì˜¤ë¥˜: {e}")
            return 'general'
    
    def _calculate_sentiment_score(self, text: str) -> Tuple[float, str]:
        """ê°ì • ì ìˆ˜ ê³„ì‚°"""
        try:
            text_lower = text.lower()
            
            positive_count = sum(1 for word in self.sentiment_keywords['positive'] if word in text_lower)
            negative_count = sum(1 for word in self.sentiment_keywords['negative'] if word in text_lower)
            neutral_count = sum(1 for word in self.sentiment_keywords['neutral'] if word in text_lower)
            
            total_sentiment_words = positive_count + negative_count + neutral_count
            
            if total_sentiment_words == 0:
                return 0.0, 'neutral'
            
            # ê°ì • ì ìˆ˜ ê³„ì‚° (-1.0 ~ 1.0)
            sentiment_score = (positive_count - negative_count) / total_sentiment_words
            
            # ë¼ë²¨ ê²°ì •
            if sentiment_score > 0.2:
                label = 'positive'
            elif sentiment_score < -0.2:
                label = 'negative'
            else:
                label = 'neutral'
            
            return sentiment_score, label
            
        except Exception as e:
            logger.error(f"ê°ì • ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0, 'neutral'
    
    # =================== 4ë‹¨ê³„ ì¶”ê°€: ê³ ê¸‰ ê°ì • ë¶„ì„ ë©”ì„œë“œ ===================
    
    def _calculate_advanced_sentiment_score(self, text: str) -> Dict[str, Any]:
        """ê³ ê¸‰ ê°ì • ë¶„ì„ - ì„¸ë¶„í™”ëœ ê°ì • ê°ì§€"""
        try:
            text_lower = text.lower()
            
            # 1. ì„¸ë¶„í™”ëœ ê°ì • ì ìˆ˜ ê³„ì‚°
            detailed_emotions = {}
            total_emotion_score = 0
            
            for emotion, keywords_data in self.detailed_emotion_keywords.items():
                emotion_score = 0
                
                # í•œêµ­ì–´ í‚¤ì›Œë“œ ë§¤ì¹­
                korean_matches = sum(1 for keyword in keywords_data['korean'] if keyword in text_lower)
                english_matches = sum(1 for keyword in keywords_data['english'] if keyword in text_lower)
                
                # ê°€ì¤‘ì¹˜ ì ìš©
                emotion_score = (korean_matches + english_matches) * keywords_data['weight']
                detailed_emotions[emotion] = emotion_score
                total_emotion_score += emotion_score
            
            # 2. í•œêµ­ì–´ íŠ¹í™” ìŠ¬ë­ ë¶„ì„
            slang_bonus = 0
            for slang_type, slang_words in self.korean_emotion_expressions.items():
                matches = sum(1 for slang in slang_words if slang in text_lower)
                if matches > 0:
                    if 'positive' in slang_type:
                        detailed_emotions['joy'] = detailed_emotions.get('joy', 0) + matches * 1.5
                        slang_bonus += matches * 0.3
                    elif 'negative' in slang_type:
                        detailed_emotions['anger'] = detailed_emotions.get('anger', 0) + matches * 1.2
                        slang_bonus += matches * 0.2
            
            # 3. ê°ì • ê°•ë„ ë¶„ì„
            intensity_score = self._calculate_emotion_intensity(text_lower)
            
            # 4. ë§¥ë½ì  ê°ì • ë¶„ì„
            contextual_info = self._analyze_contextual_sentiment(text)
            
            # 5. ì •ê·œí™” ë° ì£¼ìš” ê°ì • ê²°ì •
            if total_emotion_score > 0:
                # ê°ì • ë¶„í¬ ì •ê·œí™”
                emotion_distribution = {
                    emotion: score / total_emotion_score 
                    for emotion, score in detailed_emotions.items() 
                    if score > 0
                }
                
                # ì£¼ìš” ê°ì • ê²°ì •
                dominant_emotion = max(detailed_emotions, key=detailed_emotions.get) if detailed_emotions else 'neutral'
                
                # ê°ì • ì‹ ë¢°ë„ ê³„ì‚°
                max_score = max(detailed_emotions.values()) if detailed_emotions else 0
                confidence = min(1.0, (max_score / total_emotion_score) * 2) if total_emotion_score > 0 else 0
                
            else:
                emotion_distribution = {}
                dominant_emotion = 'neutral'
                confidence = 0.5
            
            # 6. ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ ê°ì • ì ìˆ˜ ê³„ì‚°
            basic_sentiment_score, basic_sentiment_label = self._calculate_sentiment_score(text)
            
            return {
                'detailed_emotions': detailed_emotions,
                'emotion_distribution': emotion_distribution,
                'dominant_emotion': dominant_emotion,
                'emotion_intensity': intensity_score,
                'emotion_confidence': confidence,
                'contextual_sentiment': contextual_info['type'],
                'basic_sentiment_score': basic_sentiment_score,
                'basic_sentiment_label': basic_sentiment_label,
                'slang_detected': slang_bonus > 0,
                'analysis_method': 'advanced_multilayer'
            }
            
        except Exception as e:
            logger.error(f"ê³ ê¸‰ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ fallback
            basic_score, basic_label = self._calculate_sentiment_score(text)
            return {
                'detailed_emotions': {},
                'emotion_distribution': {},
                'dominant_emotion': 'neutral',
                'emotion_intensity': 0.0,
                'emotion_confidence': 0.0,
                'contextual_sentiment': 'unknown',
                'basic_sentiment_score': basic_score,
                'basic_sentiment_label': basic_label,
                'slang_detected': False,
                'analysis_method': 'fallback_basic'
            }
    
    def _calculate_emotion_intensity(self, text: str) -> float:
        """ê°ì • ê°•ë„ ê³„ì‚°"""
        try:
            intensity_score = 0.5  # ê¸°ë³¸ ê°•ë„
            
            # ê°•ë„ ì§€ì‹œì–´ ê²€ì‚¬
            for intensity_level, modifiers in self.emotion_intensity_modifiers.items():
                matches = sum(1 for modifier in modifiers if modifier in text)
                if matches > 0:
                    if intensity_level == 'high':
                        intensity_score += matches * 0.3
                    elif intensity_level == 'medium':
                        intensity_score += matches * 0.15
                    elif intensity_level == 'low':
                        intensity_score -= matches * 0.1
            
            # ë¬¸ì¥ë¶€í˜¸ë¥¼ í†µí•œ ê°•ë„ ë¶„ì„
            exclamation_count = text.count('!')
            question_count = text.count('?')
            caps_ratio = sum(1 for c in text if c.isupper()) / len(text) if text else 0
            
            intensity_score += min(0.2, exclamation_count * 0.05)  # ëŠë‚Œí‘œ
            intensity_score += min(0.1, question_count * 0.03)     # ë¬¼ìŒí‘œ
            intensity_score += min(0.15, caps_ratio * 0.5)         # ëŒ€ë¬¸ì ë¹„ìœ¨
            
            # ë°˜ë³µ ë¬¸ì íŒ¨í„´ (ã…‹ã…‹ã…‹, ã… ã… ã…  ë“±)
            repeated_patterns = len(re.findall(r'[ã…‹ã…ã… ã…œ]{3,}', text))
            intensity_score += min(0.2, repeated_patterns * 0.1)
            
            return min(1.0, max(0.0, intensity_score))
            
        except Exception as e:
            logger.error(f"ê°ì • ê°•ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.5
    
    def _analyze_contextual_sentiment(self, text: str) -> Dict[str, Any]:
        """ë§¥ë½ì  ê°ì • ë¶„ì„"""
        try:
            context_info = {
                'type': 'direct',
                'confidence': 1.0,
                'detected_patterns': []
            }
            
            # ë°˜ì–´ë²•/ë¹„ê¼¼ ê°ì§€
            for pattern in self.contextual_patterns['sarcasm']:
                if re.search(pattern, text, re.IGNORECASE):
                    context_info['type'] = 'sarcastic'
                    context_info['confidence'] = 0.7
                    context_info['detected_patterns'].append('sarcasm')
                    break
            
            # ì•„ì´ëŸ¬ë‹ˆ ê°ì§€
            for pattern in self.contextual_patterns['irony']:
                if re.search(pattern, text, re.IGNORECASE):
                    context_info['type'] = 'ironic'
                    context_info['confidence'] = 0.8
                    context_info['detected_patterns'].append('irony')
                    break
            
            # ê°•ì¡° íŒ¨í„´ ê°ì§€
            emphasis_count = 0
            for pattern in self.contextual_patterns['emphasis']:
                emphasis_count += len(re.findall(pattern, text))
            
            if emphasis_count > 0:
                context_info['detected_patterns'].append('emphasis')
                context_info['confidence'] = min(1.0, context_info['confidence'] + emphasis_count * 0.1)
            
            # ë¬¸ì¥ êµ¬ì¡° ë¶„ì„
            sentences = re.split(r'[.!?]', text)
            if len(sentences) > 3:
                context_info['structure'] = 'complex'
            elif len(sentences) > 1:
                context_info['structure'] = 'moderate'
            else:
                context_info['structure'] = 'simple'
            
            return context_info
            
        except Exception as e:
            logger.error(f"ë§¥ë½ì  ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                'type': 'unknown',
                'confidence': 0.5,
                'detected_patterns': [],
                'structure': 'unknown'
            }
    
    async def _perform_ai_sentiment_analysis(self, text: str, content_type: str) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ê°ì • ë¶„ì„"""
        try:
            if not hasattr(self, 'ai_handler') or not self.ai_handler:
                return {}
            
            # AI ê°ì • ë¶„ì„ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ì •í™•íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text[:500]}..."

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
1. ì£¼ìš” ê°ì •: [joy/anger/sadness/fear/surprise/disgust/trust/anticipation/neutral ì¤‘ í•˜ë‚˜]
2. ê°ì • ê°•ë„: [0.0-1.0 ì‚¬ì´ì˜ ìˆ«ì]
3. ê°ì • ì‹ ë¢°ë„: [0.0-1.0 ì‚¬ì´ì˜ ìˆ«ì]
4. ì„¸ë¶€ ê°ì • ë¶„í¬: [ê° ê°ì •ë³„ ì ìˆ˜]
5. ë§¥ë½ì  íŠ¹ì§•: [ì§ì ‘ì /ë°˜ì–´ì /ì•„ì´ëŸ¬ë‹ˆì /ê°•ì¡°ì ]

ì½˜í…ì¸  íƒ€ì…: {content_type}
ì–¸ì–´: í•œêµ­ì–´ ì¤‘ì‹¬ ë¶„ì„
"""
            
            ai_response = await self.ai_handler.get_ai_response(prompt)
            
            if ai_response and ai_response.strip():
                return self._parse_ai_sentiment_response(ai_response)
            
            return {}
            
        except Exception as e:
            logger.error(f"AI ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}
    
    def _parse_ai_sentiment_response(self, response: str) -> Dict[str, Any]:
        """AI ê°ì • ë¶„ì„ ì‘ë‹µ íŒŒì‹±"""
        try:
            result = {
                'ai_dominant_emotion': 'neutral',
                'ai_intensity': 0.5,
                'ai_confidence': 0.5,
                'ai_distribution': {},
                'ai_context': 'direct'
            }
            
            lines = response.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                
                if 'ì£¼ìš” ê°ì •:' in line:
                    emotion = line.split(':')[1].strip()
                    if emotion in ['joy', 'anger', 'sadness', 'fear', 'surprise', 'disgust', 'trust', 'anticipation', 'neutral']:
                        result['ai_dominant_emotion'] = emotion
                
                elif 'ê°ì • ê°•ë„:' in line:
                    try:
                        intensity = float(line.split(':')[1].strip())
                        result['ai_intensity'] = max(0.0, min(1.0, intensity))
                    except:
                        pass
                
                elif 'ê°ì • ì‹ ë¢°ë„:' in line:
                    try:
                        confidence = float(line.split(':')[1].strip())
                        result['ai_confidence'] = max(0.0, min(1.0, confidence))
                    except:
                        pass
                
                elif 'ë§¥ë½ì  íŠ¹ì§•:' in line:
                    context = line.split(':')[1].strip()
                    if any(ctx in context for ctx in ['ë°˜ì–´', 'ì•„ì´ëŸ¬ë‹ˆ', 'ê°•ì¡°', 'ì§ì ‘']):
                        result['ai_context'] = context
            
            return result
            
        except Exception as e:
            logger.error(f"AI ê°ì • ë¶„ì„ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {}
    
    def _calculate_quality_score(self, title: str, content: str, url: str) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
        try:
            score = 50.0  # ê¸°ë³¸ ì ìˆ˜
            
            # ì œëª© í’ˆì§ˆ (20ì )
            if title and len(title.strip()) > 10:
                score += 10
            if title and len(title.strip()) > 20:
                score += 10
            
            # ì½˜í…ì¸  ê¸¸ì´ (30ì )
            content_length = len(content)
            if content_length > 500:
                score += 10
            if content_length > 1500:
                score += 10
            if content_length > 3000:
                score += 10
            
            # êµ¬ì¡°í™” ì •ë„ (20ì )
            if '.' in content or '!' in content or '?' in content:
                score += 5
            if '\n' in content:  # ë‹¨ë½ êµ¬ë¶„
                score += 5
            if any(marker in content for marker in ['1.', '2.', '-', '*']):  # ëª©ë¡ êµ¬ì¡°
                score += 10
            
            # URL ì‹ ë¢°ë„ (30ì )
            domain = urlparse(url).netloc.lower()
            if any(trusted in domain for trusted in ['edu', 'gov', 'org']):
                score += 15
            elif any(known in domain for known in ['naver', 'google', 'github', 'stackoverflow']):
                score += 10
            elif not any(suspicious in domain for suspicious in ['bit.ly', 'tinyurl']):
                score += 5
            
            # HTTPS ì‚¬ìš©
            if url.startswith('https'):
                score += 5
            
            return min(100.0, max(0.0, score))
            
        except Exception as e:
            logger.error(f"í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 50.0
    
    def _determine_complexity_level(self, content: str) -> str:
        """ë³µì¡ë„ ë ˆë²¨ ê²°ì •"""
        try:
            text_lower = content.lower()
            
            # ê° ë ˆë²¨ë³„ ì ìˆ˜ ê³„ì‚°
            level_scores = {}
            for level, keywords in self.complexity_indicators.items():
                score = sum(1 for keyword in keywords if keyword in text_lower)
                level_scores[level] = score
            
            # ìµœê³  ì ìˆ˜ ë ˆë²¨ ë°˜í™˜
            if max(level_scores.values()) == 0:
                # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì½˜í…ì¸  ê¸¸ì´ë¡œ íŒë‹¨
                content_length = len(content)
                if content_length < 1000:
                    return 'beginner'
                elif content_length < 3000:
                    return 'intermediate'
                elif content_length < 5000:
                    return 'advanced'
                else:
                    return 'expert'
            
            return max(level_scores, key=level_scores.get)
            
        except Exception as e:
            logger.error(f"ë³µì¡ë„ ë ˆë²¨ ê²°ì • ì˜¤ë¥˜: {e}")
            return 'intermediate'
    
    def _calculate_reading_time(self, content: str) -> int:
        """ì˜ˆìƒ ì½ê¸° ì‹œê°„ ê³„ì‚° (ë¶„)"""
        try:
            # í•œêµ­ì–´ í‰ê·  ì½ê¸° ì†ë„: ë¶„ë‹¹ 250ì
            # ì˜ì–´ í‰ê·  ì½ê¸° ì†ë„: ë¶„ë‹¹ 200ë‹¨ì–´ (ì•½ 1000ì)
            
            korean_chars = len(re.findall(r'[ê°€-í£]', content))
            english_words = len(re.findall(r'[a-zA-Z]+', content))
            
            korean_time = korean_chars / 250
            english_time = english_words / 200
            
            total_time = korean_time + english_time
            return max(1, int(total_time))  # ìµœì†Œ 1ë¶„
            
        except Exception as e:
            logger.error(f"ì½ê¸° ì‹œê°„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 1
    
    def _extract_topics(self, content: str, max_topics: int = 5) -> List[str]:
        """ì£¼ìš” í† í”½ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì¶”í›„ AI ê¸°ë°˜ìœ¼ë¡œ ê°œì„ )
            text = content.lower()
            
            # ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ
            tech_keywords = ['python', 'javascript', 'react', 'django', 'api', 'database', 
                           'ai', 'machine learning', 'deep learning', 'blockchain']
            
            # ë¹„ì¦ˆë‹ˆìŠ¤ í‚¤ì›Œë“œ
            business_keywords = ['ë§ˆì¼€íŒ…', 'ë¹„ì¦ˆë‹ˆìŠ¤', 'ê²½ì˜', 'ì „ëµ', 'íˆ¬ì', 'ì°½ì—…']
            
            # ì¼ë°˜ í‚¤ì›Œë“œ
            general_keywords = ['êµìœ¡', 'ê±´ê°•', 'ì—¬í–‰', 'ìŒì‹', 'ë¬¸í™”', 'ìŠ¤í¬ì¸ ']
            
            all_keywords = tech_keywords + business_keywords + general_keywords
            
            found_topics = []
            for keyword in all_keywords:
                if keyword in text and len(found_topics) < max_topics:
                    found_topics.append(keyword)
            
            return found_topics
            
        except Exception as e:
            logger.error(f"í† í”½ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _detect_language(self, content: str) -> str:
        """ì–¸ì–´ ê°ì§€"""
        try:
            korean_chars = len(re.findall(r'[ê°€-í£]', content))
            english_chars = len(re.findall(r'[a-zA-Z]', content))
            
            if korean_chars > english_chars:
                return 'korean'
            elif english_chars > korean_chars * 2:
                return 'english'
            else:
                return 'mixed'
                
        except Exception as e:
            logger.error(f"ì–¸ì–´ ê°ì§€ ì˜¤ë¥˜: {e}")
            return 'unknown'
    
    def get_analysis_stats(self) -> Dict[str, Any]:
        """ë¶„ì„ í†µê³„ ë°˜í™˜"""
        return {
            "ì´_ë¶„ì„_ìˆ˜": self.analysis_stats['total_analyzed'],
            "ì„±ê³µ_ë¶„ì„_ìˆ˜": self.analysis_stats['successful_analyses'],
            "ì‹¤íŒ¨_ë¶„ì„_ìˆ˜": self.analysis_stats['failed_analyses'],
            "í‰ê· _ì²˜ë¦¬ì‹œê°„": f"{self.analysis_stats['average_processing_time']:.2f}ì´ˆ",
            "í‰ê· _í’ˆì§ˆì ìˆ˜": f"{self.analysis_stats['average_quality_score']:.1f}ì ",
            "ìºì‹œ_ì ì¤‘ë¥ ": f"{len(self.analysis_cache)}/{self.analysis_stats['total_analyzed'] or 1}",
            "ì‹œìŠ¤í…œ_ìƒíƒœ": "ì •ìƒ ìš´ì˜ì¤‘"
        }
    
    def get_system_status(self) -> str:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë¬¸ìì—´ ë°˜í™˜"""
        stats = self.get_analysis_stats()
        
        return f"""ğŸ”§ **ì‹œìŠ¤í…œ ìƒíƒœ**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š **ë¶„ì„ í†µê³„**:
â€¢ ì´ ë¶„ì„ ìˆ˜: {stats['ì´_ë¶„ì„_ìˆ˜']}ê±´
â€¢ ì„±ê³µë¥ : {stats['ì„±ê³µ_ë¶„ì„_ìˆ˜']}/{stats['ì´_ë¶„ì„_ìˆ˜']}ê±´
â€¢ í‰ê·  ì²˜ë¦¬ì‹œê°„: {stats['í‰ê· _ì²˜ë¦¬ì‹œê°„']}
â€¢ í‰ê·  í’ˆì§ˆì ìˆ˜: {stats['í‰ê· _í’ˆì§ˆì ìˆ˜']}

ğŸ’¾ **ìºì‹œ ìƒíƒœ**:
â€¢ ë¶„ì„ ìºì‹œ: {len(self.analysis_cache)}ê°œ í•­ëª©
â€¢ ë°°ì¹˜ ìºì‹œ: {len(self.batch_cache)}ê°œ í•­ëª©

ğŸŒ **ì—°ë™ ëª¨ë“ˆ**:
â€¢ AI Handler: âœ… ì—°ê²°ë¨
â€¢ Web Search IDE: âœ… ì—°ê²°ë¨
â€¢ Google Drive: âœ… ì—°ê²°ë¨

ğŸ”‹ **ì‹œìŠ¤í…œ**: {stats['ì‹œìŠ¤í…œ_ìƒíƒœ']}"""

    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.analysis_cache.clear()
        self.batch_cache.clear()
        logger.info("ë¶„ì„ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")

    # =================== ëŒ€í™”í˜• ê°€ì´ë“œ ì‹œìŠ¤í…œ (3ë‹¨ê³„) ===================
    
    async def chat_analyze_content(self, user_message: str, context: Dict = None) -> str:
        """ëŒ€í™”í˜• ì½˜í…ì¸  ë¶„ì„ - ChatGPT ìŠ¤íƒ€ì¼"""
        try:
            # URL ìë™ ê°ì§€
            urls = self._extract_urls_from_message(user_message)
            
            if urls:
                # URLì´ ìˆìœ¼ë©´ ìë™ ë¶„ì„ í›„ ê°€ì´ë“œ ì œê³µ
                url = urls[0]
                analysis = await self.analyze_web_content(url, use_ai=True)
                
                if analysis and not analysis.error_message:
                    return self._generate_conversational_response(analysis, user_message)
                else:
                    return f"ì£„ì†¡í•©ë‹ˆë‹¤. '{url}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. URLì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”. ğŸ”"
            
            elif any(keyword in user_message.lower() for keyword in ['ë¶„ì„', 'ìš”ì•½', 'ì •ë³´', 'ë‚´ìš©']):
                return self._generate_analysis_help_message()
            
            elif any(keyword in user_message.lower() for keyword in ['ëª…ë ¹ì–´', 'ê¸°ëŠ¥', 'ë„ì›€ë§']):
                return self._generate_command_guide()
            
            else:
                return self._generate_general_help_message()
                
        except Exception as e:
            logger.error(f"ëŒ€í™”í˜• ë¶„ì„ ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ˜…"
    
    def _extract_urls_from_message(self, message: str) -> List[str]:
        """ë©”ì‹œì§€ì—ì„œ URL ì¶”ì¶œ"""
        url_pattern = r'https?://[^\\s<>\"\\{\\}\\|\\\\\\^`\\[\\]]+'
        return re.findall(url_pattern, message)
    
    def _generate_conversational_response(self, analysis: ContentAnalysisResult, user_message: str) -> str:
        """ëŒ€í™”í˜• ë¶„ì„ ì‘ë‹µ ìƒì„± - ê¸°ë³¸ ê²°ê³¼ + ì „ë¬¸ ê¸°ëŠ¥ ì•ˆë‚´"""
        
        # ê°ì • ì´ëª¨ì§€ ë§¤í•‘
        sentiment_emoji = {
            'positive': 'ğŸ˜Š',
            'negative': 'ğŸ˜”', 
            'neutral': 'ğŸ˜'
        }
        
        # í’ˆì§ˆ ë“±ê¸‰ ë§¤í•‘
        if analysis.quality_score >= 80:
            quality_grade = "â­ ë§¤ìš° ìš°ìˆ˜"
        elif analysis.quality_score >= 60:
            quality_grade = "ğŸ‘ ì–‘í˜¸"
        else:
            quality_grade = "âš ï¸ ë³´í†µ"
        
        # ê¸°ë³¸ ì‘ë‹µ ìƒì„±
        response = f"""ğŸ“± **{analysis.title}** ë¶„ì„ ì™„ë£Œ!

ğŸ“Š **í’ˆì§ˆì ìˆ˜**: {analysis.quality_score:.1f}/100 ({quality_grade})
{sentiment_emoji.get(analysis.sentiment_label, 'ğŸ˜')} **ê°ì •**: {analysis.sentiment_label} ({analysis.sentiment_score:.1f}ì )
â±ï¸ **ì½ê¸°ì‹œê°„**: {analysis.reading_time}ë¶„
ğŸ“ **ìš”ì•½**: {analysis.summary[:150]}{'...' if len(analysis.summary) > 150 else ''}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”§ **ë” ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì›í•˜ì‹ ë‹¤ë©´**:

ğŸ’ `/analyze_url {analysis.url}` - ì™„ì „í•œ ìƒì„¸ ë¶„ì„ (êµ¬ì¡°í™”ëœ ë°ì´í„°)
ğŸ¯ `/sentiment_only {analysis.url}` - ê°ì •ë¶„ì„ë§Œ ì •ë°€í•˜ê²Œ
ğŸ“Š `/quality_check {analysis.url}` - í’ˆì§ˆ í‰ê°€ ì„¸ë¶€ í•­ëª©
ğŸ” `/extract_topics {analysis.url}` - ì£¼ì œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
ğŸ“ˆ `/complexity_analysis {analysis.url}` - ë³µì¡ë„ ìƒì„¸ ë¶„ì„

ğŸ“š **ë°°ì¹˜ ë¶„ì„**:
ğŸ’¼ `/batch_analyze url1,url2,url3` - ì—¬ëŸ¬ URL ë™ì‹œ ë¶„ì„
ğŸ“‹ `/compare_urls url1,url2` - ë‘ ì½˜í…ì¸  ë¹„êµ ë¶„ì„

ğŸ¤– **AI ì‹¬í™” ë¶„ì„**:
ğŸ§  `/ai_deep_analysis {analysis.url}` - AI ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„
ğŸ“– `/generate_summary {analysis.url}` - AI ë§ì¶¤í˜• ìš”ì•½ ìƒì„±

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ **íŒ**: ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ë©´ ë” ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì–´ìš”!
"""
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ì— ë”°ë¥¸ ì¶”ê°€ ì•ˆë‚´
        if 'ìš”ì•½' in user_message:
            response += "\n\nğŸ¯ **ìš”ì•½ì— íŠ¹í™”ëœ ëª…ë ¹ì–´**: `/generate_summary {analysis.url}`"
        elif 'ê°ì •' in user_message:
            response += "\n\nğŸ¯ **ê°ì •ë¶„ì„ì— íŠ¹í™”ëœ ëª…ë ¹ì–´**: `/sentiment_only {analysis.url}`"
        elif 'í’ˆì§ˆ' in user_message:
            response += "\n\nğŸ¯ **í’ˆì§ˆë¶„ì„ì— íŠ¹í™”ëœ ëª…ë ¹ì–´**: `/quality_check {analysis.url}`"
        
        return response
    
    def _generate_analysis_help_message(self) -> str:
        """ë¶„ì„ ë„ì›€ë§ ë©”ì‹œì§€ ìƒì„±"""
        return """ğŸ” **ì›¹ ì½˜í…ì¸  ë¶„ì„ ë„ì›€ë§**

ğŸ“Œ **ì‚¬ìš©ë²•**:
1ï¸âƒ£ **ê°„ë‹¨ ë¶„ì„**: URLì„ í¬í•¨í•œ ë©”ì‹œì§€ ì „ì†¡
   ì˜ˆ: "ì´ ì‚¬ì´íŠ¸ ë¶„ì„í•´ì¤˜ https://example.com"

2ï¸âƒ£ **ì „ë¬¸ ë¶„ì„**: ëª…ë ¹ì–´ ì‚¬ìš©
   ì˜ˆ: `/analyze_url https://example.com`

ğŸ¯ **ì§€ì›í•˜ëŠ” ì½˜í…ì¸  íƒ€ì…**:
â€¢ ğŸ“° ë‰´ìŠ¤ ê¸°ì‚¬ (ë„¤ì´ë²„ë‰´ìŠ¤, ì¡°ì„ ì¼ë³´ ë“±)
â€¢ ğŸ’» ê¸°ìˆ  ë¸”ë¡œê·¸ (velog, í‹°ìŠ¤í† ë¦¬, Medium ë“±)
â€¢ ğŸ“š ê³µì‹ ë¬¸ì„œ (GitHub, Stack Overflow ë“±)
â€¢ ğŸ›’ ì‡¼í•‘ëª° (ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´, ì¿ íŒ¡ ë“±)
â€¢ ğŸ“ í•™ìˆ  ìë£Œ (ë…¼ë¬¸, ì—°êµ¬ë³´ê³ ì„œ ë“±)

ğŸ’¡ **ë¶„ì„ í•­ëª©**:
â€¢ ì½˜í…ì¸  ìš”ì•½ ë° í•µì‹¬ í¬ì¸íŠ¸
â€¢ ê°ì • ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
â€¢ í’ˆì§ˆ ì ìˆ˜ (ì‹ ë¢°ë„, ìœ ìš©ì„±)
â€¢ ì½ê¸° ì‹œê°„ ë° ë³µì¡ë„
â€¢ ì£¼ì œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ

URLì„ ì•Œë ¤ì£¼ì‹œë©´ ë°”ë¡œ ë¶„ì„í•´ë“œë¦´ê²Œìš”! ğŸ˜Š"""
    
    def _generate_command_guide(self) -> str:
        """ëª…ë ¹ì–´ ê°€ì´ë“œ ìƒì„±"""
        return """ğŸ¤– **ì „ë¬¸ ë¶„ì„ ëª…ë ¹ì–´ ê°€ì´ë“œ**

ğŸ”¥ **ê¸°ë³¸ ë¶„ì„ ëª…ë ¹ì–´**:
â€¢ `/analyze_url <URL>` - ì™„ì „í•œ ì›¹ ì½˜í…ì¸  ë¶„ì„
â€¢ `/quick_analysis <URL>` - ë¹ ë¥¸ ê¸°ë³¸ ë¶„ì„
â€¢ `/sentiment_only <URL>` - ê°ì •ë¶„ì„ë§Œ ìˆ˜í–‰
â€¢ `/quality_check <URL>` - í’ˆì§ˆ í‰ê°€ ìƒì„¸ ë¶„ì„

ğŸ“Š **ê³ ê¸‰ ë¶„ì„ ëª…ë ¹ì–´**:
â€¢ `/extract_topics <URL>` - ì£¼ì œ/í‚¤ì›Œë“œ ì¶”ì¶œ
â€¢ `/complexity_analysis <URL>` - ë³µì¡ë„ ìƒì„¸ ë¶„ì„
â€¢ `/structure_analysis <URL>` - HTML êµ¬ì¡° ë¶„ì„
â€¢ `/metadata_extract <URL>` - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

ğŸš€ **ë°°ì¹˜ ì²˜ë¦¬ ëª…ë ¹ì–´**:
â€¢ `/batch_analyze <URL1,URL2,URL3>` - ì—¬ëŸ¬ URL ë™ì‹œ ë¶„ì„
â€¢ `/compare_urls <URL1,URL2>` - ë‘ ì½˜í…ì¸  ë¹„êµ
â€¢ `/batch_sentiment <URL1,URL2,URL3>` - ë°°ì¹˜ ê°ì •ë¶„ì„

ğŸ§  **AI ì‹¬í™” ë¶„ì„ ëª…ë ¹ì–´**:
â€¢ `/ai_deep_analysis <URL>` - AI ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„
â€¢ `/generate_summary <URL>` - AI ë§ì¶¤í˜• ìš”ì•½
â€¢ `/extract_insights <URL>` - ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
â€¢ `/content_recommendations <URL>` - ê´€ë ¨ ì½˜í…ì¸  ì¶”ì²œ

âš™ï¸ **ì‹œìŠ¤í…œ ëª…ë ¹ì–´**:
â€¢ `/analysis_stats` - ë¶„ì„ í†µê³„ ì¡°íšŒ
â€¢ `/clear_cache` - ìºì‹œ ì´ˆê¸°í™”
â€¢ `/system_status` - ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸

ğŸ’¡ **ì‚¬ìš© íŒ**:
- ëª…ë ¹ì–´ëŠ” ë” ì •í™•í•˜ê³  êµ¬ì¡°í™”ëœ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤
- ëŒ€í™”í˜•ìœ¼ë¡œ ë¬¼ì–´ë³´ì‹œë©´ ì ì ˆí•œ ëª…ë ¹ì–´ë¥¼ ì¶”ì²œí•´ë“œë ¤ìš”
- ë°°ì¹˜ ëª…ë ¹ì–´ë¡œ ì—¬ëŸ¬ ì½˜í…ì¸ ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"""
    
    def _generate_general_help_message(self) -> str:
        """ì¼ë°˜ ë„ì›€ë§ ë©”ì‹œì§€ ìƒì„±"""
        return """ğŸ‘‹ **íŒœì†”ë¼ AI ì½˜í…ì¸  ë¶„ì„ë´‡ì…ë‹ˆë‹¤!**

ğŸŒŸ **í•  ìˆ˜ ìˆëŠ” ì¼**:
â€¢ ì›¹ì‚¬ì´íŠ¸ ë‚´ìš© ë¶„ì„ ë° ìš”ì•½
â€¢ ê°ì • ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
â€¢ ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€
â€¢ ì£¼ì œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
â€¢ ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ë¹„êµ ë¶„ì„

ğŸ’¬ **ì‚¬ìš©ë²•**:
1ï¸âƒ£ **ê°„ë‹¨í•˜ê²Œ**: "ì´ ì‚¬ì´íŠ¸ ë¶„ì„í•´ì¤˜ https://example.com"
2ï¸âƒ£ **ì „ë¬¸ì ìœ¼ë¡œ**: `/analyze_url https://example.com`

ğŸ” **ë” ë§ì€ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´**:
â€¢ "ë¶„ì„ ë„ì›€ë§" - ë¶„ì„ ê¸°ëŠ¥ ìƒì„¸ ì„¤ëª…
â€¢ "ëª…ë ¹ì–´ ê°€ì´ë“œ" - ì „ë¬¸ ëª…ë ¹ì–´ ëª©ë¡
â€¢ URLì„ í¬í•¨í•œ ë©”ì‹œì§€ - ë°”ë¡œ ë¶„ì„ ì‹œì‘

ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š"""

    # =================== ê²°ê³¼ í¬ë§·íŒ… ë©”ì„œë“œ ===================
    
    def format_analysis_result(self, analysis: ContentAnalysisResult, format_type: str = "detailed") -> str:
        """ë¶„ì„ ê²°ê³¼ í¬ë§·íŒ…"""
        try:
            if format_type == "simple":
                return self._format_simple_result(analysis)
            elif format_type == "detailed":
                return self._format_detailed_result(analysis)
            elif format_type == "json":
                return self._format_json_result(analysis)
            else:
                return self._format_detailed_result(analysis)
        except Exception as e:
            logger.error(f"ê²°ê³¼ í¬ë§·íŒ… ì˜¤ë¥˜: {e}")
            return f"ê²°ê³¼ í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def _format_simple_result(self, analysis: ContentAnalysisResult) -> str:
        """ê°„ë‹¨í•œ ê²°ê³¼ í¬ë§·"""
        return f"""ğŸ“Œ **{analysis.title}**
ğŸ·ï¸ íƒ€ì…: {analysis.content_type} | ğŸ“Š í’ˆì§ˆ: {analysis.quality_score:.1f}ì 
ğŸ˜Š ê°ì •: {analysis.sentiment_label} | â±ï¸ ì½ê¸°ì‹œê°„: {analysis.reading_time}ë¶„
ğŸ“ ìš”ì•½: {analysis.summary}"""
    
    def _format_detailed_result(self, analysis: ContentAnalysisResult) -> str:
        """ìƒì„¸í•œ ê²°ê³¼ í¬ë§·"""
        key_points_text = "\n".join([f"â€¢ {point}" for point in analysis.key_points])
        topics_text = " ".join([f"#{topic}" for topic in analysis.topics[:5]])
        
        return f"""ğŸ“± **{analysis.title}**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”— **URL**: {analysis.url}
ğŸ·ï¸ **ì½˜í…ì¸  íƒ€ì…**: {analysis.content_type}
ğŸ“Š **í’ˆì§ˆì ìˆ˜**: {analysis.quality_score:.1f}/100
ğŸ˜Š **ê°ì •ë¶„ì„**: {analysis.sentiment_label} ({analysis.sentiment_score:.1f}ì )
ğŸŒ **ì–¸ì–´**: {analysis.language}
â±ï¸ **ì½ê¸°ì‹œê°„**: {analysis.reading_time}ë¶„
ğŸ“ˆ **ë³µì¡ë„**: {analysis.complexity_level}
ğŸ“ **ë‹¨ì–´ìˆ˜**: {analysis.word_count}ê°œ
ğŸ–¼ï¸ **ì´ë¯¸ì§€**: {analysis.image_count}ê°œ
ğŸ”— **ë§í¬**: {analysis.link_count}ê°œ

ğŸ“– **AI ìš”ì•½**:
{analysis.summary}

ğŸ”‘ **í•µì‹¬ í¬ì¸íŠ¸**:
{key_points_text}

ğŸ·ï¸ **ì£¼ìš” í† í”½**: {topics_text}

ğŸ¤– **ë¶„ì„ ëª¨ë¸**: {analysis.ai_model_used}
â° **ë¶„ì„ì‹œê°„**: {analysis.analysis_timestamp}"""
    
    def _format_json_result(self, analysis: ContentAnalysisResult) -> str:
        """JSON ê²°ê³¼ í¬ë§·"""
        import json
        
        result_dict = {
            "url": analysis.url,
            "title": analysis.title,
            "content_type": analysis.content_type,
            "summary": analysis.summary,
            "key_points": analysis.key_points,
            "sentiment": {
                "score": analysis.sentiment_score,
                "label": analysis.sentiment_label
            },
            "quality_score": analysis.quality_score,
            "topics": analysis.topics,
            "language": analysis.language,
            "reading_time": analysis.reading_time,
            "complexity_level": analysis.complexity_level,
            "metrics": {
                "word_count": analysis.word_count,
                "image_count": analysis.image_count,
                "link_count": analysis.link_count
            },
            "metadata": {
                "ai_model_used": analysis.ai_model_used,
                "analysis_timestamp": analysis.analysis_timestamp
            }
        }
        
        return f"```json\n{json.dumps(result_dict, ensure_ascii=False, indent=2)}\n```"

    # =================== AI ë¶„ì„ ì—”ì§„ ë©”ì„œë“œ ===================
    
    async def _perform_ai_analysis(self, title: str, content: str, url: str, content_type: str) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ì½˜í…ì¸  ë¶„ì„ ìˆ˜í–‰"""
        try:
            # AI í•¸ë“¤ëŸ¬ ê°€ì ¸ì˜¤ê¸°
            if hasattr(self, 'ai_handler') and self.ai_handler:
                prompt = self._get_analysis_prompt(title, content, url, content_type)
                ai_response = await self.ai_handler.chat_with_ai(prompt, "content_analyzer")
                return self._parse_ai_response(ai_response, content_type)
            else:
                logger.warning("AI í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´")
                return self._get_fallback_analysis(title, content, content_type)
                
        except Exception as e:
            logger.error(f"AI ë¶„ì„ ì˜¤ë¥˜: {e}")
            return self._get_fallback_analysis(title, content, content_type)
    
    def _get_analysis_prompt(self, title: str, content: str, url: str, content_type: str) -> str:
        """ì½˜í…ì¸  íƒ€ì…ë³„ AI ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_prompt = f"""
ë‹¤ìŒ {content_type} ì½˜í…ì¸ ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì œëª©: {title}
URL: {url}
ë‚´ìš©: {content[:3000]}...

ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
{{
    "summary": "3-4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ìš”ì•½",
    "key_points": ["í•µì‹¬ í¬ì¸íŠ¸ 1", "í•µì‹¬ í¬ì¸íŠ¸ 2", "í•µì‹¬ í¬ì¸íŠ¸ 3"],
    "main_insights": "ì£¼ìš” ì¸ì‚¬ì´íŠ¸",
    "target_audience": "ëŒ€ìƒ ë…ì",
    "usefulness": "ë†’ìŒ/ë³´í†µ/ë‚®ìŒ",
    "credibility": "ë†’ìŒ/ë³´í†µ/ë‚®ìŒ"
}}
"""
        return base_prompt
    
    def _parse_ai_response(self, ai_response: str, content_type: str) -> Dict[str, Any]:
        """AI ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            import re
            
            # JSON ë¶€ë¶„ ì¶”ì¶œ
            json_match = re.search(r'{.*}', ai_response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                parsed_data = json.loads(json_str)
                return parsed_data
            
            # JSONì´ ì•„ë‹Œ ê²½ìš° í…ìŠ¤íŠ¸ íŒŒì‹±
            return self._parse_text_response(ai_response)
            
        except Exception as e:
            logger.error(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {"summary": "AI ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "key_points": []}
    
    def _parse_text_response(self, response: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì‘ë‹µ íŒŒì‹±"""
        try:
            lines = response.split('\n')
            parsed = {"summary": "", "key_points": []}
            
            current_section = None
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                if 'ìš”ì•½' in line or 'summary' in line.lower():
                    current_section = 'summary'
                elif 'í•µì‹¬' in line or 'key' in line.lower():
                    current_section = 'key_points'
                elif current_section == 'summary' and not parsed['summary']:
                    parsed['summary'] = line
                elif current_section == 'key_points':
                    if line.startswith(('-', 'â€¢', '*', '1.', '2.', '3.')):
                        parsed['key_points'].append(line.lstrip('-â€¢*123. '))
            
            return parsed
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {"summary": "í…ìŠ¤íŠ¸ íŒŒì‹± ì˜¤ë¥˜", "key_points": []}
    
    def _get_fallback_analysis(self, title: str, content: str, content_type: str) -> Dict[str, Any]:
        """AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë¶„ì„"""
        return {
            'summary': f"{title}ì— ëŒ€í•œ {content_type} ì½˜í…ì¸ ì…ë‹ˆë‹¤. " + content[:200] + "...",
            'key_points': self._extract_topics(content, max_topics=3),
            'analysis_method': 'fallback',
            'confidence': 'low'
        }
    
    # =================== ëˆ„ë½ëœ ë¶„ì„ ë©”ì„œë“œë“¤ ===================
    
    def _analyze_content_structure(self, html_content: str, url: str) -> Dict[str, Any]:
        """ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            structure_info = {
                'has_navigation': bool(soup.find(['nav', 'header'])),
                'has_sidebar': bool(soup.find(['aside', 'sidebar'])),
                'has_footer': bool(soup.find('footer')),
                'article_count': len(soup.find_all('article')),
                'heading_count': len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])),
                'paragraph_count': len(soup.find_all('p')),
                'list_count': len(soup.find_all(['ul', 'ol'])),
                'image_count': len(soup.find_all('img')),
                'link_count': len(soup.find_all('a')),
                'code_block_count': len(soup.find_all(['pre', 'code'])),
                'table_count': len(soup.find_all('table'))
            }
            
            return structure_info
            
        except Exception as e:
            logger.error(f"ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}
    
    def _extract_metadata(self, html_content: str) -> Dict[str, str]:
        """HTML ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            metadata = {}
            
            # ê¸°ë³¸ ë©”íƒ€ íƒœê·¸
            meta_tags = {
                'description': soup.find('meta', attrs={'name': 'description'}),
                'keywords': soup.find('meta', attrs={'name': 'keywords'}),
                'author': soup.find('meta', attrs={'name': 'author'}),
                'robots': soup.find('meta', attrs={'name': 'robots'}),
                'viewport': soup.find('meta', attrs={'name': 'viewport'})
            }
            
            for key, tag in meta_tags.items():
                if tag and tag.get('content'):
                    metadata[key] = tag.get('content')
            
            # Open Graph íƒœê·¸
            og_tags = soup.find_all('meta', attrs={'property': lambda x: x and x.startswith('og:')})
            for tag in og_tags:
                property_name = tag.get('property', '').replace('og:', '')
                if property_name and tag.get('content'):
                    metadata[f'og_{property_name}'] = tag.get('content')
            
            # Twitter Card íƒœê·¸
            twitter_tags = soup.find_all('meta', attrs={'name': lambda x: x and x.startswith('twitter:')})
            for tag in twitter_tags:
                name = tag.get('name', '').replace('twitter:', '')
                if name and tag.get('content'):
                    metadata[f'twitter_{name}'] = tag.get('content')
            
            # JSON-LD êµ¬ì¡°í™” ë°ì´í„°
            json_ld_scripts = soup.find_all('script', type='application/ld+json')
            for script in json_ld_scripts:
                try:
                    json_data = json.loads(script.string)
                    if isinstance(json_data, dict):
                        if '@type' in json_data:
                            metadata['schema_type'] = json_data['@type']
                        if 'headline' in json_data:
                            metadata['schema_headline'] = json_data['headline']
                        if 'datePublished' in json_data:
                            metadata['schema_date_published'] = json_data['datePublished']
                except:
                    continue
            
            return metadata
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {}
    
    def _detect_content_language_advanced(self, content: str, metadata: Dict[str, str]) -> str:
        """ê³ ê¸‰ ì–¸ì–´ ê°ì§€ (ë©”íƒ€ë°ì´í„° í¬í•¨)"""
        try:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì–¸ì–´ ì •ë³´ í™•ì¸
            if 'og_locale' in metadata:
                locale = metadata['og_locale'].lower()
                if 'ko' in locale:
                    return 'korean'
                elif 'en' in locale:
                    return 'english'
            
            # HTML lang ì†ì„± í™•ì¸
            if 'lang' in metadata:
                lang = metadata['lang'].lower()
                if 'ko' in lang:
                    return 'korean'
                elif 'en' in lang:
                    return 'english'
            
            # ê¸°ì¡´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„
            return self._detect_language(content)
            
        except Exception as e:
            logger.error(f"ê³ ê¸‰ ì–¸ì–´ ê°ì§€ ì˜¤ë¥˜: {e}")
            return self._detect_language(content)
    
    def _calculate_content_metrics(self, content: str, structure_info: Dict[str, Any]) -> Dict[str, int]:
        """ì½˜í…ì¸  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = {
                'word_count': len(content.split()),
                'character_count': len(content),
                'sentence_count': len(re.findall(r'[.!?]+', content)),
                'paragraph_count': structure_info.get('paragraph_count', 0),
                'heading_count': structure_info.get('heading_count', 0),
                'image_count': structure_info.get('image_count', 0),
                'link_count': structure_info.get('link_count', 0),
                'code_block_count': structure_info.get('code_block_count', 0),
                'list_count': structure_info.get('list_count', 0),
                'table_count': structure_info.get('table_count', 0)
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"ì½˜í…ì¸  ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {
                'word_count': len(content.split()) if content else 0,
                'character_count': len(content) if content else 0,
                'sentence_count': 0,
                'paragraph_count': 0,
                'heading_count': 0,
                'image_count': 0,
                'link_count': 0,
                'code_block_count': 0,
                'list_count': 0,
                'table_count': 0
            }
    
    # =================== ë©”ì¸ ë¶„ì„ ë©”ì„œë“œ ===================
    
    async def analyze_web_content(self, url: str, use_ai: bool = True) -> Optional[ContentAnalysisResult]:
        """ì›¹ ì½˜í…ì¸  ì¢…í•© ë¶„ì„ - AI ì—”ì§„ í†µí•©"""
        start_time = datetime.now()
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(url, "full_analysis")
            if cache_key in self.analysis_cache:
                logger.info(f"ìºì‹œì—ì„œ ë¶„ì„ ê²°ê³¼ ë°˜í™˜: {url}")
                return self.analysis_cache[cache_key]
            
            # ì›¹ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            content_data = await self._fetch_web_content(url)
            if not content_data:
                return self._create_error_result(url, "ì›¹ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = content_data.get('title', '')
            content = content_data.get('content', '')
            html_content = content_data.get('html', '')
            
            # 1. ì½˜í…ì¸  íƒ€ì… ê°ì§€ (2ë‹¨ê³„ì—ì„œ êµ¬í˜„í•œ ê³ ê¸‰ ë¶„ë¥˜)
            soup = BeautifulSoup(html_content, 'html.parser') if html_content else None
            content_type = self._detect_content_type(title, content, url, soup)
            
            # 2. êµ¬ì¡° ë¶„ì„ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            structure_info = self._analyze_content_structure(html_content, url) if html_content else {}
            metadata = self._extract_metadata(html_content) if html_content else {}
            
            # 3. ê¸°ë³¸ ë¶„ì„
            sentiment_score, sentiment_label = self._calculate_sentiment_score(content)
            quality_score = self._calculate_quality_score(title, content, url)
            complexity_level = self._determine_complexity_level(content)
            reading_time = self._calculate_reading_time(content)
            language = self._detect_content_language_advanced(content, metadata)
            topics = self._extract_topics(content)
            
            # =================== 4ë‹¨ê³„ ì¶”ê°€: ê³ ê¸‰ ê°ì • ë¶„ì„ í†µí•© ===================
            # ê³ ê¸‰ ê°ì • ë¶„ì„ ìˆ˜í–‰
            advanced_sentiment = self._calculate_advanced_sentiment_score(content)
            
            # AI ê¸°ë°˜ ê°ì • ë¶„ì„ (ì„ íƒì )
            ai_sentiment = {}
            if use_ai and content:
                ai_sentiment = await self._perform_ai_sentiment_analysis(content, content_type)
            
            # ê°ì • ë¶„ì„ ê²°ê³¼ í†µí•©
            final_sentiment_score = advanced_sentiment.get('basic_sentiment_score', sentiment_score)
            final_sentiment_label = advanced_sentiment.get('basic_sentiment_label', sentiment_label)
            detailed_emotions = advanced_sentiment.get('detailed_emotions', {})
            emotion_intensity = advanced_sentiment.get('emotion_intensity', 0.0)
            emotion_confidence = advanced_sentiment.get('emotion_confidence', 0.0)
            dominant_emotion = advanced_sentiment.get('dominant_emotion', 'neutral')
            emotion_distribution = advanced_sentiment.get('emotion_distribution', {})
            contextual_sentiment = advanced_sentiment.get('contextual_sentiment', 'direct')
            
            # AI ê°ì • ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ê°€ì¤‘í‰ê· ìœ¼ë¡œ í†µí•©
            if ai_sentiment:
                ai_confidence = ai_sentiment.get('ai_confidence', 0.0)
                rule_confidence = emotion_confidence
                
                # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘í‰ê· 
                if ai_confidence > 0.7 and rule_confidence > 0.0:
                    total_confidence = ai_confidence + rule_confidence
                    emotion_confidence = (ai_confidence * ai_confidence + rule_confidence * rule_confidence) / total_confidence
                    
                    # AI ê²°ê³¼ê°€ ë” ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ ì£¼ìš” ê°ì • ì—…ë°ì´íŠ¸
                    if ai_confidence > rule_confidence:
                        dominant_emotion = ai_sentiment.get('ai_dominant_emotion', dominant_emotion)
                        emotion_intensity = (emotion_intensity + ai_sentiment.get('ai_intensity', 0.0)) / 2
            
            # 4. ì½˜í…ì¸  ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = self._calculate_content_metrics(content, structure_info)
            
            # 5. AI ê¸°ë°˜ ê³ ê¸‰ ë¶„ì„ (3ë‹¨ê³„ ì‹ ê·œ ê¸°ëŠ¥)
            ai_analysis = {}
            if use_ai and content:
                ai_analysis = await self._perform_ai_analysis(title, content, url, content_type)
            
            # 6. ë¶„ì„ ê²°ê³¼ í†µí•©
            analysis_result = ContentAnalysisResult(
                url=url,
                title=title,
                content_type=content_type,
                summary=ai_analysis.get('summary', content[:200] + "..."),
                key_points=ai_analysis.get('key_points', topics[:3]),
                sentiment_score=final_sentiment_score,
                sentiment_label=final_sentiment_label,
                quality_score=quality_score,
                topics=topics,
                language=language,
                reading_time=reading_time,
                complexity_level=complexity_level,
                analysis_timestamp=datetime.now().isoformat(),
                ai_model_used="gpt-4" if use_ai and ai_analysis else "rule-based",
                word_count=metrics.get('word_count', 0),
                image_count=metrics.get('image_count', 0),
                link_count=metrics.get('link_count', 0),
                error_message="",
                
                # =================== 4ë‹¨ê³„ ì¶”ê°€: ê³ ê¸‰ ê°ì • ë¶„ì„ í•„ë“œ ===================
                detailed_emotions=detailed_emotions,
                emotion_intensity=emotion_intensity,
                emotion_confidence=emotion_confidence,
                dominant_emotion=dominant_emotion,
                emotion_distribution=emotion_distribution,
                contextual_sentiment=contextual_sentiment,
                
                # =================== 4ë‹¨ê³„ ì¶”ê°€: ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ í•„ë“œ (ì„ì‹œ) ===================
                quality_dimensions={},  # ë‹¤ìŒ ì‘ì—…ì—ì„œ êµ¬í˜„ ì˜ˆì •
                quality_report="",
                improvement_suggestions=[],
                content_reliability=0.0,
                content_usefulness=0.0,
                content_accuracy=0.0
            )
            
            # 7. ìºì‹œì— ì €ì¥
            self.analysis_cache[cache_key] = analysis_result
            
            # 8. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_analysis_stats(analysis_result, start_time)
            
            logger.info(f"ì›¹ ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ: {url} (íƒ€ì…: {content_type}, í’ˆì§ˆ: {quality_score:.1f})")
            return analysis_result
            
        except Exception as e:
            logger.error(f"ì›¹ ì½˜í…ì¸  ë¶„ì„ ì˜¤ë¥˜: {e}")
            self.analysis_stats['failed_analyses'] += 1
            return self._create_error_result(url, f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    async def _fetch_web_content(self, url: str) -> Optional[Dict[str, str]]:
        """ì›¹ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° - web_search_ide í™œìš©"""
        try:
            # web_search_ideì˜ visit_site ë©”ì„œë“œ í™œìš©
            result = self.web_search.visit_site("content_analyzer", url, extract_code=False)
            
            if result.get('success'):
                return {
                    'title': result.get('title', ''),
                    'content': result.get('content_preview', ''),
                    'html': result.get('html_content', ''),  # ì „ì²´ HTMLì´ í•„ìš”í•œ ê²½ìš°
                    'url': url
                }
            else:
                # ëŒ€ì•ˆ: ì§ì ‘ HTTP ìš”ì²­
                return await self._fetch_content_direct(url)
                
        except Exception as e:
            logger.error(f"ì›¹ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return await self._fetch_content_direct(url)
    
    async def _fetch_content_direct(self, url: str) -> Optional[Dict[str, str]]:
        """ì§ì ‘ HTTP ìš”ì²­ìœ¼ë¡œ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        
                        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
                        soup = BeautifulSoup(html_content, 'html.parser')
                        
                        # ì œëª© ì¶”ì¶œ
                        title_tag = soup.find('title')
                        title = title_tag.get_text().strip() if title_tag else "ì œëª© ì—†ìŒ"
                        
                        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        for script in soup(["script", "style"]):
                            script.decompose()
                        
                        text_content = soup.get_text()
                        clean_text = ' '.join(text_content.split())
                        
                        return {
                            'title': title,
                            'content': clean_text,
                            'html': html_content,
                            'url': url
                        }
                    else:
                        logger.warning(f"HTTP {response.status}: {url}")
                        return None
                        
        except Exception as e:
            logger.error(f"ì§ì ‘ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    def _create_error_result(self, url: str, error_message: str) -> ContentAnalysisResult:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        return ContentAnalysisResult(
            url=url,
            title="ë¶„ì„ ì‹¤íŒ¨",
            content_type="error",
            summary="",
            key_points=[],
            sentiment_score=0.0,
            sentiment_label="neutral",
            quality_score=0.0,
            topics=[],
            language="unknown",
            reading_time=0,
            complexity_level="unknown",
            analysis_timestamp=datetime.now().isoformat(),
            ai_model_used="none",
            word_count=0,
            image_count=0,
            link_count=0,
            error_message=error_message
        )
    
    def _update_analysis_stats(self, result: ContentAnalysisResult, start_time: datetime):
        """ë¶„ì„ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            processing_time = (datetime.now() - start_time).total_seconds()
            
            self.analysis_stats['total_analyzed'] += 1
            if not result.error_message:
                self.analysis_stats['successful_analyses'] += 1
            
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            current_avg = self.analysis_stats['average_processing_time']
            total = self.analysis_stats['total_analyzed']
            self.analysis_stats['average_processing_time'] = (current_avg * (total - 1) + processing_time) / total
            
            # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸
            if result.quality_score > 0:
                current_quality_avg = self.analysis_stats['average_quality_score']
                success_count = self.analysis_stats['successful_analyses']
                self.analysis_stats['average_quality_score'] = (current_quality_avg * (success_count - 1) + result.quality_score) / success_count
            
        except Exception as e:
            logger.error(f"í†µê³„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")

    # =================== ë°°ì¹˜ ë¶„ì„ ë©”ì„œë“œ ===================
    
    async def analyze_batch_urls(self, urls: List[str], max_concurrent: int = 5) -> BatchAnalysisReport:
        """ë°°ì¹˜ URL ë¶„ì„"""
        start_time = datetime.now()
        
        try:
            # ìºì‹œ í™•ì¸
            urls_key = hashlib.md5(str(sorted(urls)).encode()).hexdigest()
            cache_key = f"batch_{urls_key}"
            
            if cache_key in self.batch_cache:
                logger.info("ë°°ì¹˜ ë¶„ì„ ìºì‹œ ê²°ê³¼ ë°˜í™˜")
                return self.batch_cache[cache_key]
            
            # ë¹„ë™ê¸° ë°°ì¹˜ ë¶„ì„
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def analyze_single_url(url):
                async with semaphore:
                    return await self.analyze_web_content(url)
            
            # ëª¨ë“  URL ë™ì‹œ ë¶„ì„
            tasks = [analyze_single_url(url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì§‘ê³„
            successful_results = []
            failed_count = 0
            
            for result in results:
                if isinstance(result, ContentAnalysisResult) and not result.error_message:
                    successful_results.append(result)
                else:
                    failed_count += 1
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = self._generate_batch_report(successful_results, failed_count, len(urls), start_time)
            
            # ìºì‹œ ì €ì¥
            self.batch_cache[cache_key] = report
            
            logger.info(f"ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(successful_results)}/{len(urls)} ì„±ê³µ")
            return report
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return self._create_error_batch_report(len(urls), str(e))
    
    def _generate_batch_report(self, results: List[ContentAnalysisResult], failed_count: int, total_count: int, start_time: datetime) -> BatchAnalysisReport:
        """ë°°ì¹˜ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            processing_time = (datetime.now() - start_time).total_seconds()
            
            if not results:
                return BatchAnalysisReport(
                    total_analyzed=total_count,
                    success_count=0,
                    failed_count=failed_count,
                    average_quality_score=0.0,
                    dominant_sentiment="neutral",
                    top_topics=[],
                    content_type_distribution={},
                    analysis_summary="ë¶„ì„ëœ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    processing_time=processing_time,
                    report_timestamp=datetime.now().isoformat()
                )
            
            # í†µê³„ ê³„ì‚°
            avg_quality = sum(r.quality_score for r in results) / len(results)
            
            # ê°ì • ë¶„í¬
            sentiment_counts = {}
            for result in results:
                sentiment = result.sentiment_label
                sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
            dominant_sentiment = max(sentiment_counts, key=sentiment_counts.get)
            
            # í† í”½ ì§‘ê³„
            topic_counts = {}
            for result in results:
                for topic in result.topics:
                    topic_counts[topic] = topic_counts.get(topic, 0) + 1
            top_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            
            # ì½˜í…ì¸  íƒ€ì… ë¶„í¬
            type_distribution = {}
            for result in results:
                content_type = result.content_type
                type_distribution[content_type] = type_distribution.get(content_type, 0) + 1
            
            # ìš”ì•½ ìƒì„±
            summary = f"ì´ {len(results)}ê°œ ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ. í‰ê·  í’ˆì§ˆì ìˆ˜ {avg_quality:.1f}ì , ì£¼ìš” ê°ì • {dominant_sentiment}"
            
            return BatchAnalysisReport(
                total_analyzed=total_count,
                success_count=len(results),
                failed_count=failed_count,
                average_quality_score=avg_quality,
                dominant_sentiment=dominant_sentiment,
                top_topics=top_topics,
                content_type_distribution=type_distribution,
                analysis_summary=summary,
                processing_time=processing_time,
                report_timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._create_error_batch_report(total_count, str(e))
    
    def _create_error_batch_report(self, total_count: int, error_message: str) -> BatchAnalysisReport:
        """ì˜¤ë¥˜ ë°°ì¹˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        return BatchAnalysisReport(
            total_analyzed=total_count,
            success_count=0,
            failed_count=total_count,
            average_quality_score=0.0,
            dominant_sentiment="neutral",
            top_topics=[],
            content_type_distribution={},
            analysis_summary=f"ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {error_message}",
            processing_time=0.0,
            report_timestamp=datetime.now().isoformat()
        )
    
    # =================== 4ë‹¨ê³„ ì¶”ê°€: ì •êµí•œ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ===================
    
    def _calculate_advanced_quality_score(self, title: str, content: str, url: str, 
                                        content_type: str, structure_info: Dict = None) -> Dict[str, Any]:
        """ì •êµí•œ ë‹¤ì°¨ì› í’ˆì§ˆ í‰ê°€"""
        try:
            # ì½˜í…ì¸  íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            type_weights = self.content_type_quality_weights.get(content_type, 
                                                               self.content_type_quality_weights['general'])
            
            # ê° ì°¨ì›ë³„ ì ìˆ˜ ê³„ì‚°
            dimension_scores = {}
            detailed_analysis = {}
            
            # 1. ì‹ ë¢°ë„ (Reliability) í‰ê°€
            reliability_score, reliability_detail = self._evaluate_reliability(title, content, url, structure_info)
            dimension_scores['reliability'] = reliability_score * type_weights['reliability']
            detailed_analysis['reliability'] = reliability_detail
            
            # 2. ìœ ìš©ì„± (Usefulness) í‰ê°€
            usefulness_score, usefulness_detail = self._evaluate_usefulness(title, content, content_type)
            dimension_scores['usefulness'] = usefulness_score * type_weights['usefulness']
            detailed_analysis['usefulness'] = usefulness_detail
            
            # 3. ì •í™•ì„± (Accuracy) í‰ê°€
            accuracy_score, accuracy_detail = self._evaluate_accuracy(title, content, url)
            dimension_scores['accuracy'] = accuracy_score * type_weights['accuracy']
            detailed_analysis['accuracy'] = accuracy_detail
            
            # 4. ì™„ì„±ë„ (Completeness) í‰ê°€
            completeness_score, completeness_detail = self._evaluate_completeness(title, content, structure_info)
            dimension_scores['completeness'] = completeness_score * type_weights['completeness']
            detailed_analysis['completeness'] = completeness_detail
            
            # 5. ê°€ë…ì„± (Readability) í‰ê°€
            readability_score, readability_detail = self._evaluate_readability(title, content, structure_info)
            dimension_scores['readability'] = readability_score * type_weights['readability']
            detailed_analysis['readability'] = readability_detail
            
            # 6. ë…ì°½ì„± (Originality) í‰ê°€
            originality_score, originality_detail = self._evaluate_originality(title, content, content_type)
            dimension_scores['originality'] = originality_score * type_weights['originality']
            detailed_analysis['originality'] = originality_detail
            
            # ì–¸ì–´ í’ˆì§ˆ í‰ê°€
            language_quality = self._evaluate_language_quality(content)
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘í‰ê· )
            total_weight = sum(type_weights.values())
            overall_score = sum(dimension_scores.values()) / total_weight
            overall_score = min(100.0, max(0.0, overall_score))
            
            # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
            quality_grade = self._determine_quality_grade(overall_score)
            
            # ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„±
            improvement_suggestions = self._generate_improvement_suggestions(
                dimension_scores, detailed_analysis, content_type
            )
            
            # í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
            quality_report = self._generate_quality_report(
                dimension_scores, detailed_analysis, language_quality, 
                quality_grade, content_type
            )
            
            return {
                'overall_score': overall_score,
                'quality_grade': quality_grade,
                'dimension_scores': dimension_scores,
                'detailed_analysis': detailed_analysis,
                'language_quality': language_quality,
                'improvement_suggestions': improvement_suggestions,
                'quality_report': quality_report,
                'content_type_weights': type_weights
            }
            
        except Exception as e:
            logger.error(f"ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {e}")
            return {
                'overall_score': 50.0,
                'quality_grade': 'C',
                'dimension_scores': {},
                'detailed_analysis': {},
                'language_quality': {},
                'improvement_suggestions': ['í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'],
                'quality_report': 'í’ˆì§ˆ í‰ê°€ë¥¼ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'content_type_weights': {}
            }
    
    def _evaluate_reliability(self, title: str, content: str, url: str, structure_info: Dict = None) -> Tuple[float, Dict]:
        """ì‹ ë¢°ë„ í‰ê°€"""
        try:
            score = 50.0  # ê¸°ë³¸ ì ìˆ˜
            details = {}
            
            domain = urlparse(url).netloc.lower()
            text_lower = f"{title} {content}".lower()
            
            # ì¶œì²˜ ì‹ ë¢°ë„ í‰ê°€
            source_score = 0
            credible_domains = self.quality_dimensions['reliability']['indicators']['source_credibility']
            for domain_type in credible_domains:
                if domain_type in domain:
                    source_score += 20
                    break
            else:
                # ì•Œë ¤ì§„ ì‹ ë¢°í•  ë§Œí•œ ì‚¬ì´íŠ¸ ì²´í¬
                if any(trusted in domain for trusted in ['naver', 'google', 'wikipedia', 'github']):
                    source_score += 10
            
            details['source_credibility'] = source_score
            score += source_score
            
            # ì €ì ì „ë¬¸ì„± í‰ê°€
            expertise_score = 0
            expertise_indicators = self.quality_dimensions['reliability']['indicators']['author_expertise']
            expertise_matches = sum(1 for indicator in expertise_indicators if indicator in text_lower)
            expertise_score = min(15, expertise_matches * 5)
            
            details['author_expertise'] = expertise_score
            score += expertise_score
            
            # ì¸ìš© ë° ì°¸ê³ ë¬¸í—Œ í’ˆì§ˆ
            citation_score = 0
            citation_indicators = self.quality_dimensions['reliability']['indicators']['citation_quality']
            citation_matches = sum(1 for indicator in citation_indicators if indicator in text_lower)
            citation_score = min(10, citation_matches * 3)
            
            details['citation_quality'] = citation_score
            score += citation_score
            
            # ì‚¬ì‹¤ í™•ì¸ ë° ë°ì´í„° ê¸°ë°˜
            fact_score = 0
            fact_indicators = self.quality_dimensions['reliability']['indicators']['fact_checking']
            fact_matches = sum(1 for indicator in fact_indicators if indicator in text_lower)
            fact_score = min(10, fact_matches * 2)
            
            details['fact_checking'] = fact_score
            score += fact_score
            
            # HTTPS ì‚¬ìš© (ë³´ì•ˆ)
            if url.startswith('https'):
                score += 5
                details['security'] = 5
            else:
                details['security'] = 0
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ì‹ ë¢°ë„ í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_usefulness(self, title: str, content: str, content_type: str) -> Tuple[float, Dict]:
        """ìœ ìš©ì„± í‰ê°€"""
        try:
            score = 50.0
            details = {}
            text_lower = f"{title} {content}".lower()
            
            # ì‹¤ìš©ì  ê°€ì¹˜
            practical_score = 0
            practical_indicators = self.quality_dimensions['usefulness']['indicators']['practical_value']
            practical_matches = sum(1 for indicator in practical_indicators if indicator in text_lower)
            practical_score = min(25, practical_matches * 4)
            
            details['practical_value'] = practical_score
            score += practical_score
            
            # ì‹¤í–‰ ê°€ëŠ¥í•œ ë‚´ìš©
            actionable_score = 0
            actionable_indicators = self.quality_dimensions['usefulness']['indicators']['actionable_content']
            actionable_matches = sum(1 for indicator in actionable_indicators if indicator in text_lower)
            actionable_score = min(20, actionable_matches * 3)
            
            details['actionable_content'] = actionable_score
            score += actionable_score
            
            # ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
            problem_solving_score = 0
            problem_indicators = self.quality_dimensions['usefulness']['indicators']['problem_solving']
            problem_matches = sum(1 for indicator in problem_indicators if indicator in text_lower)
            problem_solving_score = min(15, problem_matches * 3)
            
            details['problem_solving'] = problem_solving_score
            score += problem_solving_score
            
            # í•™ìŠµ ê°€ì¹˜
            learning_score = 0
            learning_indicators = self.quality_dimensions['usefulness']['indicators']['learning_value']
            learning_matches = sum(1 for indicator in learning_indicators if indicator in text_lower)
            learning_score = min(10, learning_matches * 2)
            
            details['learning_value'] = learning_score
            score += learning_score
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ìœ ìš©ì„± í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_accuracy(self, title: str, content: str, url: str) -> Tuple[float, Dict]:
        """ì •í™•ì„± í‰ê°€"""
        try:
            score = 50.0
            details = {}
            text_lower = f"{title} {content}".lower()
            
            # ê¸°ìˆ ì  ì •ë°€ì„±
            precision_score = 0
            precision_indicators = self.quality_dimensions['accuracy']['indicators']['technical_precision']
            precision_matches = sum(1 for indicator in precision_indicators if indicator in text_lower)
            precision_score = min(25, precision_matches * 8)
            
            details['technical_precision'] = precision_score
            score += precision_score
            
            # ì˜¤ë¥˜ ì§€ì‹œì–´ í™•ì¸ (ë¶€ì •ì  í‰ê°€)
            error_penalty = 0
            error_indicators = self.quality_dimensions['accuracy']['indicators']['error_indicators']
            error_matches = sum(1 for indicator in error_indicators if indicator in text_lower)
            error_penalty = min(20, error_matches * 5)
            
            details['error_indicators'] = -error_penalty
            score -= error_penalty
            
            # ê²€ì¦ í‘œì‹œ
            verification_score = 0
            verification_indicators = self.quality_dimensions['accuracy']['indicators']['verification_marks']
            verification_matches = sum(1 for indicator in verification_indicators if indicator in text_lower)
            verification_score = min(15, verification_matches * 5)
            
            details['verification_marks'] = verification_score
            score += verification_score
            
            # ìµœì‹ ì„± í‰ê°€
            update_score = 0
            update_indicators = self.quality_dimensions['accuracy']['indicators']['update_frequency']
            update_matches = sum(1 for indicator in update_indicators if indicator in text_lower)
            update_score = min(10, update_matches * 3)
            
            details['update_frequency'] = update_score
            score += update_score
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ì •í™•ì„± í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_completeness(self, title: str, content: str, structure_info: Dict = None) -> Tuple[float, Dict]:
        """ì™„ì„±ë„ í‰ê°€"""
        try:
            score = 50.0
            details = {}
            text_lower = f"{title} {content}".lower()
            
            # í¬ê´„ì  ì»¤ë²„ë¦¬ì§€
            coverage_score = 0
            coverage_indicators = self.quality_dimensions['completeness']['indicators']['comprehensive_coverage']
            coverage_matches = sum(1 for indicator in coverage_indicators if indicator in text_lower)
            coverage_score = min(25, coverage_matches * 8)
            
            details['comprehensive_coverage'] = coverage_score
            score += coverage_score
            
            # ìƒì„¸í•œ ì„¤ëª…
            detail_score = 0
            detail_indicators = self.quality_dimensions['completeness']['indicators']['detailed_explanation']
            detail_matches = sum(1 for indicator in detail_indicators if indicator in text_lower)
            detail_score = min(20, detail_matches * 7)
            
            details['detailed_explanation'] = detail_score
            score += detail_score
            
            # ì˜ˆì œ ì œê³µ
            example_score = 0
            example_indicators = self.quality_dimensions['completeness']['indicators']['example_provision']
            example_matches = sum(1 for indicator in example_indicators if indicator in text_lower)
            example_score = min(15, example_matches * 5)
            
            details['example_provision'] = example_score
            score += example_score
            
            # ë‹¨ê³„ë³„ ì„¤ëª…
            step_score = 0
            step_indicators = self.quality_dimensions['completeness']['indicators']['step_by_step']
            step_matches = sum(1 for indicator in step_indicators if indicator in text_lower)
            step_score = min(10, step_matches * 3)
            
            details['step_by_step'] = step_score
            score += step_score
            
            # ì½˜í…ì¸  ê¸¸ì´ ë³´ë„ˆìŠ¤
            content_length = len(content)
            if content_length > 2000:
                length_bonus = min(10, (content_length - 2000) // 1000 * 2)
                score += length_bonus
                details['content_length_bonus'] = length_bonus
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ì™„ì„±ë„ í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_readability(self, title: str, content: str, structure_info: Dict = None) -> Tuple[float, Dict]:
        """ê°€ë…ì„± í‰ê°€"""
        try:
            score = 50.0
            details = {}
            text_lower = f"{title} {content}".lower()
            
            # ëª…í™•í•œ êµ¬ì¡°
            structure_score = 0
            structure_indicators = self.quality_dimensions['readability']['indicators']['clear_structure']
            structure_matches = sum(1 for indicator in structure_indicators if indicator in text_lower)
            structure_score = min(25, structure_matches * 8)
            
            details['clear_structure'] = structure_score
            score += structure_score
            
            # ê°„ë‹¨í•œ ì–¸ì–´
            language_score = 0
            language_indicators = self.quality_dimensions['readability']['indicators']['simple_language']
            language_matches = sum(1 for indicator in language_indicators if indicator in text_lower)
            language_score = min(20, language_matches * 7)
            
            details['simple_language'] = language_score
            score += language_score
            
            # ì‹œê°ì  ë„êµ¬
            visual_score = 0
            visual_indicators = self.quality_dimensions['readability']['indicators']['visual_aids']
            visual_matches = sum(1 for indicator in visual_indicators if indicator in text_lower)
            visual_score = min(15, visual_matches * 5)
            
            details['visual_aids'] = visual_score
            score += visual_score
            
            # í¬ë§·íŒ…
            formatting_score = 0
            formatting_indicators = self.quality_dimensions['readability']['indicators']['formatting']
            formatting_matches = sum(1 for indicator in formatting_indicators if indicator in text_lower)
            formatting_score = min(10, formatting_matches * 3)
            
            # êµ¬ì¡° ì •ë³´ í™œìš©
            if structure_info:
                if structure_info.get('heading_count', 0) > 2:
                    formatting_score += 5
                if structure_info.get('list_count', 0) > 0:
                    formatting_score += 3
                if structure_info.get('paragraph_count', 0) > 3:
                    formatting_score += 2
            
            details['formatting'] = formatting_score
            score += formatting_score
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ê°€ë…ì„± í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_originality(self, title: str, content: str, content_type: str) -> Tuple[float, Dict]:
        """ë…ì°½ì„± í‰ê°€"""
        try:
            score = 50.0
            details = {}
            text_lower = f"{title} {content}".lower()
            
            # ë…íŠ¹í•œ ê´€ì 
            perspective_score = 0
            perspective_indicators = self.quality_dimensions['originality']['indicators']['unique_perspective']
            perspective_matches = sum(1 for indicator in perspective_indicators if indicator in text_lower)
            perspective_score = min(25, perspective_matches * 8)
            
            details['unique_perspective'] = perspective_score
            score += perspective_score
            
            # ê°œì¸ì  í†µì°°
            insight_score = 0
            insight_indicators = self.quality_dimensions['originality']['indicators']['personal_insight']
            insight_matches = sum(1 for indicator in insight_indicators if indicator in text_lower)
            insight_score = min(20, insight_matches * 7)
            
            details['personal_insight'] = insight_score
            score += insight_score
            
            # ì°½ì˜ì  ì ‘ê·¼
            creative_score = 0
            creative_indicators = self.quality_dimensions['originality']['indicators']['creative_approach']
            creative_matches = sum(1 for indicator in creative_indicators if indicator in text_lower)
            creative_score = min(15, creative_matches * 5)
            
            details['creative_approach'] = creative_score
            score += creative_score
            
            # ì‚¬ê³  ìê·¹ì 
            thought_score = 0
            thought_indicators = self.quality_dimensions['originality']['indicators']['thought_provoking']
            thought_matches = sum(1 for indicator in thought_indicators if indicator in text_lower)
            thought_score = min(10, thought_matches * 3)
            
            details['thought_provoking'] = thought_score
            score += thought_score
            
            # ì½˜í…ì¸  íƒ€ì…ë³„ ì¡°ì •
            if content_type == 'blog':
                score += 10  # ë¸”ë¡œê·¸ëŠ” ê°œì¸ì  íŠ¹ì„±ì´ ì¤‘ìš”
            elif content_type == 'news':
                score -= 5   # ë‰´ìŠ¤ëŠ” ê°ê´€ì„±ì´ ë” ì¤‘ìš”
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ë…ì°½ì„± í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_language_quality(self, content: str) -> Dict[str, Any]:
        """ì–¸ì–´ í’ˆì§ˆ í‰ê°€"""
        try:
            analysis = {
                'grammar_errors': 0,
                'spelling_errors': 0,
                'good_expressions': 0,
                'vocabulary_diversity': 0,
                'sentence_variety': 0,
                'overall_language_score': 50.0
            }
            
            # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
            for pattern in self.language_quality_indicators['grammar_errors']:
                matches = len(re.findall(pattern, content, re.IGNORECASE))
                analysis['grammar_errors'] += matches
            
            # ë§ì¶¤ë²• ì˜¤ë¥˜ ê²€ì‚¬
            for pattern in self.language_quality_indicators['spelling_errors']:
                matches = len(re.findall(pattern, content, re.IGNORECASE))
                analysis['spelling_errors'] += matches
            
            # ì¢‹ì€ í‘œí˜„ í™•ì¸
            for pattern in self.language_quality_indicators['good_expressions']:
                matches = len(re.findall(pattern, content, re.IGNORECASE))
                analysis['good_expressions'] += matches
            
            # ì–´íœ˜ ë‹¤ì–‘ì„± (ê³ ìœ  ë‹¨ì–´ ìˆ˜ / ì „ì²´ ë‹¨ì–´ ìˆ˜)
            words = re.findall(r'[ê°€-í£a-zA-Z]+', content)
            if words:
                unique_words = set(words)
                analysis['vocabulary_diversity'] = len(unique_words) / len(words) * 100
            
            # ë¬¸ì¥ ë‹¤ì–‘ì„± (ë¬¸ì¥ ê¸¸ì´ì˜ í‘œì¤€í¸ì°¨)
            sentences = re.split(r'[.!?]', content)
            if len(sentences) > 1:
                sentence_lengths = [len(s.strip()) for s in sentences if s.strip()]
                if sentence_lengths:
                    import statistics
                    analysis['sentence_variety'] = statistics.stdev(sentence_lengths) if len(sentence_lengths) > 1 else 0
            
            # ì „ì²´ ì–¸ì–´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            score = 70.0
            score -= analysis['grammar_errors'] * 5  # ë¬¸ë²• ì˜¤ë¥˜ í˜ë„í‹°
            score -= analysis['spelling_errors'] * 3  # ë§ì¶¤ë²• ì˜¤ë¥˜ í˜ë„í‹°
            score += min(20, analysis['good_expressions'] * 2)  # ì¢‹ì€ í‘œí˜„ ë³´ë„ˆìŠ¤
            score += min(10, analysis['vocabulary_diversity'] * 0.2)  # ì–´íœ˜ ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
            
            analysis['overall_language_score'] = min(100.0, max(0.0, score))
            
            return analysis
            
        except Exception as e:
            logger.error(f"ì–¸ì–´ í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {e}")
            return {
                'grammar_errors': 0,
                'spelling_errors': 0,
                'good_expressions': 0,
                'vocabulary_diversity': 0,
                'sentence_variety': 0,
                'overall_language_score': 50.0,
                'error': str(e)
            }
    
    def _determine_quality_grade(self, score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if score >= 90:
            return 'A+'
        elif score >= 85:
            return 'A'
        elif score >= 80:
            return 'B+'
        elif score >= 75:
            return 'B'
        elif score >= 70:
            return 'C+'
        elif score >= 65:
            return 'C'
        elif score >= 60:
            return 'D+'
        elif score >= 55:
            return 'D'
        else:
            return 'F'
    
    def _generate_improvement_suggestions(self, dimension_scores: Dict[str, float], 
                                        detailed_analysis: Dict[str, Dict], 
                                        content_type: str) -> List[str]:
        """ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„±"""
        suggestions = []
        
        try:
            # ê° ì°¨ì›ë³„ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°œì„  ì œì•ˆ
            for dimension, score in dimension_scores.items():
                if score < 60:  # ë‚®ì€ ì ìˆ˜ì˜ ì°¨ì›ì— ëŒ€í•´ ì œì•ˆ
                    if dimension == 'reliability':
                        suggestions.append("ğŸ“Š ì‹ ë¢°ë„ ê°œì„ : ì¶œì²˜ë¥¼ ëª…í™•íˆ í•˜ê³ , ì „ë¬¸ê°€ ì˜ê²¬ì´ë‚˜ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
                        suggestions.append("ğŸ”— ì°¸ê³ ë¬¸í—Œì´ë‚˜ ê´€ë ¨ ë§í¬ë¥¼ í¬í•¨í•˜ì—¬ ì •ë³´ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ì„¸ìš”.")
                    
                    elif dimension == 'usefulness':
                        suggestions.append("ğŸ’¡ ìœ ìš©ì„± ê°œì„ : ì‹¤ìš©ì ì¸ íŒì´ë‚˜ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ë” ì¶”ê°€í•˜ì„¸ìš”.")
                        suggestions.append("ğŸ¯ ë…ìê°€ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”.")
                    
                    elif dimension == 'accuracy':
                        suggestions.append("âœ… ì •í™•ì„± ê°œì„ : ìµœì‹  ì •ë³´ë¡œ ì—…ë°ì´íŠ¸í•˜ê³  ì‚¬ì‹¤ í™•ì¸ì„ ê°•í™”í•˜ì„¸ìš”.")
                        suggestions.append("ğŸ” ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì˜ ì •ë°€ë„ë¥¼ ë†’ì´ê³  ì˜¤ë¥˜ë¥¼ ì ê²€í•˜ì„¸ìš”.")
                    
                    elif dimension == 'completeness':
                        suggestions.append("ğŸ“ ì™„ì„±ë„ ê°œì„ : ë” ìƒì„¸í•œ ì„¤ëª…ê³¼ ì˜ˆì œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
                        suggestions.append("ğŸ“‹ ë‹¨ê³„ë³„ ê°€ì´ë“œë‚˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì„¸ìš”.")
                    
                    elif dimension == 'readability':
                        suggestions.append("ğŸ“– ê°€ë…ì„± ê°œì„ : ëª…í™•í•œ ì œëª©ê³¼ ì†Œì œëª©ìœ¼ë¡œ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ì„¸ìš”.")
                        suggestions.append("ğŸ¨ ì‹œê°ì  ìš”ì†Œ(ì´ë¯¸ì§€, ì°¨íŠ¸, ëª©ë¡)ë¥¼ í™œìš©í•˜ì„¸ìš”.")
                    
                    elif dimension == 'originality':
                        suggestions.append("ğŸ’­ ë…ì°½ì„± ê°œì„ : ê°œì¸ì  ê²½í—˜ì´ë‚˜ ë…íŠ¹í•œ ê´€ì ì„ ì¶”ê°€í•˜ì„¸ìš”.")
                        suggestions.append("ğŸŒŸ ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë‚˜ ì°½ì˜ì  ì ‘ê·¼ë²•ì„ ì‹œë„í•˜ì„¸ìš”.")
            
            # ì½˜í…ì¸  íƒ€ì…ë³„ íŠ¹í™” ì œì•ˆ
            if content_type == 'news':
                suggestions.append("ğŸ“° ë‰´ìŠ¤ íŠ¹í™”: 5W1H(ëˆ„ê°€, ì–¸ì œ, ì–´ë””ì„œ, ë¬´ì—‡ì„, ì™œ, ì–´ë–»ê²Œ)ë¥¼ ëª…í™•íˆ í•˜ì„¸ìš”.")
            elif content_type == 'blog':
                suggestions.append("âœï¸ ë¸”ë¡œê·¸ íŠ¹í™”: ê°œì¸ì  ìŠ¤í† ë¦¬í…”ë§ê³¼ ë…ìì™€ì˜ ì†Œí†µì„ ê°•í™”í•˜ì„¸ìš”.")
            elif content_type == 'tech_doc':
                suggestions.append("âš™ï¸ ê¸°ìˆ ë¬¸ì„œ íŠ¹í™”: ì½”ë“œ ì˜ˆì œì™€ ë‹¨ê³„ë³„ ì„¤ëª…ì„ ë” ìƒì„¸íˆ ì œê³µí•˜ì„¸ìš”.")
            elif content_type == 'tutorial':
                suggestions.append("ğŸ“ íŠœí† ë¦¬ì–¼ íŠ¹í™”: í•™ìŠµìê°€ ë”°ë¼í•  ìˆ˜ ìˆëŠ” ëª…í™•í•œ ë‹¨ê³„ë¥¼ ì œì‹œí•˜ì„¸ìš”.")
            
            # ì–¸ì–´ í’ˆì§ˆ ê´€ë ¨ ì œì•ˆ
            suggestions.append("ğŸ“š ì–¸ì–´ í’ˆì§ˆ: ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì„ ì¬ì ê²€í•˜ê³  ë‹¤ì–‘í•œ ì–´íœ˜ë¥¼ í™œìš©í•˜ì„¸ìš”.")
            
            return suggestions[:5]  # ìµœëŒ€ 5ê°œ ì œì•ˆ
            
        except Exception as e:
            logger.error(f"ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„± ì˜¤ë¥˜: {e}")
            return ["í’ˆì§ˆ ê°œì„ ì„ ìœ„í•´ ë‚´ìš©ì„ ì¬ê²€í† í•´ ì£¼ì„¸ìš”."]
    
    def _generate_quality_report(self, dimension_scores: Dict[str, float], 
                               detailed_analysis: Dict[str, Dict], 
                               language_quality: Dict[str, Any],
                               quality_grade: str, content_type: str) -> str:
        """í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            report_lines = []
            
            # ì „ì²´ ë“±ê¸‰ ë° ìš”ì•½
            overall_score = sum(dimension_scores.values()) / len(dimension_scores) if dimension_scores else 50.0
            report_lines.append(f"ğŸ† **ì „ì²´ í’ˆì§ˆ ë“±ê¸‰: {quality_grade}** (ì ìˆ˜: {overall_score:.1f}/100)")
            report_lines.append(f"ğŸ“„ ì½˜í…ì¸  íƒ€ì…: {content_type}")
            report_lines.append("")
            
            # ì°¨ì›ë³„ ìƒì„¸ ë¶„ì„
            report_lines.append("ğŸ“Š **ì°¨ì›ë³„ ë¶„ì„ ê²°ê³¼**")
            
            dimension_names = {
                'reliability': 'ì‹ ë¢°ë„',
                'usefulness': 'ìœ ìš©ì„±', 
                'accuracy': 'ì •í™•ì„±',
                'completeness': 'ì™„ì„±ë„',
                'readability': 'ê°€ë…ì„±',
                'originality': 'ë…ì°½ì„±'
            }
            
            for dimension, score in dimension_scores.items():
                name = dimension_names.get(dimension, dimension)
                grade = self._determine_quality_grade(score)
                
                if score >= 80:
                    emoji = "ğŸŸ¢"
                elif score >= 60:
                    emoji = "ğŸŸ¡"
                else:
                    emoji = "ğŸ”´"
                
                report_lines.append(f"{emoji} **{name}**: {grade} ({score:.1f}ì )")
                
                # ì„¸ë¶€ ë¶„ì„ ì •ë³´
                if dimension in detailed_analysis:
                    details = detailed_analysis[dimension]
                    positive_aspects = [k for k, v in details.items() if isinstance(v, (int, float)) and v > 0]
                    if positive_aspects:
                        report_lines.append(f"   âœ… ê°•ì : {', '.join(positive_aspects[:3])}")
            
            report_lines.append("")
            
            # ì–¸ì–´ í’ˆì§ˆ ë¶„ì„
            if language_quality:
                report_lines.append("ğŸ“ **ì–¸ì–´ í’ˆì§ˆ ë¶„ì„**")
                lang_score = language_quality.get('overall_language_score', 50)
                lang_grade = self._determine_quality_grade(lang_score)
                
                if lang_score >= 80:
                    emoji = "ğŸŸ¢"
                elif lang_score >= 60:
                    emoji = "ğŸŸ¡"
                else:
                    emoji = "ğŸ”´"
                
                report_lines.append(f"{emoji} **ì–¸ì–´ í’ˆì§ˆ**: {lang_grade} ({lang_score:.1f}ì )")
                
                if language_quality.get('grammar_errors', 0) > 0:
                    report_lines.append(f"   âš ï¸ ë¬¸ë²• ì˜¤ë¥˜: {language_quality['grammar_errors']}ê°œ")
                if language_quality.get('spelling_errors', 0) > 0:
                    report_lines.append(f"   âš ï¸ ë§ì¶¤ë²• ì˜¤ë¥˜: {language_quality['spelling_errors']}ê°œ")
                if language_quality.get('good_expressions', 0) > 0:
                    report_lines.append(f"   âœ… ì¢‹ì€ í‘œí˜„: {language_quality['good_expressions']}ê°œ")
                
                vocab_diversity = language_quality.get('vocabulary_diversity', 0)
                if vocab_diversity > 0:
                    report_lines.append(f"   ğŸ“š ì–´íœ˜ ë‹¤ì–‘ì„±: {vocab_diversity:.1f}%")
            
            report_lines.append("")
            
            # ì¢…í•© í‰ê°€
            report_lines.append("ğŸ¯ **ì¢…í•© í‰ê°€**")
            if overall_score >= 85:
                report_lines.append("ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ì½˜í…ì¸ ì…ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            elif overall_score >= 70:
                report_lines.append("ì–‘í˜¸í•œ í’ˆì§ˆì˜ ì½˜í…ì¸ ì…ë‹ˆë‹¤. ì¼ë¶€ ì˜ì—­ì—ì„œ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
            elif overall_score >= 60:
                report_lines.append("ë³´í†µ ìˆ˜ì¤€ì˜ ì½˜í…ì¸ ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ì˜ì—­ì—ì„œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                report_lines.append("í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•œ ì½˜í…ì¸ ì…ë‹ˆë‹¤. ì „ë°˜ì ì¸ ì¬ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            
            return "\n".join(report_lines)
            
        except Exception as e:
            logger.error(f"í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    # =================== ë©”ì¸ ë¶„ì„ ë©”ì„œë“œ ===================
    
    async def analyze_web_content(self, url: str, use_ai: bool = True) -> Optional[ContentAnalysisResult]:
        """ì›¹ ì½˜í…ì¸  ì¢…í•© ë¶„ì„ - AI ì—”ì§„ í†µí•©"""
        start_time = datetime.now()
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(url, "full_analysis")
            if cache_key in self.analysis_cache:
                logger.info(f"ìºì‹œì—ì„œ ë¶„ì„ ê²°ê³¼ ë°˜í™˜: {url}")
                return self.analysis_cache[cache_key]
            
            # ì›¹ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            content_data = await self._fetch_web_content(url)
            if not content_data:
                return self._create_error_result(url, "ì›¹ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = content_data.get('title', '')
            content = content_data.get('content', '')
            html_content = content_data.get('html', '')
            
            # 1. ì½˜í…ì¸  íƒ€ì… ê°ì§€ (2ë‹¨ê³„ì—ì„œ êµ¬í˜„í•œ ê³ ê¸‰ ë¶„ë¥˜)
            soup = BeautifulSoup(html_content, 'html.parser') if html_content else None
            content_type = self._detect_content_type(title, content, url, soup)
            
            # 2. êµ¬ì¡° ë¶„ì„ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            structure_info = self._analyze_content_structure(html_content, url) if html_content else {}
            metadata = self._extract_metadata(html_content) if html_content else {}
            
            # 3. ê¸°ë³¸ ë¶„ì„
            sentiment_score, sentiment_label = self._calculate_sentiment_score(content)
            quality_score = self._calculate_quality_score(title, content, url)
            complexity_level = self._determine_complexity_level(content)
            reading_time = self._calculate_reading_time(content)
            language = self._detect_content_language_advanced(content, metadata)
            topics = self._extract_topics(content)
            
            # =================== 4ë‹¨ê³„ ì¶”ê°€: ê³ ê¸‰ ê°ì • ë¶„ì„ í†µí•© ===================
            # ê³ ê¸‰ ê°ì • ë¶„ì„ ìˆ˜í–‰
            advanced_sentiment = self._calculate_advanced_sentiment_score(content)
            
            # AI ê¸°ë°˜ ê°ì • ë¶„ì„ (ì„ íƒì )
            ai_sentiment = {}
            if use_ai and content:
                ai_sentiment = await self._perform_ai_sentiment_analysis(content, content_type)
            
            # ê°ì • ë¶„ì„ ê²°ê³¼ í†µí•©
            final_sentiment_score = advanced_sentiment.get('basic_sentiment_score', sentiment_score)
            final_sentiment_label = advanced_sentiment.get('basic_sentiment_label', sentiment_label)
            detailed_emotions = advanced_sentiment.get('detailed_emotions', {})
            emotion_intensity = advanced_sentiment.get('emotion_intensity', 0.0)
            emotion_confidence = advanced_sentiment.get('emotion_confidence', 0.0)
            dominant_emotion = advanced_sentiment.get('dominant_emotion', 'neutral')
            emotion_distribution = advanced_sentiment.get('emotion_distribution', {})
            contextual_sentiment = advanced_sentiment.get('contextual_sentiment', 'direct')
            
            # AI ê°ì • ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ê°€ì¤‘í‰ê· ìœ¼ë¡œ í†µí•©
            if ai_sentiment:
                ai_confidence = ai_sentiment.get('ai_confidence', 0.0)
                rule_confidence = emotion_confidence
                
                # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘í‰ê· 
                if ai_confidence > 0.7 and rule_confidence > 0.0:
                    total_confidence = ai_confidence + rule_confidence
                    emotion_confidence = (ai_confidence * ai_confidence + rule_confidence * rule_confidence) / total_confidence
                    
                    # AI ê²°ê³¼ê°€ ë” ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ ì£¼ìš” ê°ì • ì—…ë°ì´íŠ¸
                    if ai_confidence > rule_confidence:
                        dominant_emotion = ai_sentiment.get('ai_dominant_emotion', dominant_emotion)
                        emotion_intensity = (emotion_intensity + ai_sentiment.get('ai_intensity', 0.0)) / 2
            
            # 4. ì½˜í…ì¸  ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = self._calculate_content_metrics(content, structure_info)
            
            # 5. AI ê¸°ë°˜ ê³ ê¸‰ ë¶„ì„ (3ë‹¨ê³„ ì‹ ê·œ ê¸°ëŠ¥)
            ai_analysis = {}
            if use_ai and content:
                ai_analysis = await self._perform_ai_analysis(title, content, url, content_type)
            
            # 6. ë¶„ì„ ê²°ê³¼ í†µí•©
            analysis_result = ContentAnalysisResult(
                url=url,
                title=title,
                content_type=content_type,
                summary=ai_analysis.get('summary', content[:200] + "..."),
                key_points=ai_analysis.get('key_points', topics[:3]),
                sentiment_score=final_sentiment_score,
                sentiment_label=final_sentiment_label,
                quality_score=quality_score,
                topics=topics,
                language=language,
                reading_time=reading_time,
                complexity_level=complexity_level,
                analysis_timestamp=datetime.now().isoformat(),
                ai_model_used="gpt-4" if use_ai and ai_analysis else "rule-based",
                word_count=metrics.get('word_count', 0),
                image_count=metrics.get('image_count', 0),
                link_count=metrics.get('link_count', 0),
                error_message="",
                
                # =================== 4ë‹¨ê³„ ì¶”ê°€: ê³ ê¸‰ ê°ì • ë¶„ì„ í•„ë“œ ===================
                detailed_emotions=detailed_emotions,
                emotion_intensity=emotion_intensity,
                emotion_confidence=emotion_confidence,
                dominant_emotion=dominant_emotion,
                emotion_distribution=emotion_distribution,
                contextual_sentiment=contextual_sentiment,
                
                # =================== 4ë‹¨ê³„ ì¶”ê°€: ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ í•„ë“œ (ì„ì‹œ) ===================
                quality_dimensions={},  # ë‹¤ìŒ ì‘ì—…ì—ì„œ êµ¬í˜„ ì˜ˆì •
                quality_report="",
                improvement_suggestions=[],
                content_reliability=0.0,
                content_usefulness=0.0,
                content_accuracy=0.0
            )
            
            # 7. ìºì‹œì— ì €ì¥
            self.analysis_cache[cache_key] = analysis_result
            
            # 8. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_analysis_stats(analysis_result, start_time)
            
            logger.info(f"ì›¹ ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ: {url} (íƒ€ì…: {content_type}, í’ˆì§ˆ: {quality_score:.1f})")
            return analysis_result
            
        except Exception as e:
            logger.error(f"ì›¹ ì½˜í…ì¸  ë¶„ì„ ì˜¤ë¥˜: {e}")
            self.analysis_stats['failed_analyses'] += 1
            return self._create_error_result(url, f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    async def _fetch_web_content(self, url: str) -> Optional[Dict[str, str]]:
        """ì›¹ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° - web_search_ide í™œìš©"""
        try:
            # web_search_ideì˜ visit_site ë©”ì„œë“œ í™œìš©
            result = self.web_search.visit_site("content_analyzer", url, extract_code=False)
            
            if result.get('success'):
                return {
                    'title': result.get('title', ''),
                    'content': result.get('content_preview', ''),
                    'html': result.get('html_content', ''),  # ì „ì²´ HTMLì´ í•„ìš”í•œ ê²½ìš°
                    'url': url
                }
            else:
                # ëŒ€ì•ˆ: ì§ì ‘ HTTP ìš”ì²­
                return await self._fetch_content_direct(url)
                
        except Exception as e:
            logger.error(f"ì›¹ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return await self._fetch_content_direct(url)
    
    async def _fetch_content_direct(self, url: str) -> Optional[Dict[str, str]]:
        """ì§ì ‘ HTTP ìš”ì²­ìœ¼ë¡œ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        
                        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
                        soup = BeautifulSoup(html_content, 'html.parser')
                        
                        # ì œëª© ì¶”ì¶œ
                        title_tag = soup.find('title')
                        title = title_tag.get_text().strip() if title_tag else "ì œëª© ì—†ìŒ"
                        
                        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        for script in soup(["script", "style"]):
                            script.decompose()
                        
                        text_content = soup.get_text()
                        clean_text = ' '.join(text_content.split())
                        
                        return {
                            'title': title,
                            'content': clean_text,
                            'html': html_content,
                            'url': url
                        }
                    else:
                        logger.warning(f"HTTP {response.status}: {url}")
                        return None
                        
        except Exception as e:
            logger.error(f"ì§ì ‘ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    def _create_error_result(self, url: str, error_message: str) -> ContentAnalysisResult:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        return ContentAnalysisResult(
            url=url,
            title="ë¶„ì„ ì‹¤íŒ¨",
            content_type="error",
            summary="",
            key_points=[],
            sentiment_score=0.0,
            sentiment_label="neutral",
            quality_score=0.0,
            topics=[],
            language="unknown",
            reading_time=0,
            complexity_level="unknown",
            analysis_timestamp=datetime.now().isoformat(),
            ai_model_used="none",
            word_count=0,
            image_count=0,
            link_count=0,
            error_message=error_message
        )
    
    def _update_analysis_stats(self, result: ContentAnalysisResult, start_time: datetime):
        """ë¶„ì„ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            processing_time = (datetime.now() - start_time).total_seconds()
            
            self.analysis_stats['total_analyzed'] += 1
            if not result.error_message:
                self.analysis_stats['successful_analyses'] += 1
            
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            current_avg = self.analysis_stats['average_processing_time']
            total = self.analysis_stats['total_analyzed']
            self.analysis_stats['average_processing_time'] = (current_avg * (total - 1) + processing_time) / total
            
            # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸
            if result.quality_score > 0:
                current_quality_avg = self.analysis_stats['average_quality_score']
                success_count = self.analysis_stats['successful_analyses']
                self.analysis_stats['average_quality_score'] = (current_quality_avg * (success_count - 1) + result.quality_score) / success_count
            
        except Exception as e:
            logger.error(f"í†µê³„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")

    # =================== ë°°ì¹˜ ë¶„ì„ ë©”ì„œë“œ ===================
    
    async def analyze_batch_urls(self, urls: List[str], max_concurrent: int = 5) -> BatchAnalysisReport:
        """ë°°ì¹˜ URL ë¶„ì„"""
        start_time = datetime.now()
        
        try:
            # ìºì‹œ í™•ì¸
            urls_key = hashlib.md5(str(sorted(urls)).encode()).hexdigest()
            cache_key = f"batch_{urls_key}"
            
            if cache_key in self.batch_cache:
                logger.info("ë°°ì¹˜ ë¶„ì„ ìºì‹œ ê²°ê³¼ ë°˜í™˜")
                return self.batch_cache[cache_key]
            
            # ë¹„ë™ê¸° ë°°ì¹˜ ë¶„ì„
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def analyze_single_url(url):
                async with semaphore:
                    return await self.analyze_web_content(url)
            
            # ëª¨ë“  URL ë™ì‹œ ë¶„ì„
            tasks = [analyze_single_url(url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì§‘ê³„
            successful_results = []
            failed_count = 0
            
            for result in results:
                if isinstance(result, ContentAnalysisResult) and not result.error_message:
                    successful_results.append(result)
                else:
                    failed_count += 1
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = self._generate_batch_report(successful_results, failed_count, len(urls), start_time)
            
            # ìºì‹œ ì €ì¥
            self.batch_cache[cache_key] = report
            
            logger.info(f"ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(successful_results)}/{len(urls)} ì„±ê³µ")
            return report
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return self._create_error_batch_report(len(urls), str(e))
    
    def _generate_batch_report(self, results: List[ContentAnalysisResult], failed_count: int, total_count: int, start_time: datetime) -> BatchAnalysisReport:
        """ë°°ì¹˜ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            processing_time = (datetime.now() - start_time).total_seconds()
            
            if not results:
                return BatchAnalysisReport(
                    total_analyzed=total_count,
                    success_count=0,
                    failed_count=failed_count,
                    average_quality_score=0.0,
                    dominant_sentiment="neutral",
                    top_topics=[],
                    content_type_distribution={},
                    analysis_summary="ë¶„ì„ëœ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    processing_time=processing_time,
                    report_timestamp=datetime.now().isoformat()
                )
            
            # í†µê³„ ê³„ì‚°
            avg_quality = sum(r.quality_score for r in results) / len(results)
            
            # ê°ì • ë¶„í¬
            sentiment_counts = {}
            for result in results:
                sentiment = result.sentiment_label
                sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
            dominant_sentiment = max(sentiment_counts, key=sentiment_counts.get)
            
            # í† í”½ ì§‘ê³„
            topic_counts = {}
            for result in results:
                for topic in result.topics:
                    topic_counts[topic] = topic_counts.get(topic, 0) + 1
            top_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            
            # ì½˜í…ì¸  íƒ€ì… ë¶„í¬
            type_distribution = {}
            for result in results:
                content_type = result.content_type
                type_distribution[content_type] = type_distribution.get(content_type, 0) + 1
            
            # ìš”ì•½ ìƒì„±
            summary = f"ì´ {len(results)}ê°œ ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ. í‰ê·  í’ˆì§ˆì ìˆ˜ {avg_quality:.1f}ì , ì£¼ìš” ê°ì • {dominant_sentiment}"
            
            return BatchAnalysisReport(
                total_analyzed=total_count,
                success_count=len(results),
                failed_count=failed_count,
                average_quality_score=avg_quality,
                dominant_sentiment=dominant_sentiment,
                top_topics=top_topics,
                content_type_distribution=type_distribution,
                analysis_summary=summary,
                processing_time=processing_time,
                report_timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._create_error_batch_report(total_count, str(e))
    
    def _create_error_batch_report(self, total_count: int, error_message: str) -> BatchAnalysisReport:
        """ì˜¤ë¥˜ ë°°ì¹˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        return BatchAnalysisReport(
            total_analyzed=total_count,
            success_count=0,
            failed_count=total_count,
            average_quality_score=0.0,
            dominant_sentiment="neutral",
            top_topics=[],
            content_type_distribution={},
            analysis_summary=f"ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {error_message}",
            processing_time=0.0,
            report_timestamp=datetime.now().isoformat()
        )
    
    # =================== 4ë‹¨ê³„ ì¶”ê°€: ì •êµí•œ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ===================
    
    def _calculate_advanced_quality_score(self, title: str, content: str, url: str, 
                                        content_type: str, structure_info: Dict = None) -> Dict[str, Any]:
        """ì •êµí•œ ë‹¤ì°¨ì› í’ˆì§ˆ í‰ê°€"""
        try:
            # ì½˜í…ì¸  íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            type_weights = self.content_type_quality_weights.get(content_type, 
                                                               self.content_type_quality_weights['general'])
            
            # ê° ì°¨ì›ë³„ ì ìˆ˜ ê³„ì‚°
            dimension_scores = {}
            detailed_analysis = {}
            
            # 1. ì‹ ë¢°ë„ (Reliability) í‰ê°€
            reliability_score, reliability_detail = self._evaluate_reliability(title, content, url, structure_info)
            dimension_scores['reliability'] = reliability_score * type_weights['reliability']
            detailed_analysis['reliability'] = reliability_detail
            
            # 2. ìœ ìš©ì„± (Usefulness) í‰ê°€
            usefulness_score, usefulness_detail = self._evaluate_usefulness(title, content, content_type)
            dimension_scores['usefulness'] = usefulness_score * type_weights['usefulness']
            detailed_analysis['usefulness'] = usefulness_detail
            
            # 3. ì •í™•ì„± (Accuracy) í‰ê°€
            accuracy_score, accuracy_detail = self._evaluate_accuracy(title, content, url)
            dimension_scores['accuracy'] = accuracy_score * type_weights['accuracy']
            detailed_analysis['accuracy'] = accuracy_detail
            
            # 4. ì™„ì„±ë„ (Completeness) í‰ê°€
            completeness_score, completeness_detail = self._evaluate_completeness(title, content, structure_info)
            dimension_scores['completeness'] = completeness_score * type_weights['completeness']
            detailed_analysis['completeness'] = completeness_detail
            
            # 5. ê°€ë…ì„± (Readability) í‰ê°€
            readability_score, readability_detail = self._evaluate_readability(title, content, structure_info)
            dimension_scores['readability'] = readability_score * type_weights['readability']
            detailed_analysis['readability'] = readability_detail
            
            # 6. ë…ì°½ì„± (Originality) í‰ê°€
            originality_score, originality_detail = self._evaluate_originality(title, content, content_type)
            dimension_scores['originality'] = originality_score * type_weights['originality']
            detailed_analysis['originality'] = originality_detail
            
            # ì–¸ì–´ í’ˆì§ˆ í‰ê°€
            language_quality = self._evaluate_language_quality(content)
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘í‰ê· )
            total_weight = sum(type_weights.values())
            overall_score = sum(dimension_scores.values()) / total_weight
            overall_score = min(100.0, max(0.0, overall_score))
            
            # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
            quality_grade = self._determine_quality_grade(overall_score)
            
            # ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„±
            improvement_suggestions = self._generate_improvement_suggestions(
                dimension_scores, detailed_analysis, content_type
            )
            
            # í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
            quality_report = self._generate_quality_report(
                dimension_scores, detailed_analysis, language_quality, 
                quality_grade, content_type
            )
            
            return {
                'overall_score': overall_score,
                'quality_grade': quality_grade,
                'dimension_scores': dimension_scores,
                'detailed_analysis': detailed_analysis,
                'language_quality': language_quality,
                'improvement_suggestions': improvement_suggestions,
                'quality_report': quality_report,
                'content_type_weights': type_weights
            }
            
        except Exception as e:
            logger.error(f"ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {e}")
            return {
                'overall_score': 50.0,
                'quality_grade': 'C',
                'dimension_scores': {},
                'detailed_analysis': {},
                'language_quality': {},
                'improvement_suggestions': ['í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'],
                'quality_report': 'í’ˆì§ˆ í‰ê°€ë¥¼ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'content_type_weights': {}
            }
    
    def _evaluate_reliability(self, title: str, content: str, url: str, structure_info: Dict = None) -> Tuple[float, Dict]:
        """ì‹ ë¢°ë„ í‰ê°€"""
        try:
            score = 50.0  # ê¸°ë³¸ ì ìˆ˜
            details = {}
            
            domain = urlparse(url).netloc.lower()
            text_lower = f"{title} {content}".lower()
            
            # ì¶œì²˜ ì‹ ë¢°ë„ í‰ê°€
            source_score = 0
            credible_domains = self.quality_dimensions['reliability']['indicators']['source_credibility']
            for domain_type in credible_domains:
                if domain_type in domain:
                    source_score += 20
                    break
            else:
                # ì•Œë ¤ì§„ ì‹ ë¢°í•  ë§Œí•œ ì‚¬ì´íŠ¸ ì²´í¬
                if any(trusted in domain for trusted in ['naver', 'google', 'wikipedia', 'github']):
                    source_score += 10
            
            details['source_credibility'] = source_score
            score += source_score
            
            # ì €ì ì „ë¬¸ì„± í‰ê°€
            expertise_score = 0
            expertise_indicators = self.quality_dimensions['reliability']['indicators']['author_expertise']
            expertise_matches = sum(1 for indicator in expertise_indicators if indicator in text_lower)
            expertise_score = min(15, expertise_matches * 5)
            
            details['author_expertise'] = expertise_score
            score += expertise_score
            
            # ì¸ìš© ë° ì°¸ê³ ë¬¸í—Œ í’ˆì§ˆ
            citation_score = 0
            citation_indicators = self.quality_dimensions['reliability']['indicators']['citation_quality']
            citation_matches = sum(1 for indicator in citation_indicators if indicator in text_lower)
            citation_score = min(10, citation_matches * 3)
            
            details['citation_quality'] = citation_score
            score += citation_score
            
            # ì‚¬ì‹¤ í™•ì¸ ë° ë°ì´í„° ê¸°ë°˜
            fact_score = 0
            fact_indicators = self.quality_dimensions['reliability']['indicators']['fact_checking']
            fact_matches = sum(1 for indicator in fact_indicators if indicator in text_lower)
            fact_score = min(10, fact_matches * 2)
            
            details['fact_checking'] = fact_score
            score += fact_score
            
            # HTTPS ì‚¬ìš© (ë³´ì•ˆ)
            if url.startswith('https'):
                score += 5
                details['security'] = 5
            else:
                details['security'] = 0
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ì‹ ë¢°ë„ í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_usefulness(self, title: str, content: str, content_type: str) -> Tuple[float, Dict]:
        """ìœ ìš©ì„± í‰ê°€"""
        try:
            score = 50.0
            details = {}
            text_lower = f"{title} {content}".lower()
            
            # ì‹¤ìš©ì  ê°€ì¹˜
            practical_score = 0
            practical_indicators = self.quality_dimensions['usefulness']['indicators']['practical_value']
            practical_matches = sum(1 for indicator in practical_indicators if indicator in text_lower)
            practical_score = min(25, practical_matches * 4)
            
            details['practical_value'] = practical_score
            score += practical_score
            
            # ì‹¤í–‰ ê°€ëŠ¥í•œ ë‚´ìš©
            actionable_score = 0
            actionable_indicators = self.quality_dimensions['usefulness']['indicators']['actionable_content']
            actionable_matches = sum(1 for indicator in actionable_indicators if indicator in text_lower)
            actionable_score = min(20, actionable_matches * 3)
            
            details['actionable_content'] = actionable_score
            score += actionable_score
            
            # ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
            problem_solving_score = 0
            problem_indicators = self.quality_dimensions['usefulness']['indicators']['problem_solving']
            problem_matches = sum(1 for indicator in problem_indicators if indicator in text_lower)
            problem_solving_score = min(15, problem_matches * 3)
            
            details['problem_solving'] = problem_solving_score
            score += problem_solving_score
            
            # í•™ìŠµ ê°€ì¹˜
            learning_score = 0
            learning_indicators = self.quality_dimensions['usefulness']['indicators']['learning_value']
            learning_matches = sum(1 for indicator in learning_indicators if indicator in text_lower)
            learning_score = min(10, learning_matches * 2)
            
            details['learning_value'] = learning_score
            score += learning_score
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ìœ ìš©ì„± í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_accuracy(self, title: str, content: str, url: str) -> Tuple[float, Dict]:
        """ì •í™•ì„± í‰ê°€"""
        try:
            score = 50.0
            details = {}
            text_lower = f"{title} {content}".lower()
            
            # ê¸°ìˆ ì  ì •ë°€ì„±
            precision_score = 0
            precision_indicators = self.quality_dimensions['accuracy']['indicators']['technical_precision']
            precision_matches = sum(1 for indicator in precision_indicators if indicator in text_lower)
            precision_score = min(25, precision_matches * 8)
            
            details['technical_precision'] = precision_score
            score += precision_score
            
            # ì˜¤ë¥˜ ì§€ì‹œì–´ í™•ì¸ (ë¶€ì •ì  í‰ê°€)
            error_penalty = 0
            error_indicators = self.quality_dimensions['accuracy']['indicators']['error_indicators']
            error_matches = sum(1 for indicator in error_indicators if indicator in text_lower)
            error_penalty = min(20, error_matches * 5)
            
            details['error_indicators'] = -error_penalty
            score -= error_penalty
            
            # ê²€ì¦ í‘œì‹œ
            verification_score = 0
            verification_indicators = self.quality_dimensions['accuracy']['indicators']['verification_marks']
            verification_matches = sum(1 for indicator in verification_indicators if indicator in text_lower)
            verification_score = min(15, verification_matches * 5)
            
            details['verification_marks'] = verification_score
            score += verification_score
            
            # ìµœì‹ ì„± í‰ê°€
            update_score = 0
            update_indicators = self.quality_dimensions['accuracy']['indicators']['update_frequency']
            update_matches = sum(1 for indicator in update_indicators if indicator in text_lower)
            update_score = min(10, update_matches * 3)
            
            details['update_frequency'] = update_score
            score += update_score
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ì •í™•ì„± í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_completeness(self, title: str, content: str, structure_info: Dict = None) -> Tuple[float, Dict]:
        """ì™„ì„±ë„ í‰ê°€"""
        try:
            score = 50.0
            details = {}
            text_lower = f"{title} {content}".lower()
            
            # í¬ê´„ì  ì»¤ë²„ë¦¬ì§€
            coverage_score = 0
            coverage_indicators = self.quality_dimensions['completeness']['indicators']['comprehensive_coverage']
            coverage_matches = sum(1 for indicator in coverage_indicators if indicator in text_lower)
            coverage_score = min(25, coverage_matches * 8)
            
            details['comprehensive_coverage'] = coverage_score
            score += coverage_score
            
            # ìƒì„¸í•œ ì„¤ëª…
            detail_score = 0
            detail_indicators = self.quality_dimensions['completeness']['indicators']['detailed_explanation']
            detail_matches = sum(1 for indicator in detail_indicators if indicator in text_lower)
            detail_score = min(20, detail_matches * 7)
            
            details['detailed_explanation'] = detail_score
            score += detail_score
            
            # ì˜ˆì œ ì œê³µ
            example_score = 0
            example_indicators = self.quality_dimensions['completeness']['indicators']['example_provision']
            example_matches = sum(1 for indicator in example_indicators if indicator in text_lower)
            example_score = min(15, example_matches * 5)
            
            details['example_provision'] = example_score
            score += example_score
            
            # ë‹¨ê³„ë³„ ì„¤ëª…
            step_score = 0
            step_indicators = self.quality_dimensions['completeness']['indicators']['step_by_step']
            step_matches = sum(1 for indicator in step_indicators if indicator in text_lower)
            step_score = min(10, step_matches * 3)
            
            details['step_by_step'] = step_score
            score += step_score
            
            # ì½˜í…ì¸  ê¸¸ì´ ë³´ë„ˆìŠ¤
            content_length = len(content)
            if content_length > 2000:
                length_bonus = min(10, (content_length - 2000) // 1000 * 2)
                score += length_bonus
                details['content_length_bonus'] = length_bonus
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ì™„ì„±ë„ í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_readability(self, title: str, content: str, structure_info: Dict = None) -> Tuple[float, Dict]:
        """ê°€ë…ì„± í‰ê°€"""
        try:
            score = 50.0
            details = {}
            text_lower = f"{title} {content}".lower()
            
            # ëª…í™•í•œ êµ¬ì¡°
            structure_score = 0
            structure_indicators = self.quality_dimensions['readability']['indicators']['clear_structure']
            structure_matches = sum(1 for indicator in structure_indicators if indicator in text_lower)
            structure_score = min(25, structure_matches * 8)
            
            details['clear_structure'] = structure_score
            score += structure_score
            
            # ê°„ë‹¨í•œ ì–¸ì–´
            language_score = 0
            language_indicators = self.quality_dimensions['readability']['indicators']['simple_language']
            language_matches = sum(1 for indicator in language_indicators if indicator in text_lower)
            language_score = min(20, language_matches * 7)
            
            details['simple_language'] = language_score
            score += language_score
            
            # ì‹œê°ì  ë„êµ¬
            visual_score = 0
            visual_indicators = self.quality_dimensions['readability']['indicators']['visual_aids']
            visual_matches = sum(1 for indicator in visual_indicators if indicator in text_lower)
            visual_score = min(15, visual_matches * 5)
            
            details['visual_aids'] = visual_score
            score += visual_score
            
            # í¬ë§·íŒ…
            formatting_score = 0
            formatting_indicators = self.quality_dimensions['readability']['indicators']['formatting']
            formatting_matches = sum(1 for indicator in formatting_indicators if indicator in text_lower)
            formatting_score = min(10, formatting_matches * 3)
            
            # êµ¬ì¡° ì •ë³´ í™œìš©
            if structure_info:
                if structure_info.get('heading_count', 0) > 2:
                    formatting_score += 5
                if structure_info.get('list_count', 0) > 0:
                    formatting_score += 3
                if structure_info.get('paragraph_count', 0) > 3:
                    formatting_score += 2
            
            details['formatting'] = formatting_score
            score += formatting_score
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ê°€ë…ì„± í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_originality(self, title: str, content: str, content_type: str) -> Tuple[float, Dict]:
        """ë…ì°½ì„± í‰ê°€"""
        try:
            score = 50.0
            details = {}
            text_lower = f"{title} {content}".lower()
            
            # ë…íŠ¹í•œ ê´€ì 
            perspective_score = 0
            perspective_indicators = self.quality_dimensions['originality']['indicators']['unique_perspective']
            perspective_matches = sum(1 for indicator in perspective_indicators if indicator in text_lower)
            perspective_score = min(25, perspective_matches * 8)
            
            details['unique_perspective'] = perspective_score
            score += perspective_score
            
            # ê°œì¸ì  í†µì°°
            insight_score = 0
            insight_indicators = self.quality_dimensions['originality']['indicators']['personal_insight']
            insight_matches = sum(1 for indicator in insight_indicators if indicator in text_lower)
            insight_score = min(20, insight_matches * 7)
            
            details['personal_insight'] = insight_score
            score += insight_score
            
            # ì°½ì˜ì  ì ‘ê·¼
            creative_score = 0
            creative_indicators = self.quality_dimensions['originality']['indicators']['creative_approach']
            creative_matches = sum(1 for indicator in creative_indicators if indicator in text_lower)
            creative_score = min(15, creative_matches * 5)
            
            details['creative_approach'] = creative_score
            score += creative_score
            
            # ì‚¬ê³  ìê·¹ì 
            thought_score = 0
            thought_indicators = self.quality_dimensions['originality']['indicators']['thought_provoking']
            thought_matches = sum(1 for indicator in thought_indicators if indicator in text_lower)
            thought_score = min(10, thought_matches * 3)
            
            details['thought_provoking'] = thought_score
            score += thought_score
            
            # ì½˜í…ì¸  íƒ€ì…ë³„ ì¡°ì •
            if content_type == 'blog':
                score += 10  # ë¸”ë¡œê·¸ëŠ” ê°œì¸ì  íŠ¹ì„±ì´ ì¤‘ìš”
            elif content_type == 'news':
                score -= 5   # ë‰´ìŠ¤ëŠ” ê°ê´€ì„±ì´ ë” ì¤‘ìš”
            
            return min(100.0, max(0.0, score)), details
            
        except Exception as e:
            logger.error(f"ë…ì°½ì„± í‰ê°€ ì˜¤ë¥˜: {e}")
            return 50.0, {'error': str(e)}
    
    def _evaluate_language_quality(self, content: str) -> Dict[str, Any]:
        """ì–¸ì–´ í’ˆì§ˆ í‰ê°€"""
        try:
            analysis = {
                'grammar_errors': 0,
                'spelling_errors': 0,
                'good_expressions': 0,
                'vocabulary_diversity': 0,
                'sentence_variety': 0,
                'overall_language_score': 50.0
            }
            
            # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
            for pattern in self.language_quality_indicators['grammar_errors']:
                matches = len(re.findall(pattern, content, re.IGNORECASE))
                analysis['grammar_errors'] += matches
            
            # ë§ì¶¤ë²• ì˜¤ë¥˜ ê²€ì‚¬
            for pattern in self.language_quality_indicators['spelling_errors']:
                matches = len(re.findall(pattern, content, re.IGNORECASE))
                analysis['spelling_errors'] += matches
            
            # ì¢‹ì€ í‘œí˜„ í™•ì¸
            for pattern in self.language_quality_indicators['good_expressions']:
                matches = len(re.findall(pattern, content, re.IGNORECASE))
                analysis['good_expressions'] += matches
            
            # ì–´íœ˜ ë‹¤ì–‘ì„± (ê³ ìœ  ë‹¨ì–´ ìˆ˜ / ì „ì²´ ë‹¨ì–´ ìˆ˜)
            words = re.findall(r'[ê°€-í£a-zA-Z]+', content)
            if words:
                unique_words = set(words)
                analysis['vocabulary_diversity'] = len(unique_words) / len(words) * 100
            
            # ë¬¸ì¥ ë‹¤ì–‘ì„± (ë¬¸ì¥ ê¸¸ì´ì˜ í‘œì¤€í¸ì°¨)
            sentences = re.split(r'[.!?]', content)
            if len(sentences) > 1:
                sentence_lengths = [len(s.strip()) for s in sentences if s.strip()]
                if sentence_lengths:
                    import statistics
                    analysis['sentence_variety'] = statistics.stdev(sentence_lengths) if len(sentence_lengths) > 1 else 0
            
            # ì „ì²´ ì–¸ì–´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            score = 70.0
            score -= analysis['grammar_errors'] * 5  # ë¬¸ë²• ì˜¤ë¥˜ í˜ë„í‹°
            score -= analysis['spelling_errors'] * 3  # ë§ì¶¤ë²• ì˜¤ë¥˜ í˜ë„í‹°
            score += min(20, analysis['good_expressions'] * 2)  # ì¢‹ì€ í‘œí˜„ ë³´ë„ˆìŠ¤
            score += min(10, analysis['vocabulary_diversity'] * 0.2)  # ì–´íœ˜ ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
            
            analysis['overall_language_score'] = min(100.0, max(0.0, score))
            
            return analysis
            
        except Exception as e:
            logger.error(f"ì–¸ì–´ í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {e}")
            return {
                'grammar_errors': 0,
                'spelling_errors': 0,
                'good_expressions': 0,
                'vocabulary_diversity': 0,
                'sentence_variety': 0,
                'overall_language_score': 50.0,
                'error': str(e)
            }
    
    def _determine_quality_grade(self, score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if score >= 90:
            return 'A+'
        elif score >= 85:
            return 'A'
        elif score >= 80:
            return 'B+'
        elif score >= 75:
            return 'B'
        elif score >= 70:
            return 'C+'
        elif score >= 65:
            return 'C'
        elif score >= 60:
            return 'D+'
        elif score >= 55:
            return 'D'
        else:
            return 'F'
    
    def _generate_improvement_suggestions(self, dimension_scores: Dict[str, float], 
                                        detailed_analysis: Dict[str, Dict], 
                                        content_type: str) -> List[str]:
        """ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„±"""
        suggestions = []
        
        try:
            # ê° ì°¨ì›ë³„ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°œì„  ì œì•ˆ
            for dimension, score in dimension_scores.items():
                if score < 60:  # ë‚®ì€ ì ìˆ˜ì˜ ì°¨ì›ì— ëŒ€í•´ ì œì•ˆ
                    if dimension == 'reliability':
                        suggestions.append("ğŸ“Š ì‹ ë¢°ë„ ê°œì„ : ì¶œì²˜ë¥¼ ëª…í™•íˆ í•˜ê³ , ì „ë¬¸ê°€ ì˜ê²¬ì´ë‚˜ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
                        suggestions.append("ğŸ”— ì°¸ê³ ë¬¸í—Œì´ë‚˜ ê´€ë ¨ ë§í¬ë¥¼ í¬í•¨í•˜ì—¬ ì •ë³´ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ì„¸ìš”.")
                    
                    elif dimension == 'usefulness':
                        suggestions.append("ğŸ’¡ ìœ ìš©ì„± ê°œì„ : ì‹¤ìš©ì ì¸ íŒì´ë‚˜ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ë” ì¶”ê°€í•˜ì„¸ìš”.")
                        suggestions.append("ğŸ¯ ë…ìê°€ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”.")
                    
                    elif dimension == 'accuracy':
                        suggestions.append("âœ… ì •í™•ì„± ê°œì„ : ìµœì‹  ì •ë³´ë¡œ ì—…ë°ì´íŠ¸í•˜ê³  ì‚¬ì‹¤ í™•ì¸ì„ ê°•í™”í•˜ì„¸ìš”.")
                        suggestions.append("ğŸ” ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì˜ ì •ë°€ë„ë¥¼ ë†’ì´ê³  ì˜¤ë¥˜ë¥¼ ì ê²€í•˜ì„¸ìš”.")
                    
                    elif dimension == 'completeness':
                        suggestions.append("ğŸ“ ì™„ì„±ë„ ê°œì„ : ë” ìƒì„¸í•œ ì„¤ëª…ê³¼ ì˜ˆì œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
                        suggestions.append("ğŸ“‹ ë‹¨ê³„ë³„ ê°€ì´ë“œë‚˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì„¸ìš”.")
                    
                    elif dimension == 'readability':
                        suggestions.append("ğŸ“– ê°€ë…ì„± ê°œì„ : ëª…í™•í•œ ì œëª©ê³¼ ì†Œì œëª©ìœ¼ë¡œ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ì„¸ìš”.")
                        suggestions.append("ğŸ¨ ì‹œê°ì  ìš”ì†Œ(ì´ë¯¸ì§€, ì°¨íŠ¸, ëª©ë¡)ë¥¼ í™œìš©í•˜ì„¸ìš”.")
                    
                    elif dimension == 'originality':
                        suggestions.append("ğŸ’­ ë…ì°½ì„± ê°œì„ : ê°œì¸ì  ê²½í—˜ì´ë‚˜ ë…íŠ¹í•œ ê´€ì ì„ ì¶”ê°€í•˜ì„¸ìš”.")
                        suggestions.append("ğŸŒŸ ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë‚˜ ì°½ì˜ì  ì ‘ê·¼ë²•ì„ ì‹œë„í•˜ì„¸ìš”.")
            
            # ì½˜í…ì¸  íƒ€ì…ë³„ íŠ¹í™” ì œì•ˆ
            if content_type == 'news':
                suggestions.append("ğŸ“° ë‰´ìŠ¤ íŠ¹í™”: 5W1H(ëˆ„ê°€, ì–¸ì œ, ì–´ë””ì„œ, ë¬´ì—‡ì„, ì™œ, ì–´ë–»ê²Œ)ë¥¼ ëª…í™•íˆ í•˜ì„¸ìš”.")
            elif content_type == 'blog':
                suggestions.append("âœï¸ ë¸”ë¡œê·¸ íŠ¹í™”: ê°œì¸ì  ìŠ¤í† ë¦¬í…”ë§ê³¼ ë…ìì™€ì˜ ì†Œí†µì„ ê°•í™”í•˜ì„¸ìš”.")
            elif content_type == 'tech_doc':
                suggestions.append("âš™ï¸ ê¸°ìˆ ë¬¸ì„œ íŠ¹í™”: ì½”ë“œ ì˜ˆì œì™€ ë‹¨ê³„ë³„ ì„¤ëª…ì„ ë” ìƒì„¸íˆ ì œê³µí•˜ì„¸ìš”.")
            elif content_type == 'tutorial':
                suggestions.append("ğŸ“ íŠœí† ë¦¬ì–¼ íŠ¹í™”: í•™ìŠµìê°€ ë”°ë¼í•  ìˆ˜ ìˆëŠ” ëª…í™•í•œ ë‹¨ê³„ë¥¼ ì œì‹œí•˜ì„¸ìš”.")
            
            # ì–¸ì–´ í’ˆì§ˆ ê´€ë ¨ ì œì•ˆ
            suggestions.append("ğŸ“š ì–¸ì–´ í’ˆì§ˆ: ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì„ ì¬ì ê²€í•˜ê³  ë‹¤ì–‘í•œ ì–´íœ˜ë¥¼ í™œìš©í•˜ì„¸ìš”.")
            
            return suggestions[:5]  # ìµœëŒ€ 5ê°œ ì œì•ˆ
            
        except Exception as e:
            logger.error(f"ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„± ì˜¤ë¥˜: {e}")
            return ["í’ˆì§ˆ ê°œì„ ì„ ìœ„í•´ ë‚´ìš©ì„ ì¬ê²€í† í•´ ì£¼ì„¸ìš”."]
    
    def _generate_quality_report(self, dimension_scores: Dict[str, float], 
                               detailed_analysis: Dict[str, Dict], 
                               language_quality: Dict[str, Any],
                               quality_grade: str, content_type: str) -> str:
        """í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            report_lines = []
            
            # ì „ì²´ ë“±ê¸‰ ë° ìš”ì•½
            overall_score = sum(dimension_scores.values()) / len(dimension_scores) if dimension_scores else 50.0
            report_lines.append(f"ğŸ† **ì „ì²´ í’ˆì§ˆ ë“±ê¸‰: {quality_grade}** (ì ìˆ˜: {overall_score:.1f}/100)")
            report_lines.append(f"ğŸ“„ ì½˜í…ì¸  íƒ€ì…: {content_type}")
            report_lines.append("")
            
            # ì°¨ì›ë³„ ìƒì„¸ ë¶„ì„
            report_lines.append("ğŸ“Š **ì°¨ì›ë³„ ë¶„ì„ ê²°ê³¼**")
            
            dimension_names = {
                'reliability': 'ì‹ ë¢°ë„',
                'usefulness': 'ìœ ìš©ì„±', 
                'accuracy': 'ì •í™•ì„±',
                'completeness': 'ì™„ì„±ë„',
                'readability': 'ê°€ë…ì„±',
                'originality': 'ë…ì°½ì„±'
            }
            
            for dimension, score in dimension_scores.items():
                name = dimension_names.get(dimension, dimension)
                grade = self._determine_quality_grade(score)
                
                if score >= 80:
                    emoji = "ğŸŸ¢"
                elif score >= 60:
                    emoji = "ğŸŸ¡"
                else:
                    emoji = "ğŸ”´"
                
                report_lines.append(f"{emoji} **{name}**: {grade} ({score:.1f}ì )")
                
                # ì„¸ë¶€ ë¶„ì„ ì •ë³´
                if dimension in detailed_analysis:
                    details = detailed_analysis[dimension]
                    positive_aspects = [k for k, v in details.items() if isinstance(v, (int, float)) and v > 0]
                    if positive_aspects:
                        report_lines.append(f"   âœ… ê°•ì : {', '.join(positive_aspects[:3])}")
            
            report_lines.append("")
            
            # ì–¸ì–´ í’ˆì§ˆ ë¶„ì„
            if language_quality:
                report_lines.append("ğŸ“ **ì–¸ì–´ í’ˆì§ˆ ë¶„ì„**")
                lang_score = language_quality.get('overall_language_score', 50)
                lang_grade = self._determine_quality_grade(lang_score)
                
                if lang_score >= 80:
                    emoji = "ğŸŸ¢"
                elif lang_score >= 60:
                    emoji = "ğŸŸ¡"
                else:
                    emoji = "ğŸ”´"
                
                report_lines.append(f"{emoji} **ì–¸ì–´ í’ˆì§ˆ**: {lang_grade} ({lang_score:.1f}ì )")
                
                if language_quality.get('grammar_errors', 0) > 0:
                    report_lines.append(f"   âš ï¸ ë¬¸ë²• ì˜¤ë¥˜: {language_quality['grammar_errors']}ê°œ")
                if language_quality.get('spelling_errors', 0) > 0:
                    report_lines.append(f"   âš ï¸ ë§ì¶¤ë²• ì˜¤ë¥˜: {language_quality['spelling_errors']}ê°œ")
                if language_quality.get('good_expressions', 0) > 0:
                    report_lines.append(f"   âœ… ì¢‹ì€ í‘œí˜„: {language_quality['good_expressions']}ê°œ")
                
                vocab_diversity = language_quality.get('vocabulary_diversity', 0)
                if vocab_diversity > 0:
                    report_lines.append(f"   ğŸ“š ì–´íœ˜ ë‹¤ì–‘ì„±: {vocab_diversity:.1f}%")
             
            report_lines.append("")
            
            # ì¢…í•© í‰ê°€
            report_lines.append("ğŸ¯ **ì¢…í•© í‰ê°€**")
            if overall_score >= 85:
                report_lines.append("ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ì½˜í…ì¸ ì…ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            elif overall_score >= 70:
                report_lines.append("ì–‘í˜¸í•œ í’ˆì§ˆì˜ ì½˜í…ì¸ ì…ë‹ˆë‹¤. ì¼ë¶€ ì˜ì—­ì—ì„œ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
            elif overall_score >= 60:
                report_lines.append("ë³´í†µ ìˆ˜ì¤€ì˜ ì½˜í…ì¸ ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ì˜ì—­ì—ì„œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                report_lines.append("í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•œ ì½˜í…ì¸ ì…ë‹ˆë‹¤. ì „ë°˜ì ì¸ ì¬ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            
            return "\n".join(report_lines)
            
        except Exception as e:
# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
analyzer = None

def get_content_analyzer():
    """ì½˜í…ì¸  ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global analyzer
    if analyzer is None:
        analyzer = IntelligentContentAnalyzer()
    return analyzer 