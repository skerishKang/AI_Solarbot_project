"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™” ì œì•ˆ ì‹œìŠ¤í…œ
ì–¸ì–´ë³„ ì„±ëŠ¥ íŒ¨í„´ ë¶„ì„, ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ì¸¡ì •, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ì œì•ˆ
"""

import os
import re
import json
import time
import hashlib
import statistics
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import ast
import keyword

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    execution_id: str
    language: str
    code_hash: str
    execution_time: float
    memory_usage: int
    cpu_usage: float
    performance_score: float
    complexity_score: int
    algorithm_complexity: str  # O(1), O(n), O(n^2), etc.
    optimization_level: str  # 'optimal', 'good', 'needs_improvement', 'poor'
    benchmark_category: str  # 'basic', 'algorithm', 'data_structure', 'io_intensive'
    comparative_ranking: float  # 0-100, ì–¸ì–´ë³„ ë¹„êµ ìˆœìœ„
    timestamp: str

@dataclass
class OptimizationSuggestion:
    """ìµœì í™” ì œì•ˆ ë°ì´í„° í´ë˜ìŠ¤"""
    suggestion_id: str
    category: str  # 'algorithm', 'memory', 'syntax', 'style', 'performance'
    severity: str  # 'critical', 'major', 'minor', 'info'
    title: str
    description: str
    code_example: Optional[str]
    estimated_improvement: str  # e.g., "20% faster", "50% less memory"
    difficulty: str  # 'easy', 'medium', 'hard'

@dataclass
class LanguageBenchmarkData:
    """ì–¸ì–´ë³„ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ ë°ì´í„°"""
    language: str
    baseline_execution_time: Dict[str, float]  # complexity -> baseline time
    baseline_memory_usage: Dict[str, int]     # complexity -> baseline memory
    performance_multipliers: Dict[str, float] # feature -> multiplier
    common_patterns: List[str]
    optimization_rules: List[Dict[str, Any]]

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™” ì œì•ˆ ì‹œìŠ¤í…œ"""
    
    def __init__(self, data_dir: str = "data/benchmark"):
        self.data_dir = data_dir
        self.benchmark_results = []
        self.language_baselines = self._initialize_language_baselines()
        self.optimization_patterns = self._initialize_optimization_patterns()
        self.complexity_patterns = self._initialize_complexity_patterns()
        
        # ë°ì´í„° ë¡œë”©
        self._ensure_data_directory()
        self._load_benchmark_data()
    
    def _ensure_data_directory(self):
        """ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±"""
        os.makedirs(self.data_dir, exist_ok=True)
    
    def _initialize_language_baselines(self) -> Dict[str, LanguageBenchmarkData]:
        """ì–¸ì–´ë³„ ì„±ëŠ¥ ê¸°ì¤€ì„  ì´ˆê¸°í™”"""
        return {
            'python': LanguageBenchmarkData(
                language='python',
                baseline_execution_time={
                    'O(1)': 0.001,
                    'O(n)': 0.01,
                    'O(n^2)': 0.1,
                    'O(n^3)': 1.0,
                    'O(2^n)': 5.0
                },
                baseline_memory_usage={
                    'O(1)': 10,
                    'O(n)': 50,
                    'O(n^2)': 200,
                    'O(n^3)': 1000
                },
                performance_multipliers={
                    'list_comprehension': 0.8,
                    'generator': 0.6,
                    'numpy': 0.3,
                    'nested_loop': 2.0,
                    'recursion': 1.5
                },
                common_patterns=[
                    'for_loop', 'list_comprehension', 'dictionary_access',
                    'function_call', 'class_instantiation'
                ],
                optimization_rules=[
                    {
                        'pattern': r'for\s+\w+\s+in\s+range\(len\(',
                        'suggestion': 'enumerate() ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”',
                        'improvement': '10-20% ì„±ëŠ¥ í–¥ìƒ'
                    },
                    {
                        'pattern': r'\+\s*str\(',
                        'suggestion': 'f-string ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤',
                        'improvement': '20-30% ì„±ëŠ¥ í–¥ìƒ'
                    }
                ]
            ),
            'javascript': LanguageBenchmarkData(
                language='javascript',
                baseline_execution_time={
                    'O(1)': 0.0005,
                    'O(n)': 0.005,
                    'O(n^2)': 0.05,
                    'O(n^3)': 0.5,
                    'O(2^n)': 3.0
                },
                baseline_memory_usage={
                    'O(1)': 5,
                    'O(n)': 30,
                    'O(n^2)': 150,
                    'O(n^3)': 800
                },
                performance_multipliers={
                    'arrow_function': 0.9,
                    'const_let': 0.95,
                    'strict_equality': 0.98,
                    'var_declaration': 1.1
                },
                common_patterns=['function_declaration', 'arrow_function', 'array_methods'],
                optimization_rules=[
                    {
                        'pattern': r'var\s+',
                        'suggestion': 'let ë˜ëŠ” const ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤',
                        'improvement': '5-10% ì„±ëŠ¥ í–¥ìƒ'
                    },
                    {
                        'pattern': r'==\s',
                        'suggestion': '=== ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤',
                        'improvement': 'íƒ€ì… ì•ˆì „ì„± í–¥ìƒ'
                    }
                ]
            ),
            'java': LanguageBenchmarkData(
                language='java',
                baseline_execution_time={
                    'O(1)': 0.0002,
                    'O(n)': 0.002,
                    'O(n^2)': 0.02,
                    'O(n^3)': 0.2,
                    'O(2^n)': 2.0
                },
                baseline_memory_usage={
                    'O(1)': 20,
                    'O(n)': 80,
                    'O(n^2)': 300,
                    'O(n^3)': 1500
                },
                performance_multipliers={
                    'stringbuilder': 0.5,
                    'arraylist_presized': 0.8,
                    'primitive_types': 0.7,
                    'boxing_unboxing': 1.3
                },
                common_patterns=['loop', 'string_concatenation', 'collection_usage'],
                optimization_rules=[
                    {
                        'pattern': r'String\s+\w+\s*=\s*.*\+',
                        'suggestion': 'StringBuilder ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”',
                        'improvement': '50-80% ì„±ëŠ¥ í–¥ìƒ'
                    }
                ]
            ),
            'cpp': LanguageBenchmarkData(
                language='cpp',
                baseline_execution_time={
                    'O(1)': 0.0001,
                    'O(n)': 0.001,
                    'O(n^2)': 0.01,
                    'O(n^3)': 0.1,
                    'O(2^n)': 1.0
                },
                baseline_memory_usage={
                    'O(1)': 8,
                    'O(n)': 40,
                    'O(n^2)': 160,
                    'O(n^3)': 800
                },
                performance_multipliers={
                    'vector_reserve': 0.8,
                    'smart_pointers': 1.05,
                    'raw_pointers': 0.95,
                    'virtual_functions': 1.1
                },
                common_patterns=['memory_management', 'stl_usage', 'template_usage'],
                optimization_rules=[]
            ),
            'go': LanguageBenchmarkData(
                language='go',
                baseline_execution_time={
                    'O(1)': 0.0003,
                    'O(n)': 0.003,
                    'O(n^2)': 0.03,
                    'O(n^3)': 0.3,
                    'O(2^n)': 2.5
                },
                baseline_memory_usage={
                    'O(1)': 12,
                    'O(n)': 60,
                    'O(n^2)': 240,
                    'O(n^3)': 1200
                },
                performance_multipliers={
                    'goroutines': 0.7,
                    'channels': 0.9,
                    'slice_capacity': 0.8
                },
                common_patterns=['goroutine_usage', 'channel_operations', 'slice_operations'],
                optimization_rules=[]
            ),
            'rust': LanguageBenchmarkData(
                language='rust',
                baseline_execution_time={
                    'O(1)': 0.0001,
                    'O(n)': 0.001,
                    'O(n^2)': 0.01,
                    'O(n^3)': 0.1,
                    'O(2^n)': 1.0
                },
                baseline_memory_usage={
                    'O(1)': 6,
                    'O(n)': 30,
                    'O(n^2)': 120,
                    'O(n^3)': 600
                },
                performance_multipliers={
                    'zero_cost_abstractions': 1.0,
                    'ownership_system': 0.95,
                    'iterators': 0.8
                },
                common_patterns=['ownership', 'borrowing', 'iterator_chains'],
                optimization_rules=[]
            ),
            'php': LanguageBenchmarkData(
                language='php',
                baseline_execution_time={
                    'O(1)': 0.002,
                    'O(n)': 0.02,
                    'O(n^2)': 0.2,
                    'O(n^3)': 2.0,
                    'O(2^n)': 10.0
                },
                baseline_memory_usage={
                    'O(1)': 15,
                    'O(n)': 75,
                    'O(n^2)': 300,
                    'O(n^3)': 1500
                },
                performance_multipliers={
                    'array_functions': 0.8,
                    'opcache': 0.6,
                    'string_operations': 1.2
                },
                common_patterns=['array_operations', 'string_manipulation'],
                optimization_rules=[]
            ),
            'ruby': LanguageBenchmarkData(
                language='ruby',
                baseline_execution_time={
                    'O(1)': 0.003,
                    'O(n)': 0.03,
                    'O(n^2)': 0.3,
                    'O(n^3)': 3.0,
                    'O(2^n)': 15.0
                },
                baseline_memory_usage={
                    'O(1)': 20,
                    'O(n)': 100,
                    'O(n^2)': 400,
                    'O(n^3)': 2000
                },
                performance_multipliers={
                    'blocks': 0.9,
                    'symbols': 0.85,
                    'string_interpolation': 1.1
                },
                common_patterns=['block_usage', 'metaprogramming'],
                optimization_rules=[]
            ),
            'typescript': LanguageBenchmarkData(
                language='typescript',
                baseline_execution_time={
                    'O(1)': 0.0006,
                    'O(n)': 0.006,
                    'O(n^2)': 0.06,
                    'O(n^3)': 0.6,
                    'O(2^n)': 3.5
                },
                baseline_memory_usage={
                    'O(1)': 8,
                    'O(n)': 40,
                    'O(n^2)': 160,
                    'O(n^3)': 800
                },
                performance_multipliers={
                    'type_annotations': 1.0,
                    'generics': 1.0,
                    'interfaces': 1.0
                },
                common_patterns=['type_usage', 'interface_implementation'],
                optimization_rules=[]
            ),
            'csharp': LanguageBenchmarkData(
                language='csharp',
                baseline_execution_time={
                    'O(1)': 0.0002,
                    'O(n)': 0.002,
                    'O(n^2)': 0.02,
                    'O(n^3)': 0.2,
                    'O(2^n)': 2.0
                },
                baseline_memory_usage={
                    'O(1)': 16,
                    'O(n)': 64,
                    'O(n^2)': 256,
                    'O(n^3)': 1280
                },
                performance_multipliers={
                    'linq': 1.1,
                    'generics': 0.95,
                    'value_types': 0.9
                },
                common_patterns=['linq_usage', 'generic_collections'],
                optimization_rules=[]
            )
        }
    
    def _initialize_optimization_patterns(self) -> Dict[str, List[Dict[str, Any]]]:
        """ìµœì í™” íŒ¨í„´ ì´ˆê¸°í™”"""
        return {
            'algorithm': [
                {
                    'pattern': 'nested_loop',
                    'detection': r'for.*for.*',
                    'suggestion': 'ì¤‘ì²© ë£¨í”„ë¥¼ ë‹¨ì¼ ë£¨í”„ë‚˜ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì í™”',
                    'severity': 'major'
                },
                {
                    'pattern': 'inefficient_search',
                    'detection': r'for.*in.*if.*==',
                    'suggestion': 'ì„ í˜• ê²€ìƒ‰ ëŒ€ì‹  í•´ì‹œ í…Œì´ë¸”ì´ë‚˜ ì´ì§„ ê²€ìƒ‰ ì‚¬ìš©',
                    'severity': 'minor'
                }
            ],
            'memory': [
                {
                    'pattern': 'memory_leak',
                    'detection': r'new|malloc.*(?!free|delete)',
                    'suggestion': 'ë©”ëª¨ë¦¬ í•´ì œë¥¼ í™•ì¸í•˜ì„¸ìš”',
                    'severity': 'critical'
                },
                {
                    'pattern': 'large_data_structure',
                    'detection': r'.*\[.*\d{4,}.*\]',
                    'suggestion': 'í° ë°ì´í„° êµ¬ì¡°ì˜ í•„ìš”ì„±ì„ ì¬ê²€í† í•˜ì„¸ìš”',
                    'severity': 'minor'
                }
            ],
            'syntax': [
                {
                    'pattern': 'inefficient_string_concat',
                    'detection': r'\+.*str\(',
                    'suggestion': 'ë¬¸ìì—´ í¬ë§·íŒ… ìµœì í™”',
                    'severity': 'minor'
                }
            ]
        }
    
    def _initialize_complexity_patterns(self) -> Dict[str, str]:
        """ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ íŒ¨í„´ ì´ˆê¸°í™”"""
        return {
            r'for.*for.*for': 'O(n^3)',
            r'for.*for(?!.*for)': 'O(n^2)',
            r'for(?!.*for)': 'O(n)',
            r'while.*while': 'O(n^2)',
            r'while(?!.*while)': 'O(n)',
            r'\.sort\(|sorted\(': 'O(n log n)',
            r'recursive|recursion': 'O(2^n)',
            r'binary.*search|bisect': 'O(log n)',
            r'^(?!.*for|.*while|.*recursive).*$': 'O(1)'
        }
    
    def analyze_performance(self, code: str, language: str, execution_time: float, 
                          memory_usage: int, cpu_usage: float = 0.0) -> BenchmarkResult:
        """ì¢…í•© ì„±ëŠ¥ ë¶„ì„"""
        
        # ì½”ë“œ í•´ì‹œ ìƒì„±
        code_hash = hashlib.md5(code.encode()).hexdigest()
        
        # ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ë¶„ì„
        algorithm_complexity = self._analyze_algorithm_complexity(code, language)
        
        # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
        complexity_score = self._calculate_complexity_score(code, language)
        
        # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (ê¸°ì¡´ ë°©ì‹ ê°œì„ )
        performance_score = self._calculate_advanced_performance_score(
            execution_time, memory_usage, complexity_score, language, algorithm_complexity
        )
        
        # ìµœì í™” ë ˆë²¨ ê²°ì •
        optimization_level = self._determine_optimization_level(performance_score, complexity_score)
        
        # ë²¤ì¹˜ë§ˆí¬ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        benchmark_category = self._categorize_benchmark(code, language)
        
        # ì–¸ì–´ë³„ ìƒëŒ€ ìˆœìœ„ ê³„ì‚°
        comparative_ranking = self._calculate_comparative_ranking(
            performance_score, language, algorithm_complexity
        )
        
        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìƒì„±
        result = BenchmarkResult(
            execution_id=f"bench_{int(time.time() * 1000)}",
            language=language,
            code_hash=code_hash,
            execution_time=execution_time,
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            performance_score=performance_score,
            complexity_score=complexity_score,
            algorithm_complexity=algorithm_complexity,
            optimization_level=optimization_level,
            benchmark_category=benchmark_category,
            comparative_ranking=comparative_ranking,
            timestamp=datetime.now().isoformat()
        )
        
        # ê²°ê³¼ ì €ì¥
        self.benchmark_results.append(result)
        self._save_benchmark_data()
        
        return result
    
    def _analyze_algorithm_complexity(self, code: str, language: str) -> str:
        """ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ë¶„ì„"""
        code_lower = code.lower()
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë³µì¡ë„ ì¶”ì •
        for pattern, complexity in self.complexity_patterns.items():
            if re.search(pattern, code_lower, re.MULTILINE):
                return complexity
        
        return 'O(1)'  # ê¸°ë³¸ê°’
    
    def _calculate_complexity_score(self, code: str, language: str) -> int:
        """ì½”ë“œ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚° (1-100)"""
        score = 0
        
        # ì½”ë“œ ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
        lines = len([line for line in code.split('\n') if line.strip()])
        score += min(lines * 2, 30)
        
        # ì œì–´ êµ¬ì¡° ë³µì¡ë„
        control_structures = len(re.findall(r'\b(if|for|while|switch|case)\b', code.lower()))
        score += control_structures * 5
        
        # í•¨ìˆ˜/í´ë˜ìŠ¤ ì •ì˜
        functions = len(re.findall(r'\b(def|function|class|struct)\b', code.lower()))
        score += functions * 3
        
        # ì¤‘ì²© êµ¬ì¡°
        nested_structures = len(re.findall(r'for.*for|if.*if', code.lower()))
        score += nested_structures * 10
        
        return min(score, 100)
    
    def _calculate_advanced_performance_score(self, execution_time: float, memory_usage: int, 
                                           complexity_score: int, language: str, 
                                           algorithm_complexity: str) -> float:
        """í–¥ìƒëœ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°"""
        
        baseline = self.language_baselines.get(language)
        if not baseline:
            return 50.0  # ê¸°ë³¸ê°’
        
        # ê¸°ì¤€ì„  ëŒ€ë¹„ ì„±ëŠ¥ ê³„ì‚°
        baseline_time = baseline.baseline_execution_time.get(algorithm_complexity, 0.01)
        baseline_memory = baseline.baseline_memory_usage.get(algorithm_complexity, 50)
        
        # ì‹œê°„ ì ìˆ˜ (70% ê°€ì¤‘ì¹˜)
        time_ratio = execution_time / baseline_time
        time_score = max(0, 100 - (time_ratio - 1) * 50)
        
        # ë©”ëª¨ë¦¬ ì ìˆ˜ (20% ê°€ì¤‘ì¹˜)
        memory_ratio = memory_usage / baseline_memory
        memory_score = max(0, 100 - (memory_ratio - 1) * 40)
        
        # ë³µì¡ë„ ë³´ì • (10% ê°€ì¤‘ì¹˜)
        complexity_penalty = min(complexity_score / 100 * 20, 20)
        
        # ê°€ì¤‘ í‰ê· 
        total_score = (
            time_score * 0.7 +
            memory_score * 0.2 +
            (100 - complexity_penalty) * 0.1
        )
        
        return round(max(0, min(100, total_score)), 2)
    
    def _determine_optimization_level(self, performance_score: float, complexity_score: int) -> str:
        """ìµœì í™” ë ˆë²¨ ê²°ì •"""
        if performance_score >= 90 and complexity_score <= 30:
            return 'optimal'
        elif performance_score >= 70 and complexity_score <= 50:
            return 'good'
        elif performance_score >= 50:
            return 'needs_improvement'
        else:
            return 'poor'
    
    def _categorize_benchmark(self, code: str, language: str) -> str:
        """ë²¤ì¹˜ë§ˆí¬ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        code_lower = code.lower()
        
        if any(keyword in code_lower for keyword in ['sort', 'search', 'tree', 'graph']):
            return 'algorithm'
        elif any(keyword in code_lower for keyword in ['list', 'array', 'dict', 'map']):
            return 'data_structure'
        elif any(keyword in code_lower for keyword in ['file', 'read', 'write', 'input', 'output']):
            return 'io_intensive'
        else:
            return 'basic'
    
    def _calculate_comparative_ranking(self, performance_score: float, language: str, 
                                     algorithm_complexity: str) -> float:
        """ì–¸ì–´ë³„ ìƒëŒ€ ìˆœìœ„ ê³„ì‚°"""
        # ê°™ì€ ì–¸ì–´ì˜ ë¹„ìŠ·í•œ ë³µì¡ë„ ì½”ë“œë“¤ê³¼ ë¹„êµ
        similar_results = [
            r for r in self.benchmark_results 
            if r.language == language and r.algorithm_complexity == algorithm_complexity
        ]
        
        if len(similar_results) < 2:
            return 50.0  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ê°„ê°’
        
        scores = [r.performance_score for r in similar_results]
        percentile = sum(1 for score in scores if score <= performance_score) / len(scores) * 100
        
        return round(percentile, 1)
    
    def generate_optimization_suggestions(self, code: str, language: str, 
                                        benchmark_result: BenchmarkResult) -> List[OptimizationSuggestion]:
        """ìµœì í™” ì œì•ˆ ìƒì„±"""
        suggestions = []
        suggestion_counter = 0
        
        # 1. ì„±ëŠ¥ ê¸°ë°˜ ì œì•ˆ
        if benchmark_result.performance_score < 70:
            suggestions.append(OptimizationSuggestion(
                suggestion_id=f"perf_{suggestion_counter}",
                category='performance',
                severity='major' if benchmark_result.performance_score < 50 else 'minor',
                title='ì„±ëŠ¥ ìµœì í™” í•„ìš”',
                description=f'í˜„ì¬ ì„±ëŠ¥ ì ìˆ˜ {benchmark_result.performance_score:.1f}ì . ì•Œê³ ë¦¬ì¦˜ì´ë‚˜ ë°ì´í„° êµ¬ì¡° ê°œì„ ì„ ê³ ë ¤í•˜ì„¸ìš”.',
                code_example=None,
                estimated_improvement='20-50% ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥',
                difficulty='medium'
            ))
            suggestion_counter += 1
        
        # 2. ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ê¸°ë°˜ ì œì•ˆ
        if benchmark_result.algorithm_complexity in ['O(n^2)', 'O(n^3)', 'O(2^n)']:
            suggestions.append(OptimizationSuggestion(
                suggestion_id=f"algo_{suggestion_counter}",
                category='algorithm',
                severity='major',
                title='ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ê°œì„ ',
                description=f'í˜„ì¬ {benchmark_result.algorithm_complexity} ë³µì¡ë„. ë” íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.',
                code_example=self._get_algorithm_example(benchmark_result.algorithm_complexity, language),
                estimated_improvement='në°° ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥',
                difficulty='hard'
            ))
            suggestion_counter += 1
        
        # 3. ì–¸ì–´ë³„ íŠ¹í™” ì œì•ˆ
        language_suggestions = self._get_language_specific_suggestions(code, language)
        for lang_sugg in language_suggestions:
            suggestions.append(OptimizationSuggestion(
                suggestion_id=f"lang_{suggestion_counter}",
                category='syntax',
                severity='minor',
                title=lang_sugg['title'],
                description=lang_sugg['description'],
                code_example=lang_sugg.get('example'),
                estimated_improvement=lang_sugg.get('improvement', '10-20% ì„±ëŠ¥ í–¥ìƒ'),
                difficulty='easy'
            ))
            suggestion_counter += 1
        
        # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ì œì•ˆ
        if benchmark_result.memory_usage > 500:  # 500MB ì´ìƒ
            suggestions.append(OptimizationSuggestion(
                suggestion_id=f"mem_{suggestion_counter}",
                category='memory',
                severity='major',
                title='ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”',
                description=f'í˜„ì¬ {benchmark_result.memory_usage}MB ì‚¬ìš©. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° êµ¬ì¡°ë‚˜ ì•Œê³ ë¦¬ì¦˜ ê³ ë ¤í•˜ì„¸ìš”.',
                code_example=None,
                estimated_improvement='50-80% ë©”ëª¨ë¦¬ ì ˆì•½ ê°€ëŠ¥',
                difficulty='medium'
            ))
            suggestion_counter += 1
        
        # 5. ë³µì¡ë„ ì ìˆ˜ ê¸°ë°˜ ì œì•ˆ
        if benchmark_result.complexity_score > 70:
            suggestions.append(OptimizationSuggestion(
                suggestion_id=f"complex_{suggestion_counter}",
                category='style',
                severity='minor',
                title='ì½”ë“œ ë³µì¡ë„ ê°ì†Œ',
                description=f'ë³µì¡ë„ ì ìˆ˜ {benchmark_result.complexity_score}ì . ì½”ë“œë¥¼ ë” ê°„ë‹¨í•˜ê³  ì½ê¸° ì‰½ê²Œ ë¦¬íŒ©í† ë§í•˜ì„¸ìš”.',
                code_example=None,
                estimated_improvement='ê°€ë…ì„± ë° ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ',
                difficulty='easy'
            ))
        
        return suggestions[:5]  # ìµœëŒ€ 5ê°œ ì œì•ˆ
    
    def _get_algorithm_example(self, complexity: str, language: str) -> Optional[str]:
        """ì•Œê³ ë¦¬ì¦˜ ê°œì„  ì˜ˆì œ ì½”ë“œ"""
        examples = {
            'python': {
                'O(n^2)': '''# ê°œì„  ì „: O(n^2)
for i in range(len(arr)):
    for j in range(len(arr)):
        if arr[i] == target:
            return i

# ê°œì„  í›„: O(n)
try:
    return arr.index(target)
except ValueError:
    return -1''',
                'O(n^3)': '''# ê°œì„  ì „: O(n^3) ì‚¼ì¤‘ ë£¨í”„
# ê°œì„  í›„: í•´ì‹œë§µì´ë‚˜ ë” íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©''',
                'O(2^n)': '''# ê°œì„  ì „: O(2^n) ì¬ê·€
def fibonacci(n):
    if n <= 1: return n
    return fibonacci(n-1) + fibonacci(n-2)

# ê°œì„  í›„: O(n) ë™ì  í”„ë¡œê·¸ë˜ë°
def fibonacci(n):
    if n <= 1: return n
    a, b = 0, 1
    for _ in range(2, n+1):
        a, b = b, a + b
    return b'''
            }
        }
        
        return examples.get(language, {}).get(complexity)
    
    def _get_language_specific_suggestions(self, code: str, language: str) -> List[Dict[str, str]]:
        """ì–¸ì–´ë³„ íŠ¹í™” ì œì•ˆ"""
        suggestions = []
        baseline = self.language_baselines.get(language)
        
        if not baseline:
            return suggestions
        
        # ì–¸ì–´ë³„ ìµœì í™” ê·œì¹™ ì ìš©
        for rule in baseline.optimization_rules:
            if re.search(rule['pattern'], code):
                suggestions.append({
                    'title': 'ì–¸ì–´ë³„ ìµœì í™”',
                    'description': rule['suggestion'],
                    'improvement': rule['improvement']
                })
        
        # ì¶”ê°€ ì–¸ì–´ë³„ ì œì•ˆ
        if language == 'python':
            if 'for i in range(len(' in code:
                suggestions.append({
                    'title': 'enumerate ì‚¬ìš© ê¶Œì¥',
                    'description': 'range(len()) ëŒ€ì‹  enumerate() ì‚¬ìš©',
                    'example': 'for i, item in enumerate(items):',
                    'improvement': '10-15% ì„±ëŠ¥ í–¥ìƒ'
                })
            
            if code.count('print(') > 5:
                suggestions.append({
                    'title': 'logging ëª¨ë“ˆ ì‚¬ìš©',
                    'description': 'ë§ì€ printë¬¸ ëŒ€ì‹  logging ëª¨ë“ˆ ì‚¬ìš© ê¶Œì¥',
                    'improvement': 'ì„±ëŠ¥ ë° ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ'
                })
        
        elif language == 'javascript':
            if 'var ' in code:
                suggestions.append({
                    'title': 'const/let ì‚¬ìš©',
                    'description': 'var ëŒ€ì‹  const ë˜ëŠ” let ì‚¬ìš© ê¶Œì¥',
                    'example': 'const value = 10; // or let value = 10;',
                    'improvement': 'ìŠ¤ì½”í”„ ì•ˆì „ì„± í–¥ìƒ'
                })
        
        return suggestions
    
    def get_benchmark_statistics(self, language: Optional[str] = None, 
                               category: Optional[str] = None) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ í†µê³„ ì¡°íšŒ"""
        filtered_results = self.benchmark_results
        
        if language:
            filtered_results = [r for r in filtered_results if r.language == language]
        
        if category:
            filtered_results = [r for r in filtered_results if r.benchmark_category == category]
        
        if not filtered_results:
            return {"message": "í•´ë‹¹ ì¡°ê±´ì˜ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # í†µê³„ ê³„ì‚°
        performance_scores = [r.performance_score for r in filtered_results]
        execution_times = [r.execution_time for r in filtered_results]
        memory_usages = [r.memory_usage for r in filtered_results]
        
        # ì–¸ì–´ë³„ ë¶„í¬
        language_distribution = Counter([r.language for r in filtered_results])
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
        category_distribution = Counter([r.benchmark_category for r in filtered_results])
        
        # ìµœì í™” ë ˆë²¨ ë¶„í¬
        optimization_distribution = Counter([r.optimization_level for r in filtered_results])
        
        return {
            "total_benchmarks": len(filtered_results),
            "performance_statistics": {
                "average": round(statistics.mean(performance_scores), 2),
                "median": round(statistics.median(performance_scores), 2),
                "best": max(performance_scores),
                "worst": min(performance_scores),
                "std_dev": round(statistics.stdev(performance_scores) if len(performance_scores) > 1 else 0, 2)
            },
            "execution_time_statistics": {
                "average": round(statistics.mean(execution_times), 4),
                "median": round(statistics.median(execution_times), 4),
                "fastest": min(execution_times),
                "slowest": max(execution_times)
            },
            "memory_statistics": {
                "average": round(statistics.mean(memory_usages), 1),
                "median": round(statistics.median(memory_usages), 1),
                "least": min(memory_usages),
                "most": max(memory_usages)
            },
            "distributions": {
                "by_language": dict(language_distribution),
                "by_category": dict(category_distribution),
                "by_optimization_level": dict(optimization_distribution)
            },
            "insights": self._generate_statistical_insights(filtered_results)
        }
    
    def _generate_statistical_insights(self, results: List[BenchmarkResult]) -> List[str]:
        """í†µê³„ì  ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        if not results:
            return insights
        
        # ì–¸ì–´ë³„ ì„±ëŠ¥ ë¹„êµ
        language_performance = defaultdict(list)
        for result in results:
            language_performance[result.language].append(result.performance_score)
        
        if len(language_performance) > 1:
            avg_scores = {lang: statistics.mean(scores) for lang, scores in language_performance.items()}
            best_lang = max(avg_scores, key=avg_scores.get)
            worst_lang = min(avg_scores, key=avg_scores.get)
            
            insights.append(f"ğŸ† {best_lang}ì´ í‰ê·  {avg_scores[best_lang]:.1f}ì ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
            insights.append(f"ğŸ“ˆ {worst_lang}ì€ í‰ê·  {avg_scores[worst_lang]:.1f}ì ìœ¼ë¡œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ìµœì í™” ë ˆë²¨ ë¶„í¬
        optimization_counts = Counter([r.optimization_level for r in results])
        total = len(results)
        
        optimal_ratio = optimization_counts.get('optimal', 0) / total * 100
        if optimal_ratio > 50:
            insights.append(f"âœ¨ {optimal_ratio:.1f}%ì˜ ì½”ë“œê°€ ìµœì í™”ëœ ìƒíƒœì…ë‹ˆë‹¤.")
        elif optimal_ratio < 20:
            insights.append(f"âš ï¸ ìµœì í™”ëœ ì½”ë“œê°€ {optimal_ratio:.1f}%ì— ë¶ˆê³¼í•©ë‹ˆë‹¤. ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ë³µì¡ë„ ë¶„ì„
        complexity_counts = Counter([r.algorithm_complexity for r in results])
        if complexity_counts.get('O(n^2)', 0) + complexity_counts.get('O(n^3)', 0) > total * 0.3:
            insights.append("ğŸ”„ ë†’ì€ ë³µì¡ë„ ì•Œê³ ë¦¬ì¦˜ì´ ë§ìŠµë‹ˆë‹¤. íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        return insights
    
    def compare_with_baseline(self, benchmark_result: BenchmarkResult) -> Dict[str, Any]:
        """ê¸°ì¤€ì„ ê³¼ ë¹„êµ ë¶„ì„"""
        baseline = self.language_baselines.get(benchmark_result.language)
        if not baseline:
            return {"error": "í•´ë‹¹ ì–¸ì–´ì˜ ê¸°ì¤€ì„  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        baseline_time = baseline.baseline_execution_time.get(
            benchmark_result.algorithm_complexity, 0.01
        )
        baseline_memory = baseline.baseline_memory_usage.get(
            benchmark_result.algorithm_complexity, 50
        )
        
        time_ratio = benchmark_result.execution_time / baseline_time
        memory_ratio = benchmark_result.memory_usage / baseline_memory
        
        return {
            "execution_time_comparison": {
                "result": benchmark_result.execution_time,
                "baseline": baseline_time,
                "ratio": round(time_ratio, 2),
                "status": "faster" if time_ratio < 1 else "slower" if time_ratio > 1.5 else "normal"
            },
            "memory_usage_comparison": {
                "result": benchmark_result.memory_usage,
                "baseline": baseline_memory,
                "ratio": round(memory_ratio, 2),
                "status": "efficient" if memory_ratio < 1 else "heavy" if memory_ratio > 2 else "normal"
            },
            "overall_assessment": self._assess_overall_performance(time_ratio, memory_ratio),
            "ranking_percentile": benchmark_result.comparative_ranking
        }
    
    def _assess_overall_performance(self, time_ratio: float, memory_ratio: float) -> str:
        """ì „ì²´ ì„±ëŠ¥ í‰ê°€"""
        if time_ratio < 0.8 and memory_ratio < 0.8:
            return "excellent"
        elif time_ratio < 1.2 and memory_ratio < 1.5:
            return "good"
        elif time_ratio < 2.0 and memory_ratio < 3.0:
            return "acceptable"
        else:
            return "needs_improvement"
    
    def _load_benchmark_data(self):
        """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë”©"""
        try:
            file_path = os.path.join(self.data_dir, 'benchmark_results.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.benchmark_results = [
                        BenchmarkResult(**item) for item in data
                    ]
        except Exception as e:
            print(f"ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            self.benchmark_results = []
    
    def _save_benchmark_data(self):
        """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì €ì¥"""
        try:
            file_path = os.path.join(self.data_dir, 'benchmark_results.json')
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump([asdict(result) for result in self.benchmark_results], f, 
                         ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (í•„ìš”ì‹œ ì‚¬ìš©)
performance_benchmark = None

def get_performance_benchmark(data_dir: str = "data/benchmark"):
    """í•„ìš”ì‹œ PerformanceBenchmark ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜"""
    global performance_benchmark
    if performance_benchmark is None:
        try:
            performance_benchmark = PerformanceBenchmark(data_dir)
        except Exception as e:
            print(f"âš ï¸ PerformanceBenchmark ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    return performance_benchmark 