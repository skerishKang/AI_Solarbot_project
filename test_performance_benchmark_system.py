"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™” ì œì•ˆ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
PerformanceBenchmark + EnhancedPerformanceExecutor ì „ì²´ ê¸°ëŠ¥ ê²€ì¦
"""

import os
import sys
import json
import asyncio
import time
from typing import Dict, List, Any

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, 'src')
sys.path.insert(0, src_dir)
sys.path.insert(0, current_dir)

print("ğŸ§ª === ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™” ì œì•ˆ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ===")

def print_separator(title: str):
    """í…ŒìŠ¤íŠ¸ ì„¹ì…˜ êµ¬ë¶„ì„ """
    print(f"\n{'='*60}")
    print(f"ğŸ¯ {title}")
    print('='*60)

def print_result(test_name: str, success: bool, details: str = ""):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
    status = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨"
    print(f"{status} | {test_name}")
    if details:
        print(f"    ğŸ“ {details}")

async def test_performance_benchmark_system():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì „ì²´ í…ŒìŠ¤íŠ¸"""
    
    print_separator("1. ëª¨ë“ˆ ë¡œë”© ë° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸
    try:
        from performance_benchmark import PerformanceBenchmark, get_performance_benchmark
        from enhanced_performance_executor import EnhancedPerformanceExecutor, get_enhanced_performance_executor
        print_result("ëª¨ë“ˆ import", True, "ëª¨ë“  í•„ìˆ˜ ëª¨ë“ˆ ë¡œë”© ì„±ê³µ")
    except Exception as e:
        print_result("ëª¨ë“ˆ import", False, f"ì˜¤ë¥˜: {e}")
        return
    
    # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
    try:
        benchmark = get_performance_benchmark()
        executor = get_enhanced_performance_executor()
        print_result("ì¸ìŠ¤í„´ìŠ¤ ìƒì„±", True, "PerformanceBenchmark + EnhancedPerformanceExecutor ìƒì„± ì„±ê³µ")
    except Exception as e:
        print_result("ì¸ìŠ¤í„´ìŠ¤ ìƒì„±", False, f"ì˜¤ë¥˜: {e}")
        return
    
    print_separator("2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    
    # ê°„ë‹¨í•œ ì½”ë“œ ì„±ëŠ¥ ë¶„ì„
    test_codes = {
        'python': {
            'simple': 'print("Hello, World!")',
            'loop': '''
for i in range(1000):
    result = i * 2
print(result)
''',
            'nested_loop': '''
total = 0
for i in range(100):
    for j in range(100):
        total += i * j
print(total)
'''
        },
        'javascript': {
            'simple': 'console.log("Hello, World!");',
            'loop': '''
for (let i = 0; i < 1000; i++) {
    let result = i * 2;
}
console.log("Done");
'''
        }
    }
    
    for language, codes in test_codes.items():
        print(f"\nğŸ”¬ {language.upper()} ì½”ë“œ ì„±ëŠ¥ ë¶„ì„ í…ŒìŠ¤íŠ¸")
        
        for code_type, code in codes.items():
            try:
                # ì‹¤í–‰ ê²°ê³¼ ëª¨ì˜ (ì‹¤ì œ ì‹¤í–‰ ëŒ€ì‹ )
                execution_time = 0.001 if code_type == 'simple' else 0.1 if code_type == 'loop' else 1.0
                memory_usage = 10 if code_type == 'simple' else 50 if code_type == 'loop' else 200
                
                # ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
                benchmark_result = benchmark.analyze_performance(
                    code=code,
                    language=language,
                    execution_time=execution_time,
                    memory_usage=memory_usage,
                    cpu_usage=20.0
                )
                
                print_result(
                    f"{language} {code_type} ë¶„ì„",
                    True,
                    f"ì„±ëŠ¥ì ìˆ˜: {benchmark_result.performance_score:.1f}, ë³µì¡ë„: {benchmark_result.algorithm_complexity}, ë ˆë²¨: {benchmark_result.optimization_level}"
                )
                
            except Exception as e:
                print_result(f"{language} {code_type} ë¶„ì„", False, f"ì˜¤ë¥˜: {e}")
    
    print_separator("3. ìµœì í™” ì œì•ˆ ìƒì„± í…ŒìŠ¤íŠ¸")
    
    # ìµœì í™”ê°€ í•„ìš”í•œ ì½”ë“œë¡œ í…ŒìŠ¤íŠ¸
    problematic_code = '''
# ë¹„íš¨ìœ¨ì ì¸ Python ì½”ë“œ
result = ""
for i in range(1000):
    result = result + str(i)  # ë¬¸ìì—´ ì—°ê²° ë¹„íš¨ìœ¨
    
# ì¤‘ì²© ë£¨í”„
total = 0
for i in range(100):
    for j in range(100):
        total += i * j
print(total)
'''
    
    try:
        # ì„±ëŠ¥ ë¶„ì„
        benchmark_result = benchmark.analyze_performance(
            code=problematic_code,
            language='python',
            execution_time=2.5,  # ëŠë¦° ì‹¤í–‰ ì‹œê°„
            memory_usage=300,    # ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
            cpu_usage=80.0
        )
        
        # ìµœì í™” ì œì•ˆ ìƒì„±
        suggestions = benchmark.generate_optimization_suggestions(
            problematic_code, 'python', benchmark_result
        )
        
        print_result(
            "ìµœì í™” ì œì•ˆ ìƒì„±",
            len(suggestions) > 0,
            f"{len(suggestions)}ê°œ ì œì•ˆ ìƒì„±ë¨"
        )
        
        # ì œì•ˆ ë‚´ìš© ì¶œë ¥
        for i, suggestion in enumerate(suggestions[:3], 1):
            print(f"    ğŸ’¡ ì œì•ˆ {i}: {suggestion.title} ({suggestion.severity})")
            print(f"       ğŸ“‹ {suggestion.description}")
            
    except Exception as e:
        print_result("ìµœì í™” ì œì•ˆ ìƒì„±", False, f"ì˜¤ë¥˜: {e}")
    
    print_separator("4. ë²¤ì¹˜ë§ˆí¬ í†µê³„ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    
    try:
        # ì „ì²´ í†µê³„ ì¡°íšŒ
        overall_stats = benchmark.get_benchmark_statistics()
        print_result(
            "ì „ì²´ ë²¤ì¹˜ë§ˆí¬ í†µê³„",
            "total_benchmarks" in overall_stats,
            f"ì´ {overall_stats.get('total_benchmarks', 0)}ê°œ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"
        )
        
        # Python ì–¸ì–´ë³„ í†µê³„
        python_stats = benchmark.get_benchmark_statistics(language='python')
        print_result(
            "Python ì–¸ì–´ë³„ í†µê³„",
            True,
            f"Python: {python_stats.get('total_benchmarks', 0)}ê°œ ê²°ê³¼"
        )
        
        # í†µê³„ì— ì¸ì‚¬ì´íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        insights = overall_stats.get('insights', [])
        if insights:
            print("    ğŸ” ì‹œìŠ¤í…œ ì¸ì‚¬ì´íŠ¸:")
            for insight in insights[:2]:
                print(f"       â€¢ {insight}")
        
    except Exception as e:
        print_result("ë²¤ì¹˜ë§ˆí¬ í†µê³„ ë¶„ì„", False, f"ì˜¤ë¥˜: {e}")
    
    print_separator("5. ê¸°ì¤€ì„  ë¹„êµ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    
    try:
        # ìµœê·¼ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¡œ ê¸°ì¤€ì„  ë¹„êµ
        if benchmark.benchmark_results:
            recent_result = benchmark.benchmark_results[-1]
            comparison = benchmark.compare_with_baseline(recent_result)
            
            print_result(
                "ê¸°ì¤€ì„  ë¹„êµ ë¶„ì„",
                "execution_time_comparison" in comparison,
                f"ì‹¤í–‰ì‹œê°„ ë¹„ìœ¨: {comparison.get('execution_time_comparison', {}).get('ratio', 'N/A')}"
            )
            
            # ë¹„êµ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
            time_comp = comparison.get('execution_time_comparison', {})
            memory_comp = comparison.get('memory_usage_comparison', {})
            overall = comparison.get('overall_assessment', 'unknown')
            
            print(f"    â±ï¸ ì‹¤í–‰ì‹œê°„: {time_comp.get('status', 'unknown')} (ë¹„ìœ¨: {time_comp.get('ratio', 'N/A')})")
            print(f"    ğŸ’¾ ë©”ëª¨ë¦¬: {memory_comp.get('status', 'unknown')} (ë¹„ìœ¨: {memory_comp.get('ratio', 'N/A')})")
            print(f"    ğŸ¯ ì „ì²´í‰ê°€: {overall}")
        else:
            print_result("ê¸°ì¤€ì„  ë¹„êµ ë¶„ì„", True, "ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ìŠ¤í‚µ")
            
    except Exception as e:
        print_result("ê¸°ì¤€ì„  ë¹„êµ ë¶„ì„", False, f"ì˜¤ë¥˜: {e}")
    
    print_separator("6. EnhancedPerformanceExecutor í†µí•© í…ŒìŠ¤íŠ¸")
    
    # ê°„ë‹¨í•œ ì½”ë“œë¡œ í†µí•© ì‹¤í–‰ í…ŒìŠ¤íŠ¸
    simple_test_code = '''
# ê°„ë‹¨í•œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ (ë¹„íš¨ìœ¨ì  ë²„ì „)
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

result = fibonacci(10)
print(f"Fibonacci(10) = {result}")
'''
    
    try:
        # ì„±ëŠ¥ ë¶„ì„ í¬í•¨ ì‹¤í–‰ (ì‹¤ì œ ì‹¤í–‰ì€ í•˜ì§€ ì•Šê³  ëª¨ì˜)
        print("ğŸ”¬ ì„±ëŠ¥ ë¶„ì„ í¬í•¨ ì½”ë“œ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜...")
        
        # ì‹¤í–‰ ê²°ê³¼ ëª¨ì˜
        from online_code_executor import ExecutionResult
        mock_execution_result = ExecutionResult(
            success=True,
            language='python',
            output='Fibonacci(10) = 55\n',
            error='',
            execution_time=0.15,
            memory_usage=25,
            return_code=0,
            performance_score=75.0,
            optimization_suggestions=['ì¬ê·€ ëŒ€ì‹  ë°˜ë³µë¬¸ ì‚¬ìš© ê³ ë ¤'],
            dependencies_detected=[]
        )
        
        # ë²¤ì¹˜ë§ˆí¬ ë¶„ì„
        benchmark_result = benchmark.analyze_performance(
            code=simple_test_code,
            language='python',
            execution_time=0.15,
            memory_usage=25,
            cpu_usage=30.0
        )
        
        # ìµœì í™” ì œì•ˆ
        suggestions = benchmark.generate_optimization_suggestions(
            simple_test_code, 'python', benchmark_result
        )
        
        print_result(
            "í†µí•© ì„±ëŠ¥ ë¶„ì„",
            True,
            f"ì„±ëŠ¥ì ìˆ˜: {benchmark_result.performance_score:.1f}, ì œì•ˆ: {len(suggestions)}ê°œ"
        )
        
        # ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        insights = executor.get_performance_insights(
            simple_test_code, 'python', benchmark_result
        )
        
        print_result(
            "ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ìƒì„±",
            "performance_summary" in insights,
            f"ì¸ì‚¬ì´íŠ¸ ì¹´í…Œê³ ë¦¬: {len(insights)}ê°œ"
        )
        
    except Exception as e:
        print_result("í†µí•© ì„±ëŠ¥ ë¶„ì„", False, f"ì˜¤ë¥˜: {e}")
    
    print_separator("7. ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹œë®¬ë ˆì´ì…˜")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
    test_cases = [
        {
            'name': 'ê¸°ë³¸ ì¶œë ¥',
            'code': 'print("Hello, World!")'
        },
        {
            'name': 'ë‹¨ìˆœ ë°˜ë³µë¬¸',
            'code': '''
for i in range(100):
    result = i * 2
print(result)
'''
        },
        {
            'name': 'ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜',
            'code': '''
result = [i * 2 for i in range(100)]
print(len(result))
'''
        }
    ]
    
    try:
        print(f"ğŸ§ª Python ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì‹œë®¬ë ˆì´ì…˜ ({len(test_cases)}ê°œ ì¼€ì´ìŠ¤)")
        
        # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë³„ë¡œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        suite_results = []
        for i, test_case in enumerate(test_cases, 1):
            print(f"   ğŸ”¬ ì¼€ì´ìŠ¤ {i}: {test_case['name']}")
            
            # ëª¨ì˜ ì„±ëŠ¥ ë°ì´í„°
            execution_time = 0.001 * i  # ì¼€ì´ìŠ¤ë³„ë¡œ ë‹¤ë¥¸ ì‹¤í–‰ ì‹œê°„
            memory_usage = 10 + i * 5   # ì¼€ì´ìŠ¤ë³„ë¡œ ë‹¤ë¥¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            
            benchmark_result = benchmark.analyze_performance(
                code=test_case['code'],
                language='python',
                execution_time=execution_time,
                memory_usage=memory_usage,
                cpu_usage=20.0 + i * 10
            )
            
            suite_results.append({
                'name': test_case['name'],
                'performance_score': benchmark_result.performance_score,
                'complexity': benchmark_result.algorithm_complexity
            })
        
        avg_score = sum(r['performance_score'] for r in suite_results) / len(suite_results)
        
        print_result(
            "ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì‹œë®¬ë ˆì´ì…˜",
            True,
            f"í‰ê·  ì„±ëŠ¥ì ìˆ˜: {avg_score:.1f}/100"
        )
        
        # ìµœê³ /ìµœì € ì„±ëŠ¥ ì¼€ì´ìŠ¤
        best = max(suite_results, key=lambda x: x['performance_score'])
        worst = min(suite_results, key=lambda x: x['performance_score'])
        
        print(f"    ğŸ† ìµœê³ ì„±ëŠ¥: {best['name']} ({best['performance_score']:.1f}ì )")
        print(f"    ğŸ“ˆ ìµœì €ì„±ëŠ¥: {worst['name']} ({worst['performance_score']:.1f}ì )")
        
    except Exception as e:
        print_result("ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì‹œë®¬ë ˆì´ì…˜", False, f"ì˜¤ë¥˜: {e}")
    
    print_separator("8. ë°ì´í„° ì €ì¥ ë° ë¡œë”© í…ŒìŠ¤íŠ¸")
    
    try:
        # í˜„ì¬ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìˆ˜
        initial_count = len(benchmark.benchmark_results)
        
        # ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶”ê°€
        test_result = benchmark.analyze_performance(
            code='print("Data persistence test")',
            language='python',
            execution_time=0.001,
            memory_usage=8,
            cpu_usage=5.0
        )
        
        # ê²°ê³¼ê°€ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
        final_count = len(benchmark.benchmark_results)
        
        print_result(
            "ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì €ì¥",
            final_count > initial_count,
            f"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼: {initial_count} â†’ {final_count}"
        )
        
        # ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
        data_file = os.path.join(benchmark.data_dir, 'benchmark_results.json')
        file_exists = os.path.exists(data_file)
        
        print_result(
            "ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° íŒŒì¼",
            file_exists,
            f"ë°ì´í„° íŒŒì¼: {'ì¡´ì¬' if file_exists else 'ì—†ìŒ'}"
        )
        
    except Exception as e:
        print_result("ë°ì´í„° ì €ì¥ ë° ë¡œë”©", False, f"ì˜¤ë¥˜: {e}")
    
    print_separator("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ìš”ì•½")
    
    print("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™” ì œì•ˆ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print("   âœ… ì„±ëŠ¥ ë¶„ì„ ì—”ì§„ - ì •ìƒ ë™ì‘")
    print("   âœ… ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ë¶„ì„ - ì •ìƒ ë™ì‘") 
    print("   âœ… ìµœì í™” ì œì•ˆ ìƒì„± - ì •ìƒ ë™ì‘")
    print("   âœ… ë²¤ì¹˜ë§ˆí¬ í†µê³„ ë¶„ì„ - ì •ìƒ ë™ì‘")
    print("   âœ… ê¸°ì¤€ì„  ë¹„êµ ë¶„ì„ - ì •ìƒ ë™ì‘")
    print("   âœ… í†µí•© ì‹¤í–‰ê¸° - ì •ìƒ ë™ì‘")
    print("   âœ… ë°ì´í„° ì§€ì†ì„± - ì •ìƒ ë™ì‘")
    
    print("\nğŸš€ 4ë‹¨ê³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™” ì œì•ˆ ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("   ğŸ“ˆ 10ê°œ ì–¸ì–´ë³„ ì„±ëŠ¥ ê¸°ì¤€ì„  ë°ì´í„°ë² ì´ìŠ¤")
    print("   ğŸ§® ìë™ ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ë¶„ì„")
    print("   ğŸ’¡ ì§€ëŠ¥í˜• ìµœì í™” ì œì•ˆ ìƒì„±")
    print("   ğŸ“Š ì¢…í•© ì„±ëŠ¥ í†µê³„ ë° ì¸ì‚¬ì´íŠ¸")
    print("   ğŸ¯ ê°œì¸í™”ëœ ì„±ëŠ¥ ê°œì„  ê°€ì´ë“œ")

if __name__ == "__main__":
    asyncio.run(test_performance_benchmark_system())